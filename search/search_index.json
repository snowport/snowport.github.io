{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>My passion lies in transforming complex datasets into actionable insights, enabling organizations to align growth with environmental stewardship.</p> <p>\u200bWhether streamlining operations, informing sustainability strategies, or sparking innovation, I bring data to the forefront of decision-making. Committed to creating meaningful change, I'm dedicated to applying analytics to help industries operate sustainably and innovate strategically.</p> <p>Let's connect and collaborate on impactful, data-driven solutions.</p> <p>-- Noah</p> @noahportman"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#experience","title":"Experience","text":"<p>Sermonix Pharmaceuticals - Data Analysis and Business Development Associate</p> <ul> <li>Developed patient databases for ongoing clinical trials, progress reports, and data visualizations.</li> <li>Increased efficiencies by recognizing opportunities for process improvement and new system integration.</li> <li>Generated valuable insights on competitive intelligence leveraging web-based applications and life science   databases.\u200b</li> </ul> <p>N-Drip - Sustainability Associate</p> <ul> <li>Engaged with universities, think tanks, and trade associations as to the   benefits of the technology.</li> <li>Compiled a database for a range of corporations' supply chains regarding water and carbon reduction commitments.</li> <li>Researched strategies and methods for generating agricultural carbon offsets from carbon   and methane reductions.</li> </ul> <p>Columbia University - Project Manager, SuSci Integrative Capstone</p> <ul> <li>Led study to quantify air pollution on subway platforms across New York City   boroughs by deploying handheld particulate matter air sampling devices.</li> <li>Collaborated with other team members on data collection, compilation, and   analysis, in addition to the final report, which was presented at the   International Society of Exposure Science conference in October 2021.</li> </ul> Diagnostic Lab Corporation - Sustainability Associate <ul> <li>Designed UI/UX for a drinking water analysis testing kit and API linking   consumers with analytical laboratories.</li> <li>Directed business development on   sustainability best practices regarding water and the environment.</li> <li>Prepared client presentation pitch decks and proposals using analytics and strong   visual communication skills.</li> </ul> Gresham Smith - Environmental Engineer <ul> <li>Performed EMS and EHS inspections to help clients meet compliance and achieve   sustainability targets.</li> <li>Developed NPDES outfall permits through field work, data analysis, and QUAL2K modeling.</li> <li>Investigated and drafted SPCC and SWPPP plans for numerous government and private clients.</li> </ul> eggXYt - Business Development Associate <ul> <li>Showcased product demo as Startup Battlefield finalist at TechCrunch Disrupt   San Francisco.</li> <li>Managed the overall appearance of the desktop and mobile   platforms for the website.</li> <li>Constructed database for competitive intelligence   and compendium on AgTech investment space.</li> </ul> Zuckerberg Institute for Water Research - Hydrology Lab Assistant <ul> <li>Increased proficiency using centrifuge, microplate photometers, and other   hydrology and microbiology lab equipment.</li> <li>Researched zero-discharge aquaculture system analyzing nitrification efficiency of biofloc.</li> </ul>"},{"location":"cv/#education","title":"Education","text":"<p>MIT xPRO - Professional Certificate in Data Science and Analytics</p> <ul> <li>Completed an intensive program designed to enhance data science skills,   focusing on analytics and machine learning applications.</li> <li>Completed comprehensive coursework covering essential topics such as statistical   analysis, data visualization, machine learning algorithms, and predictive   modeling techniques.</li> <li>Developed proficiency in programming languages and tools   such as Python, enabling the analysis and interpretation of complex datasets.</li> <li>Built a portfolio of projects that showcased practical applications of data science.</li> </ul> <p>Visit: MIT xPRO</p> <p>Climatebase Fellowship - Cohort 3 Fellow</p> <ul> <li>Selected for a prestigious fellowship, enhancing expertise in climate policy,   strategic environmental planning, and innovative solution development.</li> <li>Distinguished as a top performer in a cohort of emerging leaders in climate   policy and sustainability.</li> <li>Developed and executed a capstone project utilizing   geospatial analysis to assess biodiversity and water quality, showcasing   innovation in environmental research.</li> <li>Enhanced strategic planning skills,   focusing on the development of actionable and impactful environmental policies   and initiatives.</li> <li>Engaged with a network of professionals and experts in   sustainability, broadening perspective and fostering collaborative   opportunities in the field.</li> </ul> <p>Visit: Climatebase Fellowship</p> <p>Columbia University - MS, Sustainability Science</p> <p>The Ohio State University - BS, Environmental Engineering</p> <p> <code>Magna Cum Laude</code> <code>Humanitarian Engineering Scholar</code> <code>Tau Beta Pi</code></p> <ul> <li>Teacher's Assistant, Department of Environmental Engineering</li> <li>Research Intern, Department of Ecological Engineering</li> </ul> Ben-Gurion University of the Negev <ul> <li>Semester-long study abroad focused on sustainable development, natural resource management, and Middle East diplomacy</li> </ul> Engineers Without Borders (Shantou University &amp; Technion University) <ul> <li>6-week Summer study abroad program</li> <li>Completed multidisciplinary engineering-related workshops, discussions, and language classes.</li> <li>Led a comprehensive health and environmental field assessment of a Chinese village.</li> </ul>"},{"location":"cv/#skills","title":"Skills","text":"Programming Languages &amp; Data Analysis <ul> <li>Proficient in <code>Python</code>, <code>R</code>, <code>SQL</code>, and <code>MATLAB</code>, with a strong ability to conduct   complex data analysis, statistical modeling, and predictive analytics</li> </ul> Geospatial Analysis <ul> <li>Skilled in Geographic Information Systems (GIS), <code>Google Earth Engine</code>, and   <code>CADD</code>, enabling advanced analysis of spatial data for environmental assessments   and planning</li> </ul> Data Visualization &amp; Reporting <ul> <li>Experienced in using <code>Tableau</code>, <code>Microsoft 365</code> (Excel, Power BI), and <code>Snowflake</code>   for creating compelling data visualizations, dashboards, and reports to   communicate insights effectively</li> </ul> Cloud Computing &amp; Database Management <ul> <li>Competent in AWS Cloud services, facilitating the management of large   datasets, cloud computing tasks, and ensuring data security and scalability</li> </ul> Software &amp; Development Tools <ul> <li>Adept in using integrated development environments (IDEs) and collaboration   tools, enhancing productivity and supporting team-based software development   projects</li> </ul>"},{"location":"cv/#publications","title":"Publications","text":"<p>Authored</p> Acknowledged Contributions <p> <ul> <li> <p>Damodaran, S., et al. (2023). Open-label, phase II, multicenter study of lasofoxifene plus abemaciclib for treating women with metastatic ER+/HER2\u2212 breast cancer and an ESR1 mutation after disease progression on prior therapies: ELAINE 2. Annals of Oncology, 34(12), 1131-1140. https://doi.org/10.1016/j.annonc.2023.09.3103</p> </li> <li> <p>Goetz, M.P., et al. (2023). Lasofoxifene versus fulvestrant for ER+/HER2\u2212 metastatic breast cancer with an ESR1 mutation: Results from the randomized, phase II ELAINE 1 trial. Annals of Oncology, 34(10), 1141\u20131151. https://doi.org/10.1016/j.annonc.2023.09.3104</p> </li> <li> <p>Rey-Sanchez, A. C., Bohrer, G., Morin, T. H., Shlomo, D., &amp; Mirfenderesgi, G. (2017). Evaporation and CO\u2082 fluxes in a coastal reef: An eddy covariance approach. Ecosystem Health and Sustainability, 3(1), 1392830. https://doi.org/10.1080/20964129.2017.1392830</p> </li> </ul> <p></p>"},{"location":"notebooks/portgeo/","title":"Portgeo","text":"<p>Check out examples from my very own Python package: Portgeo!</p> <p></p> <p>Enjoy  </p>"},{"location":"notebooks/mitxpro/01_prescriptive_data/","title":"1. Patient Scheduling","text":"<p>Knowns:</p> <ul> <li>A physician's workload is 30 patients per day.</li> <li>Unfortunately, because of no-shows, about 25% of patients fail to show up to their appointments leading to loss revenue for the physician.</li> </ul> <p>Unknowns:</p> <ul> <li>The ideal amount of patient bookings per day.</li> </ul> <p>We can use python to intuitively calculate how many patients the physician should book to stay busy if only 75% of scheduled patients show up.</p> In\u00a0[9]: Copied! <pre>import scipy\nfrom scipy.stats import binom\n</pre> import scipy from scipy.stats import binom In\u00a0[10]: Copied! <pre># Max number of patients per day\nworkload = 30\n\n# Percentage of patients that show up\nprob = 0.75\n\n# Set \"ans\" as the answer\nans = workload / prob\n</pre> # Max number of patients per day workload = 30  # Percentage of patients that show up prob = 0.75  # Set \"ans\" as the answer ans = workload / prob In\u00a0[11]: Copied! <pre>print(ans)\n</pre> print(ans) <pre>40.0\n</pre> In\u00a0[12]: Copied! <pre>import matplotlib.pyplot as plt\n\n# set  the values of x and prob\nx = 40\nprob = 0.75\n\n# defining list of r values\nr_values = list(range(x + 1))\n\n# generate the \"dist\" variable by calling the binom.pmf() function below. this will be the list of pmf values\ndist = [binom.pmf(r, x, prob) for r in r_values]\n\n# Plotting the graph. Do not change. Used for grading\nplots = plt.bar(r_values, dist)\nxy = []\nfor plot in plots:\n    xy.append(plot.get_xy())\nplt.show()\n</pre> import matplotlib.pyplot as plt  # set  the values of x and prob x = 40 prob = 0.75  # defining list of r values r_values = list(range(x + 1))  # generate the \"dist\" variable by calling the binom.pmf() function below. this will be the list of pmf values dist = [binom.pmf(r, x, prob) for r in r_values]  # Plotting the graph. Do not change. Used for grading plots = plt.bar(r_values, dist) xy = [] for plot in plots:     xy.append(plot.get_xy()) plt.show() <p>Next, use Python's scipy.stats.binom package to get more insight of the situation. Find the cumulative density function to statistically calculate how much (in percent) will the physician be overbooked if we use the linear calculations from the first step to overbook patients.</p> In\u00a0[13]: Copied! <pre># Fill in your linear prediction\nlinearPrediction = 40\nworkload = 30\nprob = 0.75\n\n# Find the amount overworked by calculating 1 - cdf(). Set the output to a variable called \"overworked\"\noverworked = 1 - binom.cdf(workload, linearPrediction, prob)\n</pre> # Fill in your linear prediction linearPrediction = 40 workload = 30 prob = 0.75  # Find the amount overworked by calculating 1 - cdf(). Set the output to a variable called \"overworked\" overworked = 1 - binom.cdf(workload, linearPrediction, prob) In\u00a0[14]: Copied! <pre>print(overworked)\n</pre> print(overworked) <pre>0.439539731672533\n</pre> <p>Finally, we can use the binomial ppf() function to find the number of appointments the physician should book if she wants to limit the risk of having more than 30 appointments per day to 5%</p> In\u00a0[15]: Copied! <pre>workload = 30\nprob = 0.75\n\n# Target amount of overbooked patients\ntarget = 0.05\n\n# Set \"total\" = workload + ppf()\ntotal = workload + binom.ppf(target, workload, 1 - prob)\n</pre> workload = 30 prob = 0.75  # Target amount of overbooked patients target = 0.05  # Set \"total\" = workload + ppf() total = workload + binom.ppf(target, workload, 1 - prob) In\u00a0[16]: Copied! <pre>print(total)\n</pre> print(total) <pre>34.0\n</pre>"},{"location":"notebooks/mitxpro/01_prescriptive_data/#optimizing-patient-scheduling-with-binomial-probability","title":"Optimizing Patient Scheduling with Binomial Probability\u00b6","text":""},{"location":"notebooks/mitxpro/01_prescriptive_data/#introduction","title":"Introduction\u00b6","text":"<p>This notebook contains a real-world application of the binomial distribution to help a physician optimize daily patient bookings. The physician sees 30 patients per day, but due to a consistent 25% no-show rate, there is a significant loss in efficiency and revenue.</p>"},{"location":"notebooks/mitxpro/01_prescriptive_data/#technical-preliminaries","title":"Technical preliminaries\u00b6","text":""},{"location":"notebooks/mitxpro/01_prescriptive_data/#setting-up-the-problem","title":"Setting Up the Problem\u00b6","text":""},{"location":"notebooks/mitxpro/01_prescriptive_data/#binomial-distribution-model","title":"Binomial Distribution Model\u00b6","text":"<p>Because we are using historical data to assume 25% of patients no-show, this is a binomial distribution model. Similarly to how we calculate the normal distribution's Cumulative Density Function (CMF) and Probability Density Function (PDF), we can do the same for this normal distribution.</p> <p>First fill in the x and probability values to display the normal distribution of the linearly calculated value we found in the previous step.</p>"},{"location":"notebooks/mitxpro/01_prescriptive_data/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/mitxpro/02_binary_classification/","title":"2. Predicting Abnormal Scans","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# initialize the seeds of different random number generators so that the\n# results will be the same every time the notebook is run\ntf.random.set_seed(42)\n\npd.options.mode.chained_assignment = None\n</pre> import tensorflow as tf from tensorflow import keras import numpy as np import pandas as pd import matplotlib.pyplot as plt  # initialize the seeds of different random number generators so that the # results will be the same every time the notebook is run tf.random.set_seed(42)  pd.options.mode.chained_assignment = None In\u00a0[2]: Copied! <pre># Because each column of data represents a datapoint we will name the columns by the sequence of datapoints\n# (1,2,3...140)\nnames = []\nfor i in range(140):\n    names.append(i)\n# The last column will be the target or dependent variable\nnames.append(\"Target\")\n</pre> # Because each column of data represents a datapoint we will name the columns by the sequence of datapoints # (1,2,3...140) names = [] for i in range(140):     names.append(i) # The last column will be the target or dependent variable names.append(\"Target\") <p>Read in the data from http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv and set the column names from the list created in the box above</p> In\u00a0[3]: Copied! <pre>df = pd.read_csv(\n    \"http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv\", header=None\n)\n\ndf.columns = names\n</pre> df = pd.read_csv(     \"http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv\", header=None )  df.columns = names In\u00a0[4]: Copied! <pre>df.shape\n</pre> df.shape Out[4]: <pre>(4998, 141)</pre> In\u00a0[5]: Copied! <pre>df.head()\n</pre> df.head() Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 131 132 133 134 135 136 137 138 139 Target 0 -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408 -1.818286 -1.250522 -0.477492 ... 0.792168 0.933541 0.796958 0.578621 0.257740 0.228077 0.123431 0.925286 0.193137 1.0 1 -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126 -0.992258 -0.754680 0.042321 ... 0.538356 0.656881 0.787490 0.724046 0.555784 0.476333 0.773820 1.119621 -1.436250 1.0 2 -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940 -1.490659 -1.183580 -0.394229 ... 0.886073 0.531452 0.311377 -0.021919 -0.713683 -0.532197 0.321097 0.904227 -0.421797 1.0 3 0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280 -1.671131 -1.333884 -0.965629 ... 0.350816 0.499111 0.600345 0.842069 0.952074 0.990133 1.086798 1.403011 -0.383564 1.0 4 0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510 -1.783423 -1.594450 -0.753199 ... 1.148884 0.958434 1.059025 1.371682 1.277392 0.960304 0.971020 1.614392 1.421456 1.0 <p>5 rows \u00d7 141 columns</p> In\u00a0[6]: Copied! <pre>numerics = names\n\n# Remove the dependent variable\nnumerics.remove(\"Target\")\n</pre> numerics = names  # Remove the dependent variable numerics.remove(\"Target\") In\u00a0[7]: Copied! <pre># Set the output to \"target_metrics\"\ntarget_metrics = df.Target.value_counts(normalize=True)\nprint(target_metrics)\n</pre> # Set the output to \"target_metrics\" target_metrics = df.Target.value_counts(normalize=True) print(target_metrics) <pre>Target\n1.0    0.584034\n0.0    0.415966\nName: proportion, dtype: float64\n</pre> <p>Extract the dependent variable</p> In\u00a0[8]: Copied! <pre># set the dependent variables to 'y'\ny = df.pop(\"Target\")\n</pre> # set the dependent variables to 'y' y = df.pop(\"Target\") <p>Before we normalize the numerics, let's split the data into an 80% training set and 20% test set (why should we split before normalization?).</p> In\u00a0[9]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[10]: Copied! <pre># split into train and test sets with the following naming conventions:\n# X_train, X_test, y_train and y_test\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, stratify=y)\n</pre> # split into train and test sets with the following naming conventions: # X_train, X_test, y_train and y_test X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, stratify=y) <p>OK, let's calculate the mean and standard deviation of every numeric variable in the training set.</p> In\u00a0[11]: Copied! <pre># Assign the means to \"means\" and standard deviation to \"sd\"\nmeans = X_train[numerics].mean()\nsd = X_train[numerics].std()\nprint(means)\n</pre> # Assign the means to \"means\" and standard deviation to \"sd\" means = X_train[numerics].mean() sd = X_train[numerics].std() print(means) <pre>0     -0.261910\n1     -1.649880\n2     -2.495738\n3     -3.123526\n4     -3.170804\n         ...   \n135   -0.779775\n136   -0.842486\n137   -0.640901\n138   -0.484135\n139   -0.704742\nLength: 140, dtype: float64\n</pre> <p>Let's normalize the train and test dataframes with these means and standard deviations.</p> In\u00a0[12]: Copied! <pre># Normalize X_train\nX_train[numerics] = (X_train[numerics] - means) / sd\n</pre> # Normalize X_train X_train[numerics] = (X_train[numerics] - means) / sd In\u00a0[13]: Copied! <pre># Normalize X_test\nX_test[numerics] = (X_test[numerics] - means) / sd\n</pre> # Normalize X_test X_test[numerics] = (X_test[numerics] - means) / sd In\u00a0[14]: Copied! <pre>X_train.head()\n</pre> X_train.head() Out[14]: 0 1 2 3 4 5 6 7 8 9 ... 130 131 132 133 134 135 136 137 138 139 4321 0.942906 0.918411 0.756059 0.549809 0.279007 -0.000239 -0.318129 -0.297419 0.114735 0.023530 ... -0.844024 -1.079494 -1.169549 -1.291348 -1.581476 -1.596743 -1.503656 -1.144827 -0.926862 -0.418502 1589 -0.676037 -0.803506 -1.016787 -1.077450 -0.916263 -0.351198 0.628702 0.524610 0.673169 1.089889 ... 0.858042 0.814006 0.978348 0.878989 0.727128 0.663161 0.757176 0.790068 0.355183 -0.646023 4109 1.272265 0.974431 0.564563 0.484635 0.155354 -0.257428 -0.765206 -0.598672 0.175432 -0.133798 ... 0.182676 0.012165 -0.367248 -0.649725 -0.888969 -1.240149 -1.619437 -1.854891 -1.307798 -0.740308 1018 1.039653 0.543801 -0.172987 -0.433091 -1.105617 -1.650335 -1.218847 -0.441137 -0.787254 -0.423532 ... -0.347807 0.082412 0.571933 0.705932 0.906023 1.013345 0.982132 0.775867 0.316683 0.817843 3552 0.888205 1.145303 1.259494 1.365041 1.064103 0.257039 -0.683586 -1.330719 -1.119904 -0.882630 ... -0.741992 -1.023779 -1.223630 -1.312176 -1.534803 -1.621368 -1.564656 -1.259108 -0.915075 -0.044013 <p>5 rows \u00d7 140 columns</p> <p>The easiest way to feed data to Keras/Tensorflow is as Numpy arrays so we convert our two dataframes to Numpy arrays.</p> In\u00a0[15]: Copied! <pre># Convert X_train and X_test to Numpy arrays\nX_train = X_train.to_numpy()\nX_test = X_test.to_numpy()\n</pre> # Convert X_train and X_test to Numpy arrays X_train = X_train.to_numpy() X_test = X_test.to_numpy() In\u00a0[16]: Copied! <pre>X_train.shape, y_train.shape\n</pre> X_train.shape, y_train.shape Out[16]: <pre>((3998, 140), (3998,))</pre> In\u00a0[17]: Copied! <pre>X_test.shape, y_test.shape\n</pre> X_test.shape, y_test.shape Out[17]: <pre>((1000, 140), (1000,))</pre> In\u00a0[18]: Copied! <pre># get the number of columns and assign it to \"num_columns\"\n\nnum_columns = X_train.shape[1]\n\n# Define the input layer. assign it to \"input\"\ninput = keras.Input(shape=(num_columns,), dtype=\"float32\")\n\n# Feed the input vector to the hidden layer. Call it \"h\"\nh = keras.layers.Dense(16, activation=\"relu\", name=\"Hidden\")(input)\n\n# Feed the output of the hidden layer to the output layer. Call it \"output\"\noutput = keras.layers.Dense(1, activation=\"sigmoid\", name=\"Output\")(h)\n\n# tell Keras that this (input,output) pair is your model. Call it \"model\"\nmodel = keras.Model(input, output)\n</pre> # get the number of columns and assign it to \"num_columns\"  num_columns = X_train.shape[1]  # Define the input layer. assign it to \"input\" input = keras.Input(shape=(num_columns,), dtype=\"float32\")  # Feed the input vector to the hidden layer. Call it \"h\" h = keras.layers.Dense(16, activation=\"relu\", name=\"Hidden\")(input)  # Feed the output of the hidden layer to the output layer. Call it \"output\" output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"Output\")(h)  # tell Keras that this (input,output) pair is your model. Call it \"model\" model = keras.Model(input, output) In\u00a0[19]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"functional\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer (InputLayer)        \u2502 (None, 140)            \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Hidden (Dense)                  \u2502 (None, 16)             \u2502         2,256 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Output (Dense)                  \u2502 (None, 1)              \u2502            17 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 2,273 (8.88 KB)\n</pre> <pre> Trainable params: 2,273 (8.88 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[20]: Copied! <pre>keras.utils.plot_model(model, show_shapes=True)\n</pre> keras.utils.plot_model(model, show_shapes=True) Out[20]: <p>Now that the model is defined, we need to tell Keras three things:</p> <ul> <li>What loss function to use - Since our output variable is binary, we will select the <code>binary_crossentropy</code> loss function.</li> <li>Which optimizer to use - we will use a 'flavor' of SGD called <code>adam</code> which is an excellent default choice</li> <li>What metrics you want Keras to report out - in classification problems like this one, <code>accuracy</code> is commonly used.</li> </ul> In\u00a0[21]: Copied! <pre>model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n</pre> model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) In\u00a0[22]: Copied! <pre># Fit your model and assign the output to \"history\"\nhistory = model.fit(\n    X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=2\n)\n</pre> # Fit your model and assign the output to \"history\" history = model.fit(     X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=2 ) <pre>Epoch 1/100\n100/100 - 2s - 20ms/step - accuracy: 0.9506 - loss: 0.1612 - val_accuracy: 0.9812 - val_loss: 0.0573\nEpoch 2/100\n100/100 - 0s - 3ms/step - accuracy: 0.9856 - loss: 0.0520 - val_accuracy: 0.9862 - val_loss: 0.0411\nEpoch 3/100\n100/100 - 0s - 3ms/step - accuracy: 0.9906 - loss: 0.0397 - val_accuracy: 0.9875 - val_loss: 0.0347\nEpoch 4/100\n100/100 - 0s - 3ms/step - accuracy: 0.9916 - loss: 0.0334 - val_accuracy: 0.9875 - val_loss: 0.0312\nEpoch 5/100\n100/100 - 0s - 3ms/step - accuracy: 0.9919 - loss: 0.0289 - val_accuracy: 0.9887 - val_loss: 0.0287\nEpoch 6/100\n100/100 - 0s - 3ms/step - accuracy: 0.9931 - loss: 0.0257 - val_accuracy: 0.9900 - val_loss: 0.0272\nEpoch 7/100\n100/100 - 0s - 3ms/step - accuracy: 0.9941 - loss: 0.0232 - val_accuracy: 0.9900 - val_loss: 0.0264\nEpoch 8/100\n100/100 - 0s - 3ms/step - accuracy: 0.9941 - loss: 0.0210 - val_accuracy: 0.9900 - val_loss: 0.0251\nEpoch 9/100\n100/100 - 1s - 6ms/step - accuracy: 0.9953 - loss: 0.0192 - val_accuracy: 0.9912 - val_loss: 0.0250\nEpoch 10/100\n100/100 - 0s - 3ms/step - accuracy: 0.9956 - loss: 0.0178 - val_accuracy: 0.9912 - val_loss: 0.0244\nEpoch 11/100\n100/100 - 0s - 3ms/step - accuracy: 0.9962 - loss: 0.0164 - val_accuracy: 0.9912 - val_loss: 0.0234\nEpoch 12/100\n100/100 - 0s - 3ms/step - accuracy: 0.9962 - loss: 0.0153 - val_accuracy: 0.9912 - val_loss: 0.0239\nEpoch 13/100\n100/100 - 0s - 3ms/step - accuracy: 0.9959 - loss: 0.0147 - val_accuracy: 0.9912 - val_loss: 0.0235\nEpoch 14/100\n100/100 - 0s - 3ms/step - accuracy: 0.9966 - loss: 0.0137 - val_accuracy: 0.9912 - val_loss: 0.0235\nEpoch 15/100\n100/100 - 0s - 3ms/step - accuracy: 0.9969 - loss: 0.0129 - val_accuracy: 0.9925 - val_loss: 0.0233\nEpoch 16/100\n100/100 - 0s - 3ms/step - accuracy: 0.9969 - loss: 0.0124 - val_accuracy: 0.9925 - val_loss: 0.0227\nEpoch 17/100\n100/100 - 0s - 3ms/step - accuracy: 0.9972 - loss: 0.0118 - val_accuracy: 0.9925 - val_loss: 0.0226\nEpoch 18/100\n100/100 - 1s - 6ms/step - accuracy: 0.9972 - loss: 0.0111 - val_accuracy: 0.9925 - val_loss: 0.0233\nEpoch 19/100\n100/100 - 0s - 3ms/step - accuracy: 0.9972 - loss: 0.0108 - val_accuracy: 0.9925 - val_loss: 0.0227\nEpoch 20/100\n100/100 - 0s - 3ms/step - accuracy: 0.9975 - loss: 0.0103 - val_accuracy: 0.9925 - val_loss: 0.0230\nEpoch 21/100\n100/100 - 1s - 6ms/step - accuracy: 0.9975 - loss: 0.0100 - val_accuracy: 0.9925 - val_loss: 0.0235\nEpoch 22/100\n100/100 - 0s - 3ms/step - accuracy: 0.9975 - loss: 0.0096 - val_accuracy: 0.9925 - val_loss: 0.0235\nEpoch 23/100\n100/100 - 0s - 3ms/step - accuracy: 0.9975 - loss: 0.0092 - val_accuracy: 0.9925 - val_loss: 0.0245\nEpoch 24/100\n100/100 - 0s - 3ms/step - accuracy: 0.9975 - loss: 0.0090 - val_accuracy: 0.9925 - val_loss: 0.0229\nEpoch 25/100\n100/100 - 0s - 3ms/step - accuracy: 0.9975 - loss: 0.0086 - val_accuracy: 0.9925 - val_loss: 0.0241\nEpoch 26/100\n100/100 - 0s - 3ms/step - accuracy: 0.9978 - loss: 0.0083 - val_accuracy: 0.9925 - val_loss: 0.0235\nEpoch 27/100\n100/100 - 1s - 6ms/step - accuracy: 0.9975 - loss: 0.0082 - val_accuracy: 0.9925 - val_loss: 0.0239\nEpoch 28/100\n100/100 - 0s - 3ms/step - accuracy: 0.9981 - loss: 0.0078 - val_accuracy: 0.9925 - val_loss: 0.0242\nEpoch 29/100\n100/100 - 0s - 3ms/step - accuracy: 0.9978 - loss: 0.0077 - val_accuracy: 0.9925 - val_loss: 0.0244\nEpoch 30/100\n100/100 - 0s - 5ms/step - accuracy: 0.9984 - loss: 0.0075 - val_accuracy: 0.9925 - val_loss: 0.0234\nEpoch 31/100\n100/100 - 1s - 6ms/step - accuracy: 0.9984 - loss: 0.0071 - val_accuracy: 0.9925 - val_loss: 0.0246\nEpoch 32/100\n100/100 - 1s - 6ms/step - accuracy: 0.9984 - loss: 0.0070 - val_accuracy: 0.9925 - val_loss: 0.0228\nEpoch 33/100\n100/100 - 1s - 5ms/step - accuracy: 0.9981 - loss: 0.0067 - val_accuracy: 0.9925 - val_loss: 0.0240\nEpoch 34/100\n100/100 - 1s - 5ms/step - accuracy: 0.9984 - loss: 0.0066 - val_accuracy: 0.9937 - val_loss: 0.0227\nEpoch 35/100\n100/100 - 0s - 5ms/step - accuracy: 0.9984 - loss: 0.0062 - val_accuracy: 0.9925 - val_loss: 0.0235\nEpoch 36/100\n100/100 - 0s - 4ms/step - accuracy: 0.9987 - loss: 0.0060 - val_accuracy: 0.9925 - val_loss: 0.0240\nEpoch 37/100\n100/100 - 1s - 6ms/step - accuracy: 0.9987 - loss: 0.0058 - val_accuracy: 0.9925 - val_loss: 0.0239\nEpoch 38/100\n100/100 - 0s - 3ms/step - accuracy: 0.9987 - loss: 0.0055 - val_accuracy: 0.9925 - val_loss: 0.0237\nEpoch 39/100\n100/100 - 1s - 5ms/step - accuracy: 0.9987 - loss: 0.0055 - val_accuracy: 0.9937 - val_loss: 0.0241\nEpoch 40/100\n100/100 - 0s - 4ms/step - accuracy: 0.9987 - loss: 0.0053 - val_accuracy: 0.9925 - val_loss: 0.0241\nEpoch 41/100\n100/100 - 0s - 3ms/step - accuracy: 0.9987 - loss: 0.0049 - val_accuracy: 0.9925 - val_loss: 0.0254\nEpoch 42/100\n100/100 - 0s - 3ms/step - accuracy: 0.9987 - loss: 0.0049 - val_accuracy: 0.9937 - val_loss: 0.0233\nEpoch 43/100\n100/100 - 0s - 3ms/step - accuracy: 0.9991 - loss: 0.0046 - val_accuracy: 0.9925 - val_loss: 0.0243\nEpoch 44/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0046 - val_accuracy: 0.9912 - val_loss: 0.0246\nEpoch 45/100\n100/100 - 0s - 3ms/step - accuracy: 0.9991 - loss: 0.0043 - val_accuracy: 0.9937 - val_loss: 0.0242\nEpoch 46/100\n100/100 - 1s - 6ms/step - accuracy: 0.9994 - loss: 0.0041 - val_accuracy: 0.9937 - val_loss: 0.0242\nEpoch 47/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0040 - val_accuracy: 0.9925 - val_loss: 0.0246\nEpoch 48/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0039 - val_accuracy: 0.9937 - val_loss: 0.0241\nEpoch 49/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0037 - val_accuracy: 0.9912 - val_loss: 0.0240\nEpoch 50/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0037 - val_accuracy: 0.9937 - val_loss: 0.0243\nEpoch 51/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0035 - val_accuracy: 0.9925 - val_loss: 0.0241\nEpoch 52/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0033 - val_accuracy: 0.9925 - val_loss: 0.0245\nEpoch 53/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0031 - val_accuracy: 0.9950 - val_loss: 0.0238\nEpoch 54/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0030 - val_accuracy: 0.9937 - val_loss: 0.0240\nEpoch 55/100\n100/100 - 0s - 3ms/step - accuracy: 0.9994 - loss: 0.0029 - val_accuracy: 0.9937 - val_loss: 0.0249\nEpoch 56/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0028 - val_accuracy: 0.9950 - val_loss: 0.0236\nEpoch 57/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0026 - val_accuracy: 0.9950 - val_loss: 0.0239\nEpoch 58/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0025 - val_accuracy: 0.9950 - val_loss: 0.0246\nEpoch 59/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0024 - val_accuracy: 0.9925 - val_loss: 0.0227\nEpoch 60/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0022 - val_accuracy: 0.9950 - val_loss: 0.0243\nEpoch 61/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0022 - val_accuracy: 0.9937 - val_loss: 0.0251\nEpoch 62/100\n100/100 - 0s - 4ms/step - accuracy: 0.9997 - loss: 0.0021 - val_accuracy: 0.9950 - val_loss: 0.0240\nEpoch 63/100\n100/100 - 1s - 6ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 0.9950 - val_loss: 0.0237\nEpoch 64/100\n100/100 - 1s - 6ms/step - accuracy: 0.9997 - loss: 0.0019 - val_accuracy: 0.9950 - val_loss: 0.0239\nEpoch 65/100\n100/100 - 0s - 5ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 0.9950 - val_loss: 0.0251\nEpoch 66/100\n100/100 - 0s - 4ms/step - accuracy: 0.9997 - loss: 0.0017 - val_accuracy: 0.9950 - val_loss: 0.0248\nEpoch 67/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 0.9950 - val_loss: 0.0240\nEpoch 68/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9950 - val_loss: 0.0255\nEpoch 69/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0014 - val_accuracy: 0.9950 - val_loss: 0.0250\nEpoch 70/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0014 - val_accuracy: 0.9950 - val_loss: 0.0247\nEpoch 71/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9950 - val_loss: 0.0248\nEpoch 72/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9950 - val_loss: 0.0247\nEpoch 73/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9950 - val_loss: 0.0251\nEpoch 74/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9950 - val_loss: 0.0266\nEpoch 75/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 0.0010 - val_accuracy: 0.9950 - val_loss: 0.0242\nEpoch 76/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 9.5420e-04 - val_accuracy: 0.9950 - val_loss: 0.0255\nEpoch 77/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 9.1561e-04 - val_accuracy: 0.9950 - val_loss: 0.0247\nEpoch 78/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 8.4136e-04 - val_accuracy: 0.9950 - val_loss: 0.0259\nEpoch 79/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 7.7600e-04 - val_accuracy: 0.9950 - val_loss: 0.0252\nEpoch 80/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 7.6722e-04 - val_accuracy: 0.9950 - val_loss: 0.0251\nEpoch 81/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 7.0823e-04 - val_accuracy: 0.9950 - val_loss: 0.0249\nEpoch 82/100\n100/100 - 0s - 3ms/step - accuracy: 0.9997 - loss: 6.3300e-04 - val_accuracy: 0.9950 - val_loss: 0.0257\nEpoch 83/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 5.8856e-04 - val_accuracy: 0.9950 - val_loss: 0.0263\nEpoch 84/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 6.0911e-04 - val_accuracy: 0.9950 - val_loss: 0.0254\nEpoch 85/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 5.5517e-04 - val_accuracy: 0.9950 - val_loss: 0.0252\nEpoch 86/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 5.1629e-04 - val_accuracy: 0.9950 - val_loss: 0.0247\nEpoch 87/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 4.5281e-04 - val_accuracy: 0.9950 - val_loss: 0.0252\nEpoch 88/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 4.6413e-04 - val_accuracy: 0.9950 - val_loss: 0.0240\nEpoch 89/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 4.2493e-04 - val_accuracy: 0.9950 - val_loss: 0.0248\nEpoch 90/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 3.9707e-04 - val_accuracy: 0.9950 - val_loss: 0.0247\nEpoch 91/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 3.5185e-04 - val_accuracy: 0.9950 - val_loss: 0.0255\nEpoch 92/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 3.4393e-04 - val_accuracy: 0.9950 - val_loss: 0.0253\nEpoch 93/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 3.4352e-04 - val_accuracy: 0.9950 - val_loss: 0.0250\nEpoch 94/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 2.9782e-04 - val_accuracy: 0.9950 - val_loss: 0.0253\nEpoch 95/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 2.9031e-04 - val_accuracy: 0.9950 - val_loss: 0.0248\nEpoch 96/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 2.6856e-04 - val_accuracy: 0.9950 - val_loss: 0.0247\nEpoch 97/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 2.5759e-04 - val_accuracy: 0.9950 - val_loss: 0.0243\nEpoch 98/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 2.3058e-04 - val_accuracy: 0.9950 - val_loss: 0.0257\nEpoch 99/100\n100/100 - 0s - 3ms/step - accuracy: 1.0000 - loss: 2.2493e-04 - val_accuracy: 0.9950 - val_loss: 0.0259\nEpoch 100/100\n100/100 - 0s - 4ms/step - accuracy: 1.0000 - loss: 2.2232e-04 - val_accuracy: 0.9950 - val_loss: 0.0245\n</pre> In\u00a0[23]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() Out[23]: <pre>dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])</pre> In\u00a0[24]: Copied! <pre>loss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n</pre> loss_values = history_dict[\"loss\"] val_loss_values = history_dict[\"val_loss\"] epochs = range(1, len(loss_values) + 1) plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\") plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\") plt.title(\"Training and validation loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend() plt.show() In\u00a0[25]: Copied! <pre>plt.clf()\nacc = history_dict[\"accuracy\"]\nval_acc = history_dict[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training acc\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n</pre> plt.clf() acc = history_dict[\"accuracy\"] val_acc = history_dict[\"val_accuracy\"] plt.plot(epochs, acc, \"bo\", label=\"Training acc\") plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\") plt.title(\"Training and validation accuracy\") plt.xlabel(\"Epochs\") plt.ylabel(\"Accuracy\") plt.legend() plt.show() In\u00a0[26]: Copied! <pre># Getting the results of your model for grading\nscore, acc = model.evaluate(X_test, y_test)\n</pre> # Getting the results of your model for grading score, acc = model.evaluate(X_test, y_test) <pre>32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9856 - loss: 0.0876  \n</pre> In\u00a0[27]: Copied! <pre>y.value_counts(normalize=True)\n</pre> y.value_counts(normalize=True) Out[27]: proportion Target 1.0 0.584034 0.0 0.415966 dtype: float64 In\u00a0[30]: Copied! <pre># Selecting a specific row (e.g., row index 300)\nrow_index = 300\ny_values = X_train[row_index, :]\nx_values = range(X_train.shape[1])  # X-axis: 0 to 139\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(x_values, y_values, linestyle=\"-\")\nplt.xlabel(\"X-Axis (Index)\")\nplt.ylabel(\"Y-Axis (Values)\")\nplt.title(f\"Plot of Row {row_index}\")\nplt.grid(True)\nplt.show()\n</pre> # Selecting a specific row (e.g., row index 300) row_index = 300 y_values = X_train[row_index, :] x_values = range(X_train.shape[1])  # X-axis: 0 to 139  # Plotting plt.figure(figsize=(10, 5)) plt.plot(x_values, y_values, linestyle=\"-\") plt.xlabel(\"X-Axis (Index)\") plt.ylabel(\"Y-Axis (Values)\") plt.title(f\"Plot of Row {row_index}\") plt.grid(True) plt.show() In\u00a0[31]: Copied! <pre>print(y_train[row_index])  # Result is abnormal scan for row_index=300\n</pre> print(y_train[row_index])  # Result is abnormal scan for row_index=300 <pre>0.0\n</pre>"},{"location":"notebooks/mitxpro/02_binary_classification/#binary-classification-on-tabular-data-predicting-abnormal-ecg-scans","title":"Binary Classification on Tabular Data - Predicting Abnormal ECG Scans\u00b6","text":""},{"location":"notebooks/mitxpro/02_binary_classification/#introduction","title":"Introduction\u00b6","text":"<p>In this notebook, you will train an autoencoder to detect anomalies on the ECG5000 dataset. This dataset contains 5,000 Electrocardiograms, each with 140 data points. You will use a simplified version of the dataset, where each example has been labeled either 0 (corresponding to an abnormal rhythm), or 1 (corresponding to a normal rhythm). You are interested in identifying the abnormal rhythms.</p>"},{"location":"notebooks/mitxpro/02_binary_classification/#technical-preliminaries","title":"Technical preliminaries\u00b6","text":""},{"location":"notebooks/mitxpro/02_binary_classification/#read-in-the-data","title":"Read in the data\u00b6","text":"<p>Conveniently, the dataset in CSV form has been made available online and we can load it into a Pandas dataframe with the very useful <code>pd.read_csv</code> command.</p>"},{"location":"notebooks/mitxpro/02_binary_classification/#preprocessing","title":"Preprocessing\u00b6","text":"<p>This dataset only has numeric variables. For consistency sake, we will assign the column names to variable numerics.</p>"},{"location":"notebooks/mitxpro/02_binary_classification/#build-a-model","title":"Build a model\u00b6","text":""},{"location":"notebooks/mitxpro/02_binary_classification/#define-model-in-keras","title":"Define model in Keras\u00b6","text":"<p>Creating an NN  is usually just a few lines of Keras code.</p> <ul> <li>We will start with a single hidden layer.</li> <li>Since this is a binary classification problem, we will use a sigmoid activation in the output layer.</li> </ul>"},{"location":"notebooks/mitxpro/02_binary_classification/#set-optimization-parameters","title":"Set optimization parameters\u00b6","text":""},{"location":"notebooks/mitxpro/02_binary_classification/#train-the-model","title":"Train the model\u00b6","text":"<p>To kickoff training, we have to decide on three things:</p> <ul> <li>The batch size - 32 is a good default</li> <li>The number of epochs (i.e., how many passes through the training data). Start by setting this to 100, but you can experiment with different values.</li> <li>Whether we want to use a validation set. This will be useful for overfitting detection and regularization via early stopping so we will ask Keras to automatically use 20% of the data points as a validation set</li> </ul>"},{"location":"notebooks/mitxpro/02_binary_classification/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's see how well the model does on the test set.</p> <p><code>model.evaluate</code> is a very handy function to calculate the performance of your model on any dataset.</p>"},{"location":"notebooks/mitxpro/03_prescriptive_model/","title":"3. Statistical Modeling","text":"<p>This notebook walks through a full data analysis pipeline \u2014 from raw data ingestion and encoding detection to exploratory data analysis (EDA), statistical modeling, and model performance evaluation. Key tools include <code>chardet</code> and <code>codecs</code> for encoding detection, <code>pandas</code> and <code>skimpy</code> for data exploration, <code>statsmodels</code> for linear and logistic regression, and <code>scikit-learn</code> for evaluation metrics such as confusion matrices and accuracy scores.</p> In\u00a0[\u00a0]: Copied! <pre># Install dependencies, if needed\n!pip install skimpy\n</pre> # Install dependencies, if needed !pip install skimpy In\u00a0[2]: Copied! <pre>import chardet\nimport pandas as pd\n\n# Read the CSV file using the detected encoding and specifying the delimiter\n\nurl = \"https://docs.google.com/spreadsheets/d/10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2/export?format=csv&amp;id=10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2&amp;gid=1710894028\"\ndf = pd.read_csv(url, index_col=False)\n\ndf.head()\n</pre> import chardet import pandas as pd  # Read the CSV file using the detected encoding and specifying the delimiter  url = \"https://docs.google.com/spreadsheets/d/10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2/export?format=csv&amp;id=10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2&amp;gid=1710894028\" df = pd.read_csv(url, index_col=False)  df.head() Out[2]: default installment log_income fico_score rev_balance inquiries records 0 0 829 4.93 737 28.85 0 0 1 0 228 4.81 707 33.62 0 0 2 0 367 4.51 682 3.51 1 0 3 0 162 4.93 712 33.67 1 0 4 0 103 4.91 667 4.74 0 0 In\u00a0[3]: Copied! <pre># imports packages to be used in the code\nimport numpy as np\nimport codecs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport sklearn as skl\nfrom statsmodels.formula.api import ols\nfrom statsmodels.formula.api import logit\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom skimpy import skim\n\nprint(skl.__version__)\n\ndf\n</pre> # imports packages to be used in the code import numpy as np import codecs import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm import sklearn as skl from statsmodels.formula.api import ols from statsmodels.formula.api import logit from scipy.stats import norm from sklearn.model_selection import train_test_split from sklearn.metrics import ConfusionMatrixDisplay from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score from skimpy import skim  print(skl.__version__)  df <pre>1.6.1\n</pre> Out[3]: default installment log_income fico_score rev_balance inquiries records 0 0 829 4.93 737 28.85 0 0 1 0 228 4.81 707 33.62 0 0 2 0 367 4.51 682 3.51 1 0 3 0 162 4.93 712 33.67 1 0 4 0 103 4.91 667 4.74 0 0 ... ... ... ... ... ... ... ... 9511 1 345 5.29 672 215.37 2 0 9512 1 258 4.84 722 0.18 5 0 9513 1 98 4.60 687 10.04 8 0 9514 1 352 4.70 692 0.00 5 0 9515 1 853 4.89 732 37.88 6 0 <p>9516 rows \u00d7 7 columns</p> In\u00a0[4]: Copied! <pre># shows first 6 rows of dataframe\ndf.head(6)\n</pre> # shows first 6 rows of dataframe df.head(6) Out[4]: default installment log_income fico_score rev_balance inquiries records 0 0 829 4.93 737 28.85 0 0 1 0 228 4.81 707 33.62 0 0 2 0 367 4.51 682 3.51 1 0 3 0 162 4.93 712 33.67 1 0 4 0 103 4.91 667 4.74 0 0 5 0 125 5.17 727 50.81 0 0 In\u00a0[5]: Copied! <pre>df.tail(6)\n</pre> df.tail(6) Out[5]: default installment log_income fico_score rev_balance inquiries records 9510 1 70 4.39 662 3.00 6 0 9511 1 345 5.29 672 215.37 2 0 9512 1 258 4.84 722 0.18 5 0 9513 1 98 4.60 687 10.04 8 0 9514 1 352 4.70 692 0.00 5 0 9515 1 853 4.89 732 37.88 6 0 In\u00a0[6]: Copied! <pre>df = df[[\"default\", \"fico_score\"]]\n</pre> df = df[[\"default\", \"fico_score\"]] In\u00a0[7]: Copied! <pre>df\n</pre> df Out[7]: default fico_score 0 0 737 1 0 707 2 0 682 3 0 712 4 0 667 ... ... ... 9511 1 672 9512 1 722 9513 1 687 9514 1 692 9515 1 732 <p>9516 rows \u00d7 2 columns</p> In\u00a0[8]: Copied! <pre>skim(df)\n</pre> skim(df) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 skimpy summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502          Data Summary                Data Types                                                                 \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513                                                          \u2502\n\u2502 \u2503 Dataframe         \u2503 Values \u2503 \u2503 Column Type \u2503 Count \u2503                                                          \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529                                                          \u2502\n\u2502 \u2502 Number of rows    \u2502 9516   \u2502 \u2502 int64       \u2502 2     \u2502                                                          \u2502\n\u2502 \u2502 Number of columns \u2502 2      \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                          \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                                                  \u2502\n\u2502                                                     number                                                      \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \u2502\n\u2502 \u2503 column          \u2503 NA   \u2503 NA %    \u2503 mean      \u2503 sd        \u2503 p0    \u2503 p25   \u2503 p50   \u2503 p75   \u2503 p100  \u2503 hist    \u2503  \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529  \u2502\n\u2502 \u2502 default         \u2502    0 \u2502       0 \u2502    0.1598 \u2502    0.3665 \u2502     0 \u2502     0 \u2502     0 \u2502     0 \u2502     1 \u2502 \u2588    \u2582  \u2502  \u2502\n\u2502 \u2502 fico_score      \u2502    0 \u2502       0 \u2502     710.8 \u2502     37.96 \u2502   612 \u2502   682 \u2502   707 \u2502   737 \u2502   827 \u2502 \u2581\u2586\u2588\u2585\u2583\u2581  \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 End \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[9]: Copied! <pre># plot target variable\n# plt.scatter(df['default'], df['inquiries'],alpha =0.0, s=200)\n# df = pd.read_excel('loans.xlsx')\nplt.scatter(df[\"default\"], df[\"fico_score\"], alpha=1.0, s=200)\nplt.title(\"fico vs. default\")\nplt.xlabel(\"default\")\nplt.ylabel(\"fico\")\n</pre> # plot target variable # plt.scatter(df['default'], df['inquiries'],alpha =0.0, s=200) # df = pd.read_excel('loans.xlsx') plt.scatter(df[\"default\"], df[\"fico_score\"], alpha=1.0, s=200) plt.title(\"fico vs. default\") plt.xlabel(\"default\") plt.ylabel(\"fico\") Out[9]: <pre>Text(0, 0.5, 'fico')</pre> In\u00a0[10]: Copied! <pre># data split into 70% train and 30% test\ndf_train, df_test = train_test_split(df, test_size=0.3)\n</pre> # data split into 70% train and 30% test df_train, df_test = train_test_split(df, test_size=0.3) In\u00a0[11]: Copied! <pre>print(df_train)\n# Save the DataFrame to a CSV file\n# Replace 'file_path' with the path where you want to save the file\nfile_path = \"df_train.csv\"\ndf_train.to_csv(file_path, index=False)\n</pre> print(df_train) # Save the DataFrame to a CSV file # Replace 'file_path' with the path where you want to save the file file_path = \"df_train.csv\" df_train.to_csv(file_path, index=False) <pre>      default  fico_score\n578         0         717\n6479        0         727\n2787        0         677\n1931        0         697\n7539        0         687\n...       ...         ...\n7505        0         672\n5436        0         667\n7359        0         742\n9056        1         702\n8920        0         672\n\n[6661 rows x 2 columns]\n</pre> In\u00a0[12]: Copied! <pre>skim(df_train)\n</pre> skim(df_train) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 skimpy summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502          Data Summary                Data Types                                                                 \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513                                                          \u2502\n\u2502 \u2503 Dataframe         \u2503 Values \u2503 \u2503 Column Type \u2503 Count \u2503                                                          \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529                                                          \u2502\n\u2502 \u2502 Number of rows    \u2502 6661   \u2502 \u2502 int64       \u2502 2     \u2502                                                          \u2502\n\u2502 \u2502 Number of columns \u2502 2      \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                          \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                                                  \u2502\n\u2502                                                     number                                                      \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \u2502\n\u2502 \u2503 column          \u2503 NA   \u2503 NA %    \u2503 mean      \u2503 sd        \u2503 p0    \u2503 p25   \u2503 p50   \u2503 p75   \u2503 p100  \u2503 hist    \u2503  \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529  \u2502\n\u2502 \u2502 default         \u2502    0 \u2502       0 \u2502    0.1599 \u2502    0.3665 \u2502     0 \u2502     0 \u2502     0 \u2502     0 \u2502     1 \u2502 \u2588    \u2582  \u2502  \u2502\n\u2502 \u2502 fico_score      \u2502    0 \u2502       0 \u2502     710.8 \u2502     37.98 \u2502   612 \u2502   682 \u2502   707 \u2502   737 \u2502   827 \u2502 \u2581\u2586\u2588\u2586\u2583\u2581  \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 End \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[13]: Copied! <pre>skim(df_test)\n</pre> skim(df_test) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 skimpy summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502          Data Summary                Data Types                                                                 \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513                                                          \u2502\n\u2502 \u2503 Dataframe         \u2503 Values \u2503 \u2503 Column Type \u2503 Count \u2503                                                          \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529                                                          \u2502\n\u2502 \u2502 Number of rows    \u2502 2855   \u2502 \u2502 int64       \u2502 2     \u2502                                                          \u2502\n\u2502 \u2502 Number of columns \u2502 2      \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                          \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                                                  \u2502\n\u2502                                                     number                                                      \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \u2502\n\u2502 \u2503 column          \u2503 NA   \u2503 NA %    \u2503 mean      \u2503 sd        \u2503 p0    \u2503 p25   \u2503 p50   \u2503 p75   \u2503 p100  \u2503 hist    \u2503  \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529  \u2502\n\u2502 \u2502 default         \u2502    0 \u2502       0 \u2502    0.1597 \u2502    0.3664 \u2502     0 \u2502     0 \u2502     0 \u2502     0 \u2502     1 \u2502 \u2588    \u2582  \u2502  \u2502\n\u2502 \u2502 fico_score      \u2502    0 \u2502       0 \u2502       711 \u2502     37.91 \u2502   627 \u2502   682 \u2502   707 \u2502   737 \u2502   822 \u2502 \u2581\u2587\u2588\u2585\u2583\u2581  \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 End \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[14]: Copied! <pre># build formula,  target (dependent variable) ~ features (independent variables)\n# build model, fit the formula to the training data using a logistic algorithm (logit)\nest = logit(formula=\"default ~ fico_score\", data=df_train).fit()\n\n# print the results of the model (est).\n# Examine; Pseudo R-square\n\nprint(est.summary())\n</pre> # build formula,  target (dependent variable) ~ features (independent variables) # build model, fit the formula to the training data using a logistic algorithm (logit) est = logit(formula=\"default ~ fico_score\", data=df_train).fit()  # print the results of the model (est). # Examine; Pseudo R-square  print(est.summary()) <pre>Optimization terminated successfully.\n         Current function value: 0.428098\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                default   No. Observations:                 6661\nModel:                          Logit   Df Residuals:                     6659\nMethod:                           MLE   Df Model:                            1\nDate:                Fri, 30 May 2025   Pseudo R-squ.:                 0.02590\nTime:                        12:45:42   Log-Likelihood:                -2851.6\nconverged:                       True   LL-Null:                       -2927.4\nCovariance Type:            nonrobust   LLR p-value:                 7.607e-35\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.5097      0.688      9.455      0.000       5.160       7.859\nfico_score    -0.0116      0.001    -11.792      0.000      -0.014      -0.010\n==============================================================================\n</pre> In\u00a0[15]: Copied! <pre># apply the model (est) to the test data and make predictions\npreds = est.predict(df_test)\ndf_test[\"predicted_probability\"] = preds\n\n# print top 6 predicted probabilities\ndf_test.head(6)\n</pre> # apply the model (est) to the test data and make predictions preds = est.predict(df_test) df_test[\"predicted_probability\"] = preds  # print top 6 predicted probabilities df_test.head(6) Out[15]: default fico_score predicted_probability 1393 1 692 0.181684 3933 0 672 0.218686 5889 0 707 0.157267 9452 0 702 0.165095 5003 0 697 0.173233 8064 1 702 0.165095 In\u00a0[16]: Copied! <pre># test for 'predicted_probability &gt; 0.5, if yes assign will_default to 1, otherwise to 0\ndf_test[\"will_default\"] = np.where(df_test[\"predicted_probability\"] &gt; 0.25, 1, 0)\ndf_test.head(6)\nprint(df_test)\n</pre> # test for 'predicted_probability &gt; 0.5, if yes assign will_default to 1, otherwise to 0 df_test[\"will_default\"] = np.where(df_test[\"predicted_probability\"] &gt; 0.25, 1, 0) df_test.head(6) print(df_test) <pre>      default  fico_score  predicted_probability  will_default\n1393        1         692               0.181684             0\n3933        0         672               0.218686             0\n5889        0         707               0.157267             0\n9452        0         702               0.165095             0\n5003        0         697               0.173233             0\n...       ...         ...                    ...           ...\n8651        0         677               0.208953             0\n2222        0         672               0.218686             0\n7909        1         657               0.249813             0\n1047        0         742               0.110654             0\n6932        0         732               0.122576             0\n\n[2855 rows x 4 columns]\n</pre> In\u00a0[17]: Copied! <pre># Evaluation Metrics\n# print confusion matrix with labels\n\n# Plot the confusion matrix with the custom Seaborn-based colormap\ndisp = ConfusionMatrixDisplay.from_predictions(\n    df_test[\"default\"],\n    df_test[\"will_default\"],\n    display_labels=[\"No Default\", \"Default\"],\n    cmap=\"Blues\",\n)\n\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# print accuracy\nprint(\"Accuracy: \" + str(accuracy_score(df_test[\"default\"], df_test[\"will_default\"])))\n</pre> # Evaluation Metrics # print confusion matrix with labels  # Plot the confusion matrix with the custom Seaborn-based colormap disp = ConfusionMatrixDisplay.from_predictions(     df_test[\"default\"],     df_test[\"will_default\"],     display_labels=[\"No Default\", \"Default\"],     cmap=\"Blues\", )  plt.title(\"Confusion Matrix\") plt.show()  # print accuracy print(\"Accuracy: \" + str(accuracy_score(df_test[\"default\"], df_test[\"will_default\"]))) <pre>Accuracy: 0.8217162872154116\n</pre> In\u00a0[18]: Copied! <pre>matrix = confusion_matrix(df_test[\"default\"], df_test[\"will_default\"])\n\n# Normalize the matrix to get percentages\nnormalized_matrix = matrix / np.sum(matrix)\n\n# Create label overlay\nlabels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\nlabels = np.asarray(labels).reshape(2, 2)\n\n# Format labels with percentages + class names\nannot = np.empty_like(labels, dtype=object)\nfor i in range(2):\n    for j in range(2):\n        annot[i, j] = f\"{labels[i, j]}\\n{normalized_matrix[i, j]:.2%}\"\n\n# Plot heatmap with combined labels and percentages\nsns.heatmap(normalized_matrix, annot=annot, fmt=\"\", cmap=\"Blues\", cbar=False)\n\nplt.title(\"Confusion Matrix with Custom Labels\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.xticks([0.5, 1.5], [\"No Default\", \"Default\"])\nplt.yticks([0.5, 1.5], [\"No Default\", \"Default\"], rotation=0)\nplt.show()\n</pre> matrix = confusion_matrix(df_test[\"default\"], df_test[\"will_default\"])  # Normalize the matrix to get percentages normalized_matrix = matrix / np.sum(matrix)  # Create label overlay labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"] labels = np.asarray(labels).reshape(2, 2)  # Format labels with percentages + class names annot = np.empty_like(labels, dtype=object) for i in range(2):     for j in range(2):         annot[i, j] = f\"{labels[i, j]}\\n{normalized_matrix[i, j]:.2%}\"  # Plot heatmap with combined labels and percentages sns.heatmap(normalized_matrix, annot=annot, fmt=\"\", cmap=\"Blues\", cbar=False)  plt.title(\"Confusion Matrix with Custom Labels\") plt.xlabel(\"Predicted Label\") plt.ylabel(\"True Label\") plt.xticks([0.5, 1.5], [\"No Default\", \"Default\"]) plt.yticks([0.5, 1.5], [\"No Default\", \"Default\"], rotation=0) plt.show()"},{"location":"notebooks/mitxpro/03_prescriptive_model/#data-cleaning-eda-and-statistical-modeling-workbook","title":"Data Cleaning, EDA, and Statistical Modeling Workbook\u00b6","text":""},{"location":"notebooks/mitxpro/03_prescriptive_model/#introduction","title":"Introduction\u00b6","text":""},{"location":"notebooks/mitxpro/03_prescriptive_model/#confusion-matrix","title":"Confusion Matrix\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/","title":"4. Image Classification","text":"In\u00a0[2]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# initialize the seeds of different random number generators so that the\n# results will be the same every time the notebook is run\n# keras.utils.set_random_seed(42)\ntf.random.set_seed(42)\n</pre> import tensorflow as tf from tensorflow import keras import numpy as np import pandas as pd import matplotlib.pyplot as plt  # initialize the seeds of different random number generators so that the # results will be the same every time the notebook is run # keras.utils.set_random_seed(42) tf.random.set_seed(42) <p>With the technical preliminaries out of the way, let's load the dataset and take a look.</p> In\u00a0[3]: Copied! <pre># load data into x_train, y_train, x_test, y_test\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n</pre> # load data into x_train, y_train, x_test, y_test  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() <pre>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 0us/step\n</pre> In\u00a0[4]: Copied! <pre>print(x_train.shape, y_train.shape)\n</pre> print(x_train.shape, y_train.shape) <pre>(60000, 28, 28) (60000,)\n</pre> <p>There are 60,000 images in the training set, each of which is a 28x28 matrix.</p> In\u00a0[5]: Copied! <pre>print(x_test.shape, y_test.shape)\n</pre> print(x_test.shape, y_test.shape) <pre>(10000, 28, 28) (10000,)\n</pre> <p>The remaining 10,000 images are in the test set.</p> <p>OK, let's look at the first 10 rows of the dependent variable  \ud835\udc66 .</p> In\u00a0[6]: Copied! <pre>y_train[:10]\n</pre> y_train[:10] Out[6]: <pre>array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8)</pre> <p>What do these numbers mean?</p> <p>According to the fashion_mnist Github site, this is what each number 0-9 corresponds to:</p> Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot <p>Create a little Python list so that we can go from numbers to descriptions easily.</p> In\u00a0[7]: Copied! <pre># Call the list \"labels\"\n\nlabels = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n</pre> # Call the list \"labels\"  labels = [     \"T-shirt/top\",     \"Trouser\",     \"Pullover\",     \"Dress\",     \"Coat\",     \"Sandal\",     \"Shirt\",     \"Sneaker\",     \"Bag\",     \"Ankle boot\", ] <p>Given a number, the description is now a simple look-up. Let's see what the very first training example is about.</p> In\u00a0[8]: Copied! <pre>labels[y_train[0]]\n</pre> labels[y_train[0]] Out[8]: <pre>'Ankle boot'</pre> <p>The very first image is an \"Ankle boot\"!</p> <p>Let's take a look at the raw data for the image.</p> In\u00a0[9]: Copied! <pre>x_train[0]\n</pre> x_train[0] Out[9]: <pre>ndarray (28, 28) show data</pre><pre>array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n          1,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n          0,   3],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n         10,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n         72,  15],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n        172,  66],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n        229,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n        173,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n        202,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n        209,  52],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n        167,  56],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n         92,   0],\n       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n         77,   0],\n       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n        159,   0],\n       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n        215,   0],\n       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n        246,   0],\n       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n        225,   0],\n       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n        229,  29],\n       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n        230,  67],\n       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n        206, 115],\n       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n        210,  92],\n       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n        170,   0],\n       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]], dtype=uint8)</pre> <p>Let's look at the first 25 images using the handy <code>plt.imshow()</code> command</p> In\u00a0[10]: Copied! <pre># You'll create two variables namely \"fig\" and \"ax\" as shown in the screencast.\n\nfig = plt.figure(figsize=(30, 10))\nfor i in range(25):\n    ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n    ax.set_title(f\"{labels[y_train[i]]}\")\n    ax.imshow(x_train[i], cmap=\"gray\")\n</pre> # You'll create two variables namely \"fig\" and \"ax\" as shown in the screencast.  fig = plt.figure(figsize=(30, 10)) for i in range(25):     ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])     ax.set_title(f\"{labels[y_train[i]]}\")     ax.imshow(x_train[i], cmap=\"gray\") <p>The images are a bit small but they will do for now.</p> <p>Our first NN will be a simple one with a single hidden layer.</p> <p>Tip: NNs learn best when each independent variable is in a small range. So, standardize them by either</p> <ul> <li>subtracting the mean and dividing by the standard deviation or</li> <li>if they are in a guaranteed range, just divide by the max value.</li> </ul> <p>The inputs here range from 0 to 255. Let's normalize to the 0-1 range by dividing everything by 255.</p> In\u00a0[11]: Copied! <pre># Standardize x_train and x_test\n\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n</pre> # Standardize x_train and x_test  x_train = x_train / 255.0 x_test = x_test / 255.0 <p>As we saw in the previous module, creating an NN is usually just a few lines of Keras code.</p> <ul> <li>The input will be 28 x 28 matrices of numbers. These will have to be flattened into a long vector and then fed to the hidden layer.</li> <li>We will start with a single hidden layer of 256 ReLU neurons.</li> <li>Since this is a multi-class classification problem (e.g., we need to predict one of 10 clothing categories), the output layer has to produce a 10-element vector of probabilities that sum up to 1.0 =&gt; we will use the softmax layer that we learned about in the previous lecture.</li> </ul> In\u00a0[12]: Copied! <pre># define the input layer\ninput = keras.Input(shape=(28, 28))\n\n# convert the 28 x 28 matrix of numbers into a long vector\nh = keras.layers.Flatten()(input)\n\n# feed the long vector to the hidden layer\nh = keras.layers.Dense(256, activation=\"relu\", name=\"Hidden\")(h)\n\n# feed the output of the hidden layer to the output layer\noutput = keras.layers.Dense(10, activation=\"softmax\", name=\"Output\")(h)\n\n# tell Keras that this (input,output) pair is your model\nmodel = keras.Model(input, output)\n</pre> # define the input layer input = keras.Input(shape=(28, 28))  # convert the 28 x 28 matrix of numbers into a long vector h = keras.layers.Flatten()(input)  # feed the long vector to the hidden layer h = keras.layers.Dense(256, activation=\"relu\", name=\"Hidden\")(h)  # feed the output of the hidden layer to the output layer output = keras.layers.Dense(10, activation=\"softmax\", name=\"Output\")(h)  # tell Keras that this (input,output) pair is your model model = keras.Model(input, output) In\u00a0[13]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"functional\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer (InputLayer)        \u2502 (None, 28, 28)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatten (Flatten)               \u2502 (None, 784)            \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Hidden (Dense)                  \u2502 (None, 256)            \u2502       200,960 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Output (Dense)                  \u2502 (None, 10)             \u2502         2,570 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 203,530 (795.04 KB)\n</pre> <pre> Trainable params: 203,530 (795.04 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>Let's hand-calculate the number of parameters to verify.</p> In\u00a0[14]: Copied! <pre># calculate the number of parameters and set the output to \"parameters\"\n\nparameters = (784 * 256 + 256) + (256 * 10 + 10)\n\nprint(parameters)\n</pre> # calculate the number of parameters and set the output to \"parameters\"  parameters = (784 * 256 + 256) + (256 * 10 + 10)  print(parameters) <pre>203530\n</pre> <p>Now that the model is defined, we need to tell Keras three things:</p> <ul> <li>What loss function to use</li> <li>Which optimizer to use - we will again use Adam which is an excellent set-and-forget choice</li> <li>What metrics you want Keras to report out - in classification problems like this one, Accuracy is usually the metric you want to see.</li> </ul> <p>Since our output variable is categorical with 10 levels, we will select the <code>sparse_categorical_crossentropy</code> loss function.</p> In\u00a0[15]: Copied! <pre># Compile your model\nmodel.compile(\n    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n)\n</pre> # Compile your model model.compile(     loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) <ul> <li>The batch size: 32 or 64 are commonly used</li> <li>The number of epochs i.e., how many passes through the training data: start with 10-20.</li> </ul> <p>OK, let's train the model using the <code>model.fit</code> function!</p> In\u00a0[16]: Copied! <pre># fit your model first try with a batch size of 32 and 10 epochs\n\nbatch_size = 64\nepochs = 10\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n</pre> # fit your model first try with a batch size of 32 and 10 epochs  batch_size = 64 epochs = 10  model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs) <pre>Epoch 1/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 7ms/step - accuracy: 0.7800 - loss: 0.6253\nEpoch 2/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 6ms/step - accuracy: 0.8624 - loss: 0.3846\nEpoch 3/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 7ms/step - accuracy: 0.8775 - loss: 0.3395\nEpoch 4/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 7ms/step - accuracy: 0.8847 - loss: 0.3120\nEpoch 5/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 6ms/step - accuracy: 0.8923 - loss: 0.2913\nEpoch 6/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 6ms/step - accuracy: 0.8983 - loss: 0.2741\nEpoch 7/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 6ms/step - accuracy: 0.9045 - loss: 0.2588\nEpoch 8/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 6ms/step - accuracy: 0.9094 - loss: 0.2457\nEpoch 9/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 6ms/step - accuracy: 0.9140 - loss: 0.2345\nEpoch 10/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 6ms/step - accuracy: 0.9167 - loss: 0.2241\n</pre> Out[16]: <pre>&lt;keras.src.callbacks.history.History at 0x7a82668da9d0&gt;</pre> In\u00a0[17]: Copied! <pre># Evaluate model on test data set\nmodel.evaluate(x_test, y_test)\n</pre> # Evaluate model on test data set model.evaluate(x_test, y_test) <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.8790 - loss: 0.3467\n</pre> Out[17]: <pre>[0.3425934910774231, 0.8804000020027161]</pre> <p>Did the NNs we create take advantage of the fact that the input data is images?</p> <p>Convolutional Layers:</p> <p>Convolutional (typically abbreviated to \"conv\") layers were the key breakthrough that led to all the exciting advances in AI for Computer Vision problems like Image Classification, Image Recognition etc. They were designed to specifically work with images.</p> <p>Conv layers are the reason why your iPhone can recognize your face!</p> <p>We will follow the same sequence of steps as we did above:</p> <ul> <li>Data Prep</li> <li>Define Model</li> <li>Set Optimization Parameters</li> <li>Train Model</li> <li>Evaluate Model</li> </ul> <p>The data has already been normalized so that the numbers are between 0 and 1. We don't need to do it again.</p> In\u00a0[18]: Copied! <pre>x_train.shape\n</pre> x_train.shape Out[18]: <pre>(60000, 28, 28)</pre> <p>For reasons that will become clear when you work with color images, we also need to add another dimension to each example so that it goes from 28x28 to 28x28x1</p> In\u00a0[19]: Copied! <pre># add another dimension to x_train and x_test\n\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n</pre> # add another dimension to x_train and x_test  x_train = np.expand_dims(x_train, -1) x_test = np.expand_dims(x_test, -1) In\u00a0[20]: Copied! <pre>x_train.shape\n</pre> x_train.shape Out[20]: <pre>(60000, 28, 28, 1)</pre> <p>OK, we are ready to create our very first Convolutional Neural Network (CNN)!</p> In\u00a0[21]: Copied! <pre>input = keras.Input(shape=x_train.shape[1:])\n\n# first convolutional block\nx = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=\"relu\", name=\"Conv_1\")(\n    input\n)  # convolutional layer\nx = keras.layers.MaxPool2D()(x)  # pooling layer\n\n# second convolutional block\nx = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=\"relu\", name=\"Conv_2\")(\n    x\n)  # convolutional layer\nx = keras.layers.MaxPool2D()(x)  # pooling layer\n\n# Flatten the layers\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(256, activation=\"relu\")(x)\n\n# create an output called \"output\"\noutput = keras.layers.Dense(10, activation=\"softmax\")(x)\n\nmodel = keras.Model(input, output)\n</pre> input = keras.Input(shape=x_train.shape[1:])  # first convolutional block x = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=\"relu\", name=\"Conv_1\")(     input )  # convolutional layer x = keras.layers.MaxPool2D()(x)  # pooling layer  # second convolutional block x = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=\"relu\", name=\"Conv_2\")(     x )  # convolutional layer x = keras.layers.MaxPool2D()(x)  # pooling layer  # Flatten the layers x = keras.layers.Flatten()(x) x = keras.layers.Dense(256, activation=\"relu\")(x)  # create an output called \"output\" output = keras.layers.Dense(10, activation=\"softmax\")(x)  model = keras.Model(input, output) In\u00a0[22]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"functional_1\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer_1 (InputLayer)      \u2502 (None, 28, 28, 1)      \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Conv_1 (Conv2D)                 \u2502 (None, 27, 27, 32)     \u2502           160 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 max_pooling2d (MaxPooling2D)    \u2502 (None, 13, 13, 32)     \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Conv_2 (Conv2D)                 \u2502 (None, 12, 12, 32)     \u2502         4,128 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 max_pooling2d_1 (MaxPooling2D)  \u2502 (None, 6, 6, 32)       \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatten_1 (Flatten)             \u2502 (None, 1152)           \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense (Dense)                   \u2502 (None, 256)            \u2502       295,168 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 10)             \u2502         2,570 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 302,026 (1.15 MB)\n</pre> <pre> Trainable params: 302,026 (1.15 MB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[23]: Copied! <pre># Compile model using sparse_categorical_crossentropy\n# and adam, and accuracy as a metric\nmodel.compile(\n    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n)\n</pre> # Compile model using sparse_categorical_crossentropy # and adam, and accuracy as a metric model.compile(     loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) <p>DISCLAIMER: This will take some time to complete</p> In\u00a0[24]: Copied! <pre># Train the model with either 32 or 64 as the batch size and using 10 epochs\n\nmodel.fit(x_train, y_train, batch_size=64, epochs=10)\n</pre> # Train the model with either 32 or 64 as the batch size and using 10 epochs  model.fit(x_train, y_train, batch_size=64, epochs=10) <pre>Epoch 1/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39s 40ms/step - accuracy: 0.7675 - loss: 0.6546\nEpoch 2/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41s 40ms/step - accuracy: 0.8804 - loss: 0.3286\nEpoch 3/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41s 41ms/step - accuracy: 0.8973 - loss: 0.2795\nEpoch 4/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39s 39ms/step - accuracy: 0.9090 - loss: 0.2489\nEpoch 5/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40s 37ms/step - accuracy: 0.9168 - loss: 0.2249\nEpoch 6/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 36s 39ms/step - accuracy: 0.9252 - loss: 0.2046\nEpoch 7/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 36s 39ms/step - accuracy: 0.9318 - loss: 0.1847\nEpoch 8/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 37s 39ms/step - accuracy: 0.9387 - loss: 0.1667\nEpoch 9/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 37s 40ms/step - accuracy: 0.9453 - loss: 0.1496\nEpoch 10/10\n938/938 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40s 39ms/step - accuracy: 0.9510 - loss: 0.1346\n</pre> Out[24]: <pre>&lt;keras.src.callbacks.history.History at 0x7a82670ce110&gt;</pre> In\u00a0[25]: Copied! <pre># Get the score of the model\nscore = model.evaluate(x_test, y_test)\nprint(\"Test accuracy:\", score[1])\n</pre> # Get the score of the model score = model.evaluate(x_test, y_test) print(\"Test accuracy:\", score[1]) <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 7ms/step - accuracy: 0.9022 - loss: 0.3132\nTest accuracy: 0.9072999954223633\n</pre> <p>Back to Fashion MNIST. Let's see what the state of the art (SOTA) accuracy is.</p> <p>It is 96.91%!</p> <p>Challenge: Can you get to SOTA by playing around with the architecture of the network?</p>"},{"location":"notebooks/mitxpro/04_image_classification/#learning-an-image-classification-model-from-scratch","title":"Learning an Image Classification Model from Scratch\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#introduction","title":"Introduction\u00b6","text":"<p>The fashion_mnist dataset consists of 70,000 images of clothing items across 10 categories.</p> <p></p> <p>Luckily for us, this dataset is available in a convenient format through Keras, so we will load it and take a look.</p> <p>But first, let's get the usual technical preliminaries out of the way.</p> <p>As we did previously, we will first import the following packages and set the seed for the random number generator.</p>"},{"location":"notebooks/mitxpro/04_image_classification/#a-nn-model-first-attempt","title":"A NN Model - First Attempt\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#data-prep","title":"Data Prep\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#define-model-in-keras","title":"Define Model in Keras\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#set-optimization-parameters","title":"Set Optimization Parameters\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#train-the-model","title":"Train the Model\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#evaluate-the-model","title":"Evaluate the Model\u00b6","text":"<p>You can see from the above that our model achieves over 91% accuracy on the train set but, as we know, doing well on the training set isn't all that impressive due to the possibility of overfitting. So the real question is how well does it do on the test set?</p> <p><code>model.evaluate</code> is a very handy function to calculate the performance of your model on any dataset.</p>"},{"location":"notebooks/mitxpro/04_image_classification/#a-convolutional-neural-network","title":"A Convolutional Neural Network\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#data-prep","title":"Data Prep\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#define-model","title":"Define Model\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#set-optimization-parameters","title":"Set Optimization Parameters\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#train-the-model","title":"Train the Model\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#evaluate-the-model","title":"Evaluate the Model\u00b6","text":""},{"location":"notebooks/mitxpro/04_image_classification/#conclusion","title":"Conclusion\u00b6","text":"<p>We have built a Deep Learning model that can classify grayscale images of clothing items with over 90% accuracy!!</p>"},{"location":"notebooks/mitxpro/05_XGBoost/","title":"5. Predictive Modeling","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install xgboost==1.6.1\n</pre> !pip install xgboost==1.6.1 In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nimport xgboost\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt  from datetime import datetime import xgboost from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.metrics import auc from sklearn.metrics import roc_auc_score from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score   import warnings  warnings.simplefilter(action=\"ignore\", category=UserWarning) <p>Using the LendingClub loans dataset.</p> In\u00a0[3]: Copied! <pre>url = \"https://docs.google.com/spreadsheets/d/10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2/export?format=csv&amp;id=10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2&amp;gid=1710894028\"\ndf = pd.read_csv(url, index_col=False)\n</pre> url = \"https://docs.google.com/spreadsheets/d/10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2/export?format=csv&amp;id=10L8BpkV4q1Zsou4daYoWul_8PFA9rsv2&amp;gid=1710894028\" df = pd.read_csv(url, index_col=False) In\u00a0[4]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9516 entries, 0 to 9515\nData columns (total 7 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   default      9516 non-null   int64  \n 1   installment  9516 non-null   int64  \n 2   log_income   9516 non-null   float64\n 3   fico_score   9516 non-null   int64  \n 4   rev_balance  9516 non-null   float64\n 5   inquiries    9516 non-null   int64  \n 6   records      9516 non-null   int64  \ndtypes: float64(2), int64(5)\nmemory usage: 520.5 KB\n</pre> In\u00a0[5]: Copied! <pre>df.head(6)\n</pre> df.head(6) Out[5]: default installment log_income fico_score rev_balance inquiries records 0 0 829 4.93 737 28.85 0 0 1 0 228 4.81 707 33.62 0 0 2 0 367 4.51 682 3.51 1 0 3 0 162 4.93 712 33.67 1 0 4 0 103 4.91 667 4.74 0 0 5 0 125 5.17 727 50.81 0 0 In\u00a0[6]: Copied! <pre>df.default.value_counts(normalize=True)\n</pre> df.default.value_counts(normalize=True) Out[6]: proportion default 0 0.840164 1 0.159836 dtype: float64 <p>Let's split the data 70/30 into a training set (which we will use to build models) and a test set (on which we will evaluate any model we build).</p> In\u00a0[7]: Copied! <pre>X = df.drop([\"default\"], axis=1)\ny = df[\"default\"]\n\n\n# Encode string class values as integers to avoid errors in newer versions of XGBoost\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\ny = label_encoder.transform(y)\n\n\n# Splitting data into training and test set:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\neval_set = [(X_test, y_test)]\nprint(X_train.shape, X_test.shape)\n</pre> X = df.drop([\"default\"], axis=1) y = df[\"default\"]   # Encode string class values as integers to avoid errors in newer versions of XGBoost from sklearn.preprocessing import LabelEncoder  label_encoder = LabelEncoder() label_encoder = label_encoder.fit(y) y = label_encoder.transform(y)   # Splitting data into training and test set: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7) eval_set = [(X_test, y_test)] print(X_train.shape, X_test.shape) <pre>(6661, 6) (2855, 6)\n</pre> In\u00a0[8]: Copied! <pre>print(\"Initializing xgboost.sklearn.XGBClassifier and starting training...\")\n\nst = datetime.now()\n\nclf = xgboost.sklearn.XGBClassifier(\n    objective=\"binary:logistic\",\n    learning_rate=0.05,\n    seed=9616,\n    max_depth=20,\n    gamma=10,\n    n_estimators=500,\n)\n\n\nclf.fit(\n    X_train,\n    y_train,\n    eval_set=eval_set,\n    eval_metric=\"auc\",\n    early_stopping_rounds=20,\n    verbose=False,\n)\n\nprint(f\"Training time: {datetime.now() - st}\")\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\nprint(datetime.now() - st)\n\naccuracy = accuracy_score(np.array(y_test).flatten(), y_pred)\nprint(\"Accuracy: %.10f%%\" % (accuracy * 100.0))\n\naccuracy_per_roc_auc = roc_auc_score(np.array(y_test).flatten(), y_pred)\nprint(\"ROC-AUC: %.10f%%\" % (accuracy_per_roc_auc * 100))\n</pre> print(\"Initializing xgboost.sklearn.XGBClassifier and starting training...\")  st = datetime.now()  clf = xgboost.sklearn.XGBClassifier(     objective=\"binary:logistic\",     learning_rate=0.05,     seed=9616,     max_depth=20,     gamma=10,     n_estimators=500, )   clf.fit(     X_train,     y_train,     eval_set=eval_set,     eval_metric=\"auc\",     early_stopping_rounds=20,     verbose=False, )  print(f\"Training time: {datetime.now() - st}\")  # Make predictions y_pred = clf.predict(X_test)  print(datetime.now() - st)  accuracy = accuracy_score(np.array(y_test).flatten(), y_pred) print(\"Accuracy: %.10f%%\" % (accuracy * 100.0))  accuracy_per_roc_auc = roc_auc_score(np.array(y_test).flatten(), y_pred) print(\"ROC-AUC: %.10f%%\" % (accuracy_per_roc_auc * 100)) <pre>Initializing xgboost.sklearn.XGBClassifier and starting training...\nTraining time: 0:00:07.489654\n0:00:07.497859\nAccuracy: 83.4325744308%\nROC-AUC: 50.0000000000%\n</pre> In\u00a0[9]: Copied! <pre># Remember: The F score is based on how often a feature is used to split the data across all trees in the model, so this gives you a relative sense of importance, not causality.\n\nxgboost.plot_importance(clf)\n</pre> # Remember: The F score is based on how often a feature is used to split the data across all trees in the model, so this gives you a relative sense of importance, not causality.  xgboost.plot_importance(clf) Out[9]: <pre>&lt;Axes: title={'center': 'Feature importance'}, xlabel='F score', ylabel='Features'&gt;</pre> <p>1. Top Predictive Features:</p> <p><code>fico_score</code> is by far the most important feature (F score: 83), suggesting that the model heavily relies on creditworthiness when predicting the target (or likely default).</p> <p><code>installment</code> (72) and <code>rev_balance</code> (58) are also strongly predictive \u2014 indicating that loan repayment terms and revolving balance significantly influence the model's decision-making.</p> <p>2. Moderately Important Features:</p> <p><code>inquiries</code> (52) and <code>log_income</code> (47) contribute meaningfully, possibly capturing borrower activity and financial capability.</p> <p>3. Low Importance Feature:</p> <p><code>records</code> (11) contributes very little to the model. This might mean it either has little variance or isn\u2019t strongly correlated with default risk.</p>"},{"location":"notebooks/mitxpro/05_XGBoost/#predictive-modeling-with-xgboost","title":"Predictive Modeling with XGBoost\u00b6","text":""},{"location":"notebooks/mitxpro/05_XGBoost/#introduction","title":"Introduction\u00b6","text":"<p>An end-to-end workflow using NumPy, Pandas, Matplotlib, and XGBoost to evaluate model performance with ROC AUC, accuracy, and regression metrics.</p>"},{"location":"notebooks/mitxpro/05_XGBoost/#module-import","title":"Module Import\u00b6","text":""},{"location":"notebooks/mitxpro/05_XGBoost/#dataset-import","title":"Dataset Import\u00b6","text":""},{"location":"notebooks/mitxpro/05_XGBoost/#training-and-test-datasets","title":"Training and Test Datasets\u00b6","text":""},{"location":"notebooks/mitxpro/05_XGBoost/#model-interpretation","title":"Model Interpretation:\u00b6","text":""},{"location":"notebooks/mitxpro/about/","title":"MIT xPRO","text":"<p>This portfolio showcases projects and insights from the MIT xPRO Professional Certificate in Data Science and Analytics program.</p> <p>This program emphasized hands-on learning and real-world applications, culminating in projects that address current challenges across industries.</p> <p>You can learn more about MIT xPRO courses and programs here.</p>"},{"location":"notebooks/mitxpro/about/#about-the-program","title":"About the Program","text":"<p>The MIT xPRO Professional Certificate in Data Science and Analytics is a rigorous, industry-aligned curriculum designed to develop advanced skills in:</p> <ul> <li>Statistical Analysis: Leveraging data to identify trends, correlations, and actionable insights.</li> <li>Machine Learning: Building predictive models and applying cutting-edge algorithms to solve complex problems.</li> <li>Data Visualization: Communicating data-driven insights effectively using compelling visual narratives.</li> <li>Programming and Tools: Proficiency in <code>Python</code>, <code>R</code>, <code>SQL</code>, and other data-centric tools to manipulate, analyze, and present data.</li> </ul>"},{"location":"notebooks/other/integrated_gradients/","title":"Integrated Gradients","text":"View on TensorFlow.org Run in Google Colab View on GitHub Download notebook See TF Hub model <p>This tutorial demonstrates how to implement Integrated Gradients (IG), an Explainable AI technique introduced in the paper Axiomatic Attribution for Deep Networks. IG aims to explain the relationship between a model's predictions in terms of its features. It has many use cases including understanding feature importances, identifying data skew, and debugging model performance.</p> <p>IG has become a popular interpretability technique due to its broad applicability to any differentiable model (e.g. images, text, structured data), ease of implementation, theoretical justifications, and computational efficiency relative to alternative approaches that allow it to scale to large networks and feature spaces such as images.</p> <p>In this tutorial, you will walk through an implementation of IG step-by-step to understand the pixel feature importances of an image classifier. As an example, consider this image of a fireboat spraying jets of water. You would classify this image as a fireboat and might highlight the pixels making up the boat and water cannons as being important to your decision. Your model will also classify this image as a fireboat later on in this tutorial; however, does it highlight the same pixels as important when explaining its decision?</p> <p>In the images below titled \"IG Attribution Mask\" and \"Original + IG Mask Overlay\" you can see that your model instead highlights (in purple) the pixels comprising the boat's water cannons and jets of water as being more important than the boat itself to its decision. How will your model generalize to new fireboats? What about fireboats without water jets? Read on to learn more about how IG works and how to apply IG to your models to better understand the relationship between their predictions and underlying features.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pylab as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n</pre> import matplotlib.pylab as plt import numpy as np import tensorflow as tf import tensorflow_hub as hub <p>IG can be applied to any differentiable model. In the spirit of the original paper, you will use a pre-trained version of the same model, Inception V1, which you will download from TensorFlow Hub.</p> In\u00a0[\u00a0]: Copied! <pre># model = tf.keras.Sequential([\n#     hub.KerasLayer(\n#         name='inception_v1',\n#         handle='https://tfhub.dev/google/imagenet/inception_v1/classification/4',\n#         trainable=False),\n# ])\n# model.build([None, 224, 224, 3])\n# model.summary()\n\n\n# Input layer\ninputs = tf.keras.Input(shape=(224, 224, 3), dtype=tf.float32)\n\n# Wrap hub layer in a Lambda to avoid the symbolic error\nhub_layer = tf.keras.layers.Lambda(\n    lambda x: hub.KerasLayer(\n        \"https://tfhub.dev/google/imagenet/inception_v1/classification/4\",\n        trainable=False,\n    )(x)\n)(inputs)\n\n# Model\nmodel = tf.keras.Model(inputs=inputs, outputs=hub_layer)\n\n# Confirm model structure\nmodel.summary()\n</pre> # model = tf.keras.Sequential([ #     hub.KerasLayer( #         name='inception_v1', #         handle='https://tfhub.dev/google/imagenet/inception_v1/classification/4', #         trainable=False), # ]) # model.build([None, 224, 224, 3]) # model.summary()   # Input layer inputs = tf.keras.Input(shape=(224, 224, 3), dtype=tf.float32)  # Wrap hub layer in a Lambda to avoid the symbolic error hub_layer = tf.keras.layers.Lambda(     lambda x: hub.KerasLayer(         \"https://tfhub.dev/google/imagenet/inception_v1/classification/4\",         trainable=False,     )(x) )(inputs)  # Model model = tf.keras.Model(inputs=inputs, outputs=hub_layer)  # Confirm model structure model.summary() <p>From the module page, you need to keep in mind the following about Inception V1:</p> <p>Inputs: The expected input shape for the model is <code>(None, 224, 224, 3)</code>. This is a dense 4D tensor of dtype float32 and shape <code>(batch_size, height, width, RGB channels)</code> whose elements are RGB color values of pixels normalized to the range [0, 1]. The first element is <code>None</code> to indicate that the model can take any integer batch size.</p> <p>Outputs: A <code>tf.Tensor</code> of logits in the shape of <code>(batch_size, 1001)</code>. Each row represents the model's predicted score for 1,001 classes from ImageNet. For the model's top predicted class index you can use <code>tf.math.argmax(predictions, axis=-1)</code>. Furthermore, you can also convert the model's logit output to predicted probabilities across all classes using <code>tf.nn.softmax(predictions, axis=-1)</code> to quantify the model's uncertainty and explore similar predicted classes for debugging.</p> In\u00a0[\u00a0]: Copied! <pre>def load_imagenet_labels(file_path):\n    labels_file = tf.keras.utils.get_file(\"ImageNetLabels.txt\", file_path)\n    with open(labels_file) as reader:\n        f = reader.read()\n        labels = f.splitlines()\n    return np.array(labels)\n</pre> def load_imagenet_labels(file_path):     labels_file = tf.keras.utils.get_file(\"ImageNetLabels.txt\", file_path)     with open(labels_file) as reader:         f = reader.read()         labels = f.splitlines()     return np.array(labels) In\u00a0[\u00a0]: Copied! <pre>imagenet_labels = load_imagenet_labels(\n    \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n)\n</pre> imagenet_labels = load_imagenet_labels(     \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\" ) In\u00a0[\u00a0]: Copied! <pre>def read_image(file_name):\n    image = tf.io.read_file(file_name)\n    image = tf.io.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n    return image\n</pre> def read_image(file_name):     image = tf.io.read_file(file_name)     image = tf.io.decode_jpeg(image, channels=3)     image = tf.image.convert_image_dtype(image, tf.float32)     image = tf.image.resize_with_pad(image, target_height=224, target_width=224)     return image In\u00a0[\u00a0]: Copied! <pre>img_url = {\n    \"Fireboat\": \"http://storage.googleapis.com/download.tensorflow.org/example_images/San_Francisco_fireboat_showing_off.jpg\",\n    \"Sailboat\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Sailing_boat_on_water.jpg/800px-Sailing_boat_on_water.jpg?20130301043125\",\n    \"Coyote\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Canis_latrans_%28Yosemite%2C_2009%29.jpg/800px-Canis_latrans_%28Yosemite%2C_2009%29.jpg?20130211174745\",\n    \"Coyote-2\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Coyote_closeup.jpg/960px-Coyote_closeup.jpg?20041212220622\",\n    \"Dino-2\": \"https://upload.wikimedia.org/wikipedia/commons/3/33/Stan_the_Trex_at_Manchester_Museum.jpg\",\n}\n\nimg_paths = {\n    name: tf.keras.utils.get_file(name, url) for (name, url) in img_url.items()\n}\nimg_name_tensors = {\n    name: read_image(img_path) for (name, img_path) in img_paths.items()\n}\n</pre> img_url = {     \"Fireboat\": \"http://storage.googleapis.com/download.tensorflow.org/example_images/San_Francisco_fireboat_showing_off.jpg\",     \"Sailboat\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Sailing_boat_on_water.jpg/800px-Sailing_boat_on_water.jpg?20130301043125\",     \"Coyote\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Canis_latrans_%28Yosemite%2C_2009%29.jpg/800px-Canis_latrans_%28Yosemite%2C_2009%29.jpg?20130211174745\",     \"Coyote-2\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Coyote_closeup.jpg/960px-Coyote_closeup.jpg?20041212220622\",     \"Dino-2\": \"https://upload.wikimedia.org/wikipedia/commons/3/33/Stan_the_Trex_at_Manchester_Museum.jpg\", }  img_paths = {     name: tf.keras.utils.get_file(name, url) for (name, url) in img_url.items() } img_name_tensors = {     name: read_image(img_path) for (name, img_path) in img_paths.items() } In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 8))\nfor n, (name, img_tensors) in enumerate(img_name_tensors.items()):\n    ax = plt.subplot(1, 5, n + 1)\n    ax.imshow(img_tensors)\n    ax.set_title(name)\n    ax.axis(\"off\")\nplt.tight_layout()\n</pre> plt.figure(figsize=(8, 8)) for n, (name, img_tensors) in enumerate(img_name_tensors.items()):     ax = plt.subplot(1, 5, n + 1)     ax.imshow(img_tensors)     ax.set_title(name)     ax.axis(\"off\") plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>def top_k_predictions(img, k=3):\n    image_batch = tf.expand_dims(img, 0)\n    predictions = model(image_batch)\n    probs = tf.nn.softmax(predictions, axis=-1)\n    top_probs, top_idxs = tf.math.top_k(input=probs, k=k)\n    top_labels = imagenet_labels[tuple(top_idxs)]\n    return top_labels, top_probs[0]\n</pre> def top_k_predictions(img, k=3):     image_batch = tf.expand_dims(img, 0)     predictions = model(image_batch)     probs = tf.nn.softmax(predictions, axis=-1)     top_probs, top_idxs = tf.math.top_k(input=probs, k=k)     top_labels = imagenet_labels[tuple(top_idxs)]     return top_labels, top_probs[0] In\u00a0[\u00a0]: Copied! <pre>for name, img_tensor in img_name_tensors.items():\n    plt.imshow(img_tensor)\n    plt.title(name, fontweight=\"bold\")\n    plt.axis(\"off\")\n    plt.show()\n\n    pred_label, pred_prob = top_k_predictions(img_tensor)\n    for label, prob in zip(pred_label, pred_prob):\n        print(f\"{label}: {prob:0.1%}\")\n</pre> for name, img_tensor in img_name_tensors.items():     plt.imshow(img_tensor)     plt.title(name, fontweight=\"bold\")     plt.axis(\"off\")     plt.show()      pred_label, pred_prob = top_k_predictions(img_tensor)     for label, prob in zip(pred_label, pred_prob):         print(f\"{label}: {prob:0.1%}\") <p>Your model, Inception V1, is a learned function that describes a mapping between your input feature space, image pixel values, and an output space defined by ImageNet class probability values between 0 and 1. Early interpretability methods for neural networks assigned feature importance scores using gradients, which tell you which pixels have the steepest local relative to your model's prediction at a given point along your model's prediction function. However, gradients only describe local changes in your model's prediction function with respect to pixel values and do not fully describe your entire model prediction function. As your model fully \"learns\" the relationship between the range of an individual pixel and the correct ImageNet class, the gradient for this pixel will saturate, meaning become increasingly small and even go to zero. Consider the simple model function below:</p> In\u00a0[\u00a0]: Copied! <pre>def f(x):\n    \"\"\"A simplified model function.\"\"\"\n    return tf.where(x &lt; 0.8, x, 0.8)\n\n\ndef interpolated_path(x):\n    \"\"\"A straight line path.\"\"\"\n    return tf.zeros_like(x)\n\n\nx = tf.linspace(start=0.0, stop=1.0, num=6)\ny = f(x)\n</pre> def f(x):     \"\"\"A simplified model function.\"\"\"     return tf.where(x &lt; 0.8, x, 0.8)   def interpolated_path(x):     \"\"\"A straight line path.\"\"\"     return tf.zeros_like(x)   x = tf.linspace(start=0.0, stop=1.0, num=6) y = f(x) In\u00a0[\u00a0]: Copied! <pre># @title\nfig = plt.figure(figsize=(12, 5))\nax0 = fig.add_subplot(121)\nax0.plot(x, f(x), marker=\"o\")\nax0.set_title(\"Gradients saturate over F(x)\", fontweight=\"bold\")\nax0.text(0.2, 0.5, \"Gradients &gt; 0 = \\n x is important\")\nax0.text(0.7, 0.85, \"Gradients = 0 \\n x not important\")\nax0.set_yticks(tf.range(0, 1.5, 0.5))\nax0.set_xticks(tf.range(0, 1.5, 0.5))\nax0.set_ylabel(\"F(x) - model true class predicted probability\")\nax0.set_xlabel(\"x - (pixel value)\")\n\nax1 = fig.add_subplot(122)\nax1.plot(x, f(x), marker=\"o\")\nax1.plot(x, interpolated_path(x), marker=\"&gt;\")\nax1.set_title(\"IG intuition\", fontweight=\"bold\")\nax1.text(0.25, 0.1, \"Accumulate gradients along path\")\nax1.set_ylabel(\"F(x) - model true class predicted probability\")\nax1.set_xlabel(\"x - (pixel value)\")\nax1.set_yticks(tf.range(0, 1.5, 0.5))\nax1.set_xticks(tf.range(0, 1.5, 0.5))\nax1.annotate(\n    \"Baseline\",\n    xy=(0.0, 0.0),\n    xytext=(0.0, 0.2),\n    arrowprops=dict(facecolor=\"black\", shrink=0.1),\n)\nax1.annotate(\n    \"Input\",\n    xy=(1.0, 0.0),\n    xytext=(0.95, 0.2),\n    arrowprops=dict(facecolor=\"black\", shrink=0.1),\n)\nplt.show();\n</pre> # @title fig = plt.figure(figsize=(12, 5)) ax0 = fig.add_subplot(121) ax0.plot(x, f(x), marker=\"o\") ax0.set_title(\"Gradients saturate over F(x)\", fontweight=\"bold\") ax0.text(0.2, 0.5, \"Gradients &gt; 0 = \\n x is important\") ax0.text(0.7, 0.85, \"Gradients = 0 \\n x not important\") ax0.set_yticks(tf.range(0, 1.5, 0.5)) ax0.set_xticks(tf.range(0, 1.5, 0.5)) ax0.set_ylabel(\"F(x) - model true class predicted probability\") ax0.set_xlabel(\"x - (pixel value)\")  ax1 = fig.add_subplot(122) ax1.plot(x, f(x), marker=\"o\") ax1.plot(x, interpolated_path(x), marker=\"&gt;\") ax1.set_title(\"IG intuition\", fontweight=\"bold\") ax1.text(0.25, 0.1, \"Accumulate gradients along path\") ax1.set_ylabel(\"F(x) - model true class predicted probability\") ax1.set_xlabel(\"x - (pixel value)\") ax1.set_yticks(tf.range(0, 1.5, 0.5)) ax1.set_xticks(tf.range(0, 1.5, 0.5)) ax1.annotate(     \"Baseline\",     xy=(0.0, 0.0),     xytext=(0.0, 0.2),     arrowprops=dict(facecolor=\"black\", shrink=0.1), ) ax1.annotate(     \"Input\",     xy=(1.0, 0.0),     xytext=(0.95, 0.2),     arrowprops=dict(facecolor=\"black\", shrink=0.1), ) plt.show(); <ul> <li><p>left: Your model's gradients for pixel <code>x</code> are positive between 0.0 and 0.8 but go to 0.0 between 0.8 and 1.0. Pixel <code>x</code> clearly has a significant impact on pushing your model toward 80% predicted probability on the true class. Does it make sense that pixel <code>x</code>'s importance is small or discontinuous?</p> </li> <li><p>right: The intuition behind IG is to accumulate pixel <code>x</code>'s local gradients and attribute its importance as a score for how much it adds or subtracts to your model's overall output class probability. You can break down and compute IG in 3 parts:</p> <ol> <li>interpolate small steps along a straight line in the feature space between 0 (a baseline or starting point) and 1 (input pixel's value)</li> <li>compute gradients at each step between your model's predictions with respect to each step</li> <li>approximate the integral between your baseline and input by accumulating (cumulative average) these local gradients.</li> </ol> </li> </ul> <p>To reinforce this intuition, you will walk through these 3 parts by applying IG to the example \"Fireboat\" image below.</p> <p>A baseline is an input image used as a starting point for calculating feature importance. Intuitively, you can think of the baseline's explanatory role as representing the impact of the absence of each pixel on the \"Fireboat\" prediction to contrast with its impact of each pixel on the \"Fireboat\" prediction when present in the input image. As a result, the choice of the baseline plays a central role in interpreting and visualizing pixel feature importances. For additional discussion of baseline selection, see the resources in the \"Next steps\" section at the bottom of this tutorial. Here, you will use a black image whose pixel values are all zero.</p> <p>Other choices you could experiment with include an all white image, or a random image, which you can create with <code>tf.random.uniform(shape=(224,224,3), minval=0.0, maxval=1.0)</code>.</p> In\u00a0[\u00a0]: Copied! <pre>baseline = tf.zeros(shape=(224, 224, 3))\n</pre> baseline = tf.zeros(shape=(224, 224, 3)) In\u00a0[\u00a0]: Copied! <pre>plt.imshow(baseline)\nplt.title(\"Baseline\")\nplt.axis(\"off\")\nplt.show()\n</pre> plt.imshow(baseline) plt.title(\"Baseline\") plt.axis(\"off\") plt.show() <p>The formula for Integrated Gradients is as follows:</p> <p>$IntegratedGradients_{i}(x) ::= (x_{i} - x'_{i})\\times\\int_{\\alpha=0}^1\\frac{\\partial F(x'+\\alpha \\times (x - x'))}{\\partial x_i}{d\\alpha}$</p> <p>where:</p> <p>$_{i}$ = feature $x$ = input $x'$ = baseline $\\alpha$ = interpolation constant to perturb features by</p> <p>In practice, computing a definite integral is not always numerically possible and can be computationally costly, so you compute the following numerical approximation:</p> <p>$IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\\times\\sum_{k=1}^{m}\\frac{\\partial F(x' + \\frac{k}{m}\\times(x - x'))}{\\partial x_{i}} \\times \\frac{1}{m}$</p> <p>where:</p> <p>$_{i}$ = feature (individual pixel) $x$ = input (image tensor) $x'$ = baseline (image tensor) $k$ = scaled feature perturbation constant $m$ = number of steps in the Riemann sum approximation of the integral $(x_{i}-x'_{i})$ = a term for the difference from the baseline. This is necessary to scale the integrated gradients and keep them in terms of the original image. The path from the baseline image to the input is in pixel space. Since with IG you are integrating in a straight line (linear transformation) this ends up being roughly equivalent to the integral term of the derivative of the interpolated image function with respect to $\\alpha$ with enough steps. The integral sums each pixel's gradient times the change in the pixel along the path. It's simpler to implement this integration as uniform steps from one image to the other, substituting $x := (x' + \\alpha(x-x'))$. So the change of variables gives $dx = (x-x')d\\alpha$. The $(x-x')$ term is constant and is factored out of the integral.</p> <p>$IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\\times\\sum_{k=1}^{m}\\frac{\\partial F(\\overbrace{x' + \\frac{k}{m}\\times(x - x')}^\\text{interpolate m images at k intervals})}{\\partial x_{i}} \\times \\frac{1}{m}$</p> <p>First, you will generate a linear interpolation between the baseline and the original image. You can think of interpolated images as small steps in the feature space between your baseline and input, represented by $\\alpha$ in the original equation.</p> In\u00a0[\u00a0]: Copied! <pre>m_steps = 50\nalphas = tf.linspace(\n    start=0.0, stop=1.0, num=m_steps + 1\n)  # Generate m_steps intervals for integral_approximation() below.\n</pre> m_steps = 50 alphas = tf.linspace(     start=0.0, stop=1.0, num=m_steps + 1 )  # Generate m_steps intervals for integral_approximation() below. In\u00a0[\u00a0]: Copied! <pre>def interpolate_images(baseline, image, alphas):\n    alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n    baseline_x = tf.expand_dims(baseline, axis=0)\n    input_x = tf.expand_dims(image, axis=0)\n    delta = input_x - baseline_x\n    images = baseline_x + alphas_x * delta\n    return images\n</pre> def interpolate_images(baseline, image, alphas):     alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]     baseline_x = tf.expand_dims(baseline, axis=0)     input_x = tf.expand_dims(image, axis=0)     delta = input_x - baseline_x     images = baseline_x + alphas_x * delta     return images <p>Use the above function to generate interpolated images along a linear path at alpha intervals between a black baseline image and the example \"Fireboat\" image.</p> In\u00a0[\u00a0]: Copied! <pre>interpolated_images = interpolate_images(\n    baseline=baseline, image=img_name_tensors[\"Fireboat\"], alphas=alphas\n)\n</pre> interpolated_images = interpolate_images(     baseline=baseline, image=img_name_tensors[\"Fireboat\"], alphas=alphas ) <p>Visualize the interpolated images. Note: another way of thinking about the $\\alpha$ constant is that it is consistently increasing each interpolated image's intensity.</p> In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(20, 20))\n\ni = 0\nfor alpha, image in zip(alphas[0::10], interpolated_images[0::10]):\n    i += 1\n    plt.subplot(1, len(alphas[0::10]), i)\n    plt.title(f\"alpha: {alpha:.1f}\")\n    plt.imshow(image)\n    plt.axis(\"off\")\n\nplt.tight_layout();\n</pre> fig = plt.figure(figsize=(20, 20))  i = 0 for alpha, image in zip(alphas[0::10], interpolated_images[0::10]):     i += 1     plt.subplot(1, len(alphas[0::10]), i)     plt.title(f\"alpha: {alpha:.1f}\")     plt.imshow(image)     plt.axis(\"off\")  plt.tight_layout(); <p>This section explains how to calculate the gradients to measure the relationship between changes to a feature and changes in the model's predictions. In the case of images, the gradient tells us which pixels have the strongest effect on the model's predicted class probabilities.</p> <p>$IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\\times\\sum_{k=1}^{m}\\frac{\\overbrace{\\partial F(\\text{interpolated images})}^\\text{compute gradients}}{\\partial x_{i}} \\times \\frac{1}{m}$</p> <p>where: $F()$ = your model's prediction function $\\frac{\\partial{F}}{\\partial{x_i}}$ = gradient (vector of partial derivatives $\\partial$) of your model F's prediction function relative to each feature $x_i$</p> <p>TensorFlow makes computing gradients easy for you with a <code>tf.GradientTape</code>.</p> In\u00a0[\u00a0]: Copied! <pre>def compute_gradients(images, target_class_idx):\n    with tf.GradientTape() as tape:\n        tape.watch(images)\n        logits = model(images)\n        probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n    return tape.gradient(probs, images)\n</pre> def compute_gradients(images, target_class_idx):     with tf.GradientTape() as tape:         tape.watch(images)         logits = model(images)         probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]     return tape.gradient(probs, images) <p>Compute the gradients for each image along the interpolation path with respect to the correct output. Recall that your model returns a <code>(1, 1001)</code> shaped <code>Tensor</code> with logits that you convert to predicted probabilities for each class. You need to pass the correct ImageNet target class index to the <code>compute_gradients</code> function for your image.</p> In\u00a0[\u00a0]: Copied! <pre>path_gradients = compute_gradients(images=interpolated_images, target_class_idx=555)\n</pre> path_gradients = compute_gradients(images=interpolated_images, target_class_idx=555) <p>Note the output shape of <code>(n_interpolated_images, img_height, img_width, RGB)</code>, which gives us the gradient for every pixel of every image along the interpolation path. You can think of these gradients as measuring the change in your model's predictions for each small step in the feature space.</p> In\u00a0[\u00a0]: Copied! <pre>print(path_gradients.shape)\n</pre> print(path_gradients.shape) <p>Visualizing gradient saturation</p> <p>Recall that the gradients you just calculated above describe local changes to your model's predicted probability of \"Fireboat\" and can saturate.</p> <p>These concepts are visualized using the gradients you calculated above in the 2 plots below.</p> In\u00a0[\u00a0]: Copied! <pre>pred = model(interpolated_images)\npred_proba = tf.nn.softmax(pred, axis=-1)[:, 555]\n</pre> pred = model(interpolated_images) pred_proba = tf.nn.softmax(pred, axis=-1)[:, 555] In\u00a0[\u00a0]: Copied! <pre># @title\nplt.figure(figsize=(10, 4))\nax1 = plt.subplot(1, 2, 1)\nax1.plot(alphas, pred_proba)\nax1.set_title(\"Target class predicted probability over alpha\")\nax1.set_ylabel(\"model p(target class)\")\nax1.set_xlabel(\"alpha\")\nax1.set_ylim([0, 1])\n\nax2 = plt.subplot(1, 2, 2)\n# Average across interpolation steps\naverage_grads = tf.reduce_mean(path_gradients, axis=[1, 2, 3])\n# Normalize gradients to 0 to 1 scale. E.g., (x - min(x))/(max(x)-min(x))\naverage_grads_norm = (average_grads - tf.math.reduce_min(average_grads)) / (\n    tf.math.reduce_max(average_grads) - tf.reduce_min(average_grads)\n)\nax2.plot(alphas, average_grads_norm)\nax2.set_title(\"Average pixel gradients (normalized) over alpha\")\nax2.set_ylabel(\"Average pixel gradients\")\nax2.set_xlabel(\"alpha\")\nax2.set_ylim([0, 1]);\n</pre> # @title plt.figure(figsize=(10, 4)) ax1 = plt.subplot(1, 2, 1) ax1.plot(alphas, pred_proba) ax1.set_title(\"Target class predicted probability over alpha\") ax1.set_ylabel(\"model p(target class)\") ax1.set_xlabel(\"alpha\") ax1.set_ylim([0, 1])  ax2 = plt.subplot(1, 2, 2) # Average across interpolation steps average_grads = tf.reduce_mean(path_gradients, axis=[1, 2, 3]) # Normalize gradients to 0 to 1 scale. E.g., (x - min(x))/(max(x)-min(x)) average_grads_norm = (average_grads - tf.math.reduce_min(average_grads)) / (     tf.math.reduce_max(average_grads) - tf.reduce_min(average_grads) ) ax2.plot(alphas, average_grads_norm) ax2.set_title(\"Average pixel gradients (normalized) over alpha\") ax2.set_ylabel(\"Average pixel gradients\") ax2.set_xlabel(\"alpha\") ax2.set_ylim([0, 1]); <ul> <li><p>left: This plot shows how your model's confidence in the \"Fireboat\" class varies across alphas. Notice how the gradients, or slope of the line, largely flattens or saturates between 0.6 and 1.0 before settling at the final \"Fireboat\" predicted probability of about 40%.</p> </li> <li><p>right: The right plot shows the average gradients magnitudes over alpha more directly. Note how the values sharply approach and even briefly dip below zero. In fact, your model \"learns\" the most from gradients at lower values of alpha before saturating. Intuitively, you can think of this as your model has learned the pixels e.g. water cannons to make the correct prediction, sending these pixel gradients to zero, but is still quite uncertain and focused on spurious bridge or water jet pixels as the alpha values approach the original input image.</p> </li> </ul> <p>To make sure these important water cannon pixels are reflected as important to the \"Fireboat\" prediction, you will continue on below to learn how to accumulate these gradients to accurately approximate how each pixel impacts your \"Fireboat\" predicted probability.</p> <p>There are many different ways you can go about computing the numerical approximation of an integral for IG with different tradeoffs in accuracy and convergence across varying functions. A popular class of methods is called Riemann sums. Here, you will use the Trapezoidal rule (you can find additional code to explore different approximation methods at the end of this tutorial).</p> <p>$IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\\times \\overbrace{\\sum_{k=1}^{m}}^\\text{Sum m local gradients}\\text{gradients(interpolated images)} \\times \\overbrace{\\frac{1}{m}}^\\text{Divide by m steps}$</p> <p>From the equation, you can see you are summing over <code>m</code> gradients and dividing by <code>m</code> steps. You can implement the two operations together for part 3 as an average of the local gradients of <code>m</code> interpolated predictions and input images.</p> In\u00a0[\u00a0]: Copied! <pre>def integral_approximation(gradients):\n    # riemann_trapezoidal\n    grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)\n    integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n    return integrated_gradients\n</pre> def integral_approximation(gradients):     # riemann_trapezoidal     grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)     integrated_gradients = tf.math.reduce_mean(grads, axis=0)     return integrated_gradients <p>The <code>integral_approximation</code> function takes the gradients of the predicted probability of the target class with respect to the interpolated images between the baseline and the original image.</p> In\u00a0[\u00a0]: Copied! <pre>ig = integral_approximation(gradients=path_gradients)\n</pre> ig = integral_approximation(gradients=path_gradients) <p>You can confirm averaging across the gradients of <code>m</code> interpolated images returns an integrated gradients tensor with the same shape as the original \"Giant Panda\" image.</p> In\u00a0[\u00a0]: Copied! <pre>print(ig.shape)\n</pre> print(ig.shape) <p>Now you will combine the 3 previous general parts together into an <code>IntegratedGradients</code> function and utilize a @tf.function decorator to compile it into a high performance callable TensorFlow graph. This is implemented as 5 smaller steps below:</p> <p>$IntegratedGrads^{approx}_{i}(x)::=\\overbrace{(x_{i}-x'_{i})}^\\text{5.}\\times \\overbrace{\\sum_{k=1}^{m}}^\\text{4.} \\frac{\\partial \\overbrace{F(\\overbrace{x' + \\overbrace{\\frac{k}{m}}^\\text{1.}\\times(x - x'))}^\\text{2.}}^\\text{3.}}{\\partial x_{i}} \\times \\overbrace{\\frac{1}{m}}^\\text{4.}$</p> <ol> <li><p>Generate alphas $\\alpha$</p> </li> <li><p>Generate interpolated images = $(x' + \\frac{k}{m}\\times(x - x'))$</p> </li> <li><p>Compute gradients between model $F$ output predictions with respect to input features = $\\frac{\\partial F(\\text{interpolated path inputs})}{\\partial x_{i}}$</p> </li> <li><p>Integral approximation through averaging gradients = $\\sum_{k=1}^m \\text{gradients} \\times \\frac{1}{m}$</p> </li> <li><p>Scale integrated gradients with respect to original image = $(x_{i}-x'_{i}) \\times \\text{integrated gradients}$. The reason this step is necessary is to make sure that the attribution values accumulated across multiple interpolated images are in the same units and faithfully represent the pixel importances on the original image.</p> </li> </ol> In\u00a0[\u00a0]: Copied! <pre>def integrated_gradients(baseline, image, target_class_idx, m_steps=50, batch_size=32):\n    # Generate alphas.\n    alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps + 1)\n\n    # Collect gradients.\n    gradient_batches = []\n\n    # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.\n    for alpha in tf.range(0, len(alphas), batch_size):\n        from_ = alpha\n        to = tf.minimum(from_ + batch_size, len(alphas))\n        alpha_batch = alphas[from_:to]\n\n        gradient_batch = one_batch(baseline, image, alpha_batch, target_class_idx)\n        gradient_batches.append(gradient_batch)\n\n    # Concatenate path gradients together row-wise into single tensor.\n    total_gradients = tf.concat(gradient_batches, axis=0)\n\n    # Integral approximation through averaging gradients.\n    avg_gradients = integral_approximation(gradients=total_gradients)\n\n    # Scale integrated gradients with respect to input.\n    integrated_gradients = (image - baseline) * avg_gradients\n\n    return integrated_gradients\n</pre> def integrated_gradients(baseline, image, target_class_idx, m_steps=50, batch_size=32):     # Generate alphas.     alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps + 1)      # Collect gradients.     gradient_batches = []      # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.     for alpha in tf.range(0, len(alphas), batch_size):         from_ = alpha         to = tf.minimum(from_ + batch_size, len(alphas))         alpha_batch = alphas[from_:to]          gradient_batch = one_batch(baseline, image, alpha_batch, target_class_idx)         gradient_batches.append(gradient_batch)      # Concatenate path gradients together row-wise into single tensor.     total_gradients = tf.concat(gradient_batches, axis=0)      # Integral approximation through averaging gradients.     avg_gradients = integral_approximation(gradients=total_gradients)      # Scale integrated gradients with respect to input.     integrated_gradients = (image - baseline) * avg_gradients      return integrated_gradients In\u00a0[\u00a0]: Copied! <pre>@tf.function\ndef one_batch(baseline, image, alpha_batch, target_class_idx):\n    # Generate interpolated inputs between baseline and input.\n    interpolated_path_input_batch = interpolate_images(\n        baseline=baseline, image=image, alphas=alpha_batch\n    )\n\n    # Compute gradients between model outputs and interpolated inputs.\n    gradient_batch = compute_gradients(\n        images=interpolated_path_input_batch, target_class_idx=target_class_idx\n    )\n    return gradient_batch\n</pre> @tf.function def one_batch(baseline, image, alpha_batch, target_class_idx):     # Generate interpolated inputs between baseline and input.     interpolated_path_input_batch = interpolate_images(         baseline=baseline, image=image, alphas=alpha_batch     )      # Compute gradients between model outputs and interpolated inputs.     gradient_batch = compute_gradients(         images=interpolated_path_input_batch, target_class_idx=target_class_idx     )     return gradient_batch In\u00a0[\u00a0]: Copied! <pre>ig_attributions = integrated_gradients(\n    baseline=baseline,\n    image=img_name_tensors[\"Fireboat\"],\n    target_class_idx=555,\n    m_steps=240,\n)\n</pre> ig_attributions = integrated_gradients(     baseline=baseline,     image=img_name_tensors[\"Fireboat\"],     target_class_idx=555,     m_steps=240, ) <p>Again, you can check that the IG feature attributions have the same shape as the input \"Fireboat\" image.</p> In\u00a0[\u00a0]: Copied! <pre>print(ig_attributions.shape)\n</pre> print(ig_attributions.shape) <p>The paper suggests the number of steps to range between 20 to 300 depending upon the example (although in practice this can be higher in the 1,000s to accurately approximate the integral). You can find additional code to check for the appropriate number of steps in the \"Next steps\" resources at the end of this tutorial.</p> <p>You are ready to visualize attributions, and overlay them on the original image. The code below sums the absolute values of the integrated gradients across the color channels to produce an attribution mask. This plotting method captures the relative impact of pixels on the model's predictions.</p> In\u00a0[\u00a0]: Copied! <pre># @title\ndef plot_img_attributions(\n    baseline, image, target_class_idx, m_steps=50, cmap=None, overlay_alpha=0.4\n):\n\n    attributions = integrated_gradients(\n        baseline=baseline,\n        image=image,\n        target_class_idx=target_class_idx,\n        m_steps=m_steps,\n    )\n\n    # Sum of the attributions across color channels for visualization.\n    # The attribution mask shape is a grayscale image with height and width\n    # equal to the original image.\n    attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)\n\n    fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(8, 8))\n\n    axs[0, 0].set_title(\"Baseline image\")\n    axs[0, 0].imshow(baseline)\n    axs[0, 0].axis(\"off\")\n\n    axs[0, 1].set_title(\"Original image\")\n    axs[0, 1].imshow(image)\n    axs[0, 1].axis(\"off\")\n\n    axs[1, 0].set_title(\"Attribution mask\")\n    axs[1, 0].imshow(attribution_mask, cmap=cmap)\n    axs[1, 0].axis(\"off\")\n\n    axs[1, 1].set_title(\"Overlay\")\n    axs[1, 1].imshow(attribution_mask, cmap=cmap)\n    axs[1, 1].imshow(image, alpha=overlay_alpha)\n    axs[1, 1].axis(\"off\")\n\n    plt.tight_layout()\n    return fig\n</pre> # @title def plot_img_attributions(     baseline, image, target_class_idx, m_steps=50, cmap=None, overlay_alpha=0.4 ):      attributions = integrated_gradients(         baseline=baseline,         image=image,         target_class_idx=target_class_idx,         m_steps=m_steps,     )      # Sum of the attributions across color channels for visualization.     # The attribution mask shape is a grayscale image with height and width     # equal to the original image.     attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)      fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(8, 8))      axs[0, 0].set_title(\"Baseline image\")     axs[0, 0].imshow(baseline)     axs[0, 0].axis(\"off\")      axs[0, 1].set_title(\"Original image\")     axs[0, 1].imshow(image)     axs[0, 1].axis(\"off\")      axs[1, 0].set_title(\"Attribution mask\")     axs[1, 0].imshow(attribution_mask, cmap=cmap)     axs[1, 0].axis(\"off\")      axs[1, 1].set_title(\"Overlay\")     axs[1, 1].imshow(attribution_mask, cmap=cmap)     axs[1, 1].imshow(image, alpha=overlay_alpha)     axs[1, 1].axis(\"off\")      plt.tight_layout()     return fig <p>Looking at the attributions on the \"Fireboat\" image, you can see the model identifies the water cannons and spouts as contributing to its correct prediction.</p> In\u00a0[\u00a0]: Copied! <pre>_ = plot_img_attributions(\n    image=img_name_tensors[\"Fireboat\"],\n    baseline=baseline,\n    target_class_idx=555,\n    m_steps=240,\n    cmap=plt.cm.inferno,\n    overlay_alpha=0.4,\n)\n</pre> _ = plot_img_attributions(     image=img_name_tensors[\"Fireboat\"],     baseline=baseline,     target_class_idx=555,     m_steps=240,     cmap=plt.cm.inferno,     overlay_alpha=0.4, ) In\u00a0[\u00a0]: Copied! <pre>_ = plot_img_attributions(\n    image=img_name_tensors[\"Sailboat\"],\n    baseline=baseline,\n    target_class_idx=555,\n    m_steps=240,\n    cmap=plt.cm.inferno,\n    overlay_alpha=0.4,\n)\n</pre> _ = plot_img_attributions(     image=img_name_tensors[\"Sailboat\"],     baseline=baseline,     target_class_idx=555,     m_steps=240,     cmap=plt.cm.inferno,     overlay_alpha=0.4, ) <p>On the \"Giant Panda\" image, the attributions highlight the texture, nose, and the fur of the Panda's face.</p> In\u00a0[\u00a0]: Copied! <pre># _ = plot_img_attributions(image=img_name_tensors['Giant Panda'],\n#                           baseline=baseline,\n#                           target_class_idx=389,\n#                           m_steps=55,\n#                           cmap=plt.cm.viridis,\n#                           overlay_alpha=0.5)\n\n_ = plot_img_attributions(\n    image=img_name_tensors[\"Coyote\"],\n    baseline=baseline,\n    target_class_idx=389,\n    m_steps=55,\n    cmap=plt.cm.viridis,\n    overlay_alpha=0.5,\n)\n</pre> # _ = plot_img_attributions(image=img_name_tensors['Giant Panda'], #                           baseline=baseline, #                           target_class_idx=389, #                           m_steps=55, #                           cmap=plt.cm.viridis, #                           overlay_alpha=0.5)  _ = plot_img_attributions(     image=img_name_tensors[\"Coyote\"],     baseline=baseline,     target_class_idx=389,     m_steps=55,     cmap=plt.cm.viridis,     overlay_alpha=0.5, ) In\u00a0[\u00a0]: Copied! <pre>_ = plot_img_attributions(\n    image=img_name_tensors[\"Coyote-2\"],\n    baseline=baseline,\n    target_class_idx=389,\n    m_steps=55,\n    cmap=plt.cm.viridis,\n    overlay_alpha=0.5,\n)\n</pre> _ = plot_img_attributions(     image=img_name_tensors[\"Coyote-2\"],     baseline=baseline,     target_class_idx=389,     m_steps=55,     cmap=plt.cm.viridis,     overlay_alpha=0.5, ) In\u00a0[\u00a0]: Copied! <pre>_ = plot_img_attributions(\n    image=img_name_tensors[\"Dino-2\"],\n    baseline=baseline,\n    target_class_idx=389,\n    m_steps=55,\n    cmap=plt.cm.viridis,\n    overlay_alpha=0.5,\n)\n</pre> _ = plot_img_attributions(     image=img_name_tensors[\"Dino-2\"],     baseline=baseline,     target_class_idx=389,     m_steps=55,     cmap=plt.cm.viridis,     overlay_alpha=0.5, ) <p>Use cases</p> <ul> <li>Employing techniques like Integrated Gradients before deploying your model can help you develop intuition for how and why it works. Do the features highlighted by this technique match your intuition? If not, that may be indicative of a bug in your model or dataset, or overfitting.</li> </ul> <p>Limitations</p> <ul> <li><p>The Integrated Gradients technique provides feature importances on individual examples. However, it does not provide global feature importances across an entire dataset.</p> </li> <li><p>The Integrated Gradients technique provides individual feature importances, but it does not explain feature interactions and combinations.</p> </li> </ul> <p>This tutorial presented a basic implementation of Integrated Gradients. As a next step, you can use this notebook to try this technique with different models and images yourself.</p> <p>For interested readers, there is a lengthier version of this tutorial (which includes code for different baselines, to compute integral approximations, and to determine a sufficient number of steps) which you can find here.</p> <p>To deepen your understanding, check out the paper Axiomatic Attribution for Deep Networks and Github repository, which contains an implementation in a previous version of TensorFlow. You can also explore feature attribution, and the impact of different baselines, on distill.pub.</p> <p>Interested in incorporating IG into your production machine learning workflows for feature importances, model error analysis, and data skew monitoring? Check out Google Cloud's Explainable AI product that supports IG attributions. The Google AI PAIR research group also open-sourced the What-if tool which can be used for model debugging, including visualizing IG feature attributions.</p>"},{"location":"notebooks/other/integrated_gradients/#integrated-gradients","title":"Integrated gradients\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#download-a-pretrained-image-classifier-from-tf-hub","title":"Download a pretrained image classifier from TF-Hub\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#load-and-preprocess-images-with-tfimage","title":"Load and preprocess images with <code>tf.image</code>\u00b6","text":"<p>You will illustrate IG using two images from Wikimedia Commons: a Fireboat, and a Giant Panda.</p>"},{"location":"notebooks/other/integrated_gradients/#classify-images","title":"Classify images\u00b6","text":"<p>Start by classifying these images and displaying the top 3 most confident predictions. The following is a utility function to retrieve the top k predicted labels and probabilities.</p>"},{"location":"notebooks/other/integrated_gradients/#calculate-integrated-gradients","title":"Calculate Integrated Gradients\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#establish-a-baseline","title":"Establish a baseline\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#unpack-formulas-into-code","title":"Unpack formulas into code\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#interpolate-images","title":"Interpolate images\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#compute-gradients","title":"Compute gradients\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#accumulate-gradients-integral-approximation","title":"Accumulate gradients (integral approximation)\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#putting-it-all-together","title":"Putting it all together\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#visualize-attributions","title":"Visualize attributions\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#uses-and-limitations","title":"Uses and limitations\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#next-steps","title":"Next steps\u00b6","text":""},{"location":"notebooks/other/integrated_gradients/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":"<pre><code>    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.</code></pre>"},{"location":"notebooks/other/regression/","title":"Regression","text":"<p>Lets start with importing what we need...</p> In\u00a0[10]: Copied! <pre># imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import norm\nfrom skimpy import skim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn import datasets\n</pre> # imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from statsmodels.formula.api import ols from scipy.stats import norm from skimpy import skim from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score from sklearn import datasets <p>Lets read in the data into a Pandas 'dataframe'. We'll see that this is a nifty datastruture. We'll take a look at the data frame shortly.</p> In\u00a0[18]: Copied! <pre>url = \"https://docs.google.com/spreadsheets/d/1rgHhBRqSZG5sfAMbbEVP2-IA0tAw7PftO6JLmJL4dY0/export?format=csv&amp;id=1rgHhBRqSZG5sfAMbbEVP2-IA0tAw7PftO6JLmJL4dY0&amp;gid=713155364\"\n\ndf = pd.read_csv(url)\n</pre> url = \"https://docs.google.com/spreadsheets/d/1rgHhBRqSZG5sfAMbbEVP2-IA0tAw7PftO6JLmJL4dY0/export?format=csv&amp;id=1rgHhBRqSZG5sfAMbbEVP2-IA0tAw7PftO6JLmJL4dY0&amp;gid=713155364\"  df = pd.read_csv(url) In\u00a0[20]: Copied! <pre>df.info\n</pre> df.info Out[20]: <pre>&lt;bound method DataFrame.info of       rentals  month  day  hour day_of_week  weekend  temp  temp_wb  \\\n0           4      1    1     0         Mon        0     2        0   \n1           6      1    1     1         Mon        0     1        0   \n2           6      1    1     2         Mon        0     1       -1   \n3           1      1    1     5         Mon        0     0       -2   \n4           3      1    1     6         Mon        0     0       -2   \n...       ...    ...  ...   ...         ...      ...   ...      ...   \n8598       36     12   31    19         Mon        0    39       36   \n8599       25     12   31    20         Mon        0    38       36   \n8600       13     12   31    21         Mon        0    37       36   \n8601        6     12   31    22         Mon        0    38       37   \n8602        7     12   31    23         Mon        0    39       38   \n\n      rel_humidity  windspeed  precipitation  \n0               59         16           0.00  \n1               59         11           0.00  \n2               54         21           0.00  \n3               54         18           0.00  \n4               54         15           0.00  \n...            ...        ...            ...  \n8598            76         13           0.02  \n8599            86         14           0.04  \n8600            89         15           0.07  \n8601            89         14           0.09  \n8602            89         17           0.19  \n\n[8603 rows x 11 columns]&gt;</pre> In\u00a0[21]: Copied! <pre>df.head()\n</pre> df.head() Out[21]: rentals month day hour day_of_week weekend temp temp_wb rel_humidity windspeed precipitation 0 4 1 1 0 Mon 0 2 0 59 16 0.0 1 6 1 1 1 Mon 0 1 0 59 11 0.0 2 6 1 1 2 Mon 0 1 -1 54 21 0.0 3 1 1 1 5 Mon 0 0 -2 54 18 0.0 4 3 1 1 6 Mon 0 0 -2 54 15 0.0 In\u00a0[22]: Copied! <pre>df.tail(6)\n</pre> df.tail(6) Out[22]: rentals month day hour day_of_week weekend temp temp_wb rel_humidity windspeed precipitation 8597 69 12 31 18 Mon 0 42 37 60 10 0.00 8598 36 12 31 19 Mon 0 39 36 76 13 0.02 8599 25 12 31 20 Mon 0 38 36 86 14 0.04 8600 13 12 31 21 Mon 0 37 36 89 15 0.07 8601 6 12 31 22 Mon 0 38 37 89 14 0.09 8602 7 12 31 23 Mon 0 39 38 89 17 0.19 <p>Looking at the data, its clear that things like 'day_of_week' or 'day' are not truly numerical (in the sense of a temperature or a price) but rather categorical. So we go ahead an declare those columns of the dataframe as such.</p> In\u00a0[23]: Copied! <pre>categorical_variables = [\"month\", \"day\", \"hour\", \"day_of_week\", \"weekend\"]\ndf[categorical_variables] = df[categorical_variables].astype(\"category\")\n</pre> categorical_variables = [\"month\", \"day\", \"hour\", \"day_of_week\", \"weekend\"] df[categorical_variables] = df[categorical_variables].astype(\"category\") In\u00a0[24]: Copied! <pre>skim(df)\n</pre> skim(df) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 skimpy summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502          Data Summary                Data Types               Categories                                        \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513                                \u2502\n\u2502 \u2503 Dataframe         \u2503 Values \u2503 \u2503 Column Type \u2503 Count \u2503 \u2503 Categorical Variables \u2503                                \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529                                \u2502\n\u2502 \u2502 Number of rows    \u2502 8603   \u2502 \u2502 int64       \u2502 5     \u2502 \u2502 month                 \u2502                                \u2502\n\u2502 \u2502 Number of columns \u2502 11     \u2502 \u2502 category    \u2502 5     \u2502 \u2502 day                   \u2502                                \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 float64     \u2502 1     \u2502 \u2502 hour                  \u2502                                \u2502\n\u2502                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 day_of_week           \u2502                                \u2502\n\u2502                                                        \u2502 weekend               \u2502                                \u2502\n\u2502                                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n\u2502                                                     number                                                      \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \u2502\n\u2502 \u2503 column             \u2503 NA   \u2503 NA %   \u2503 mean       \u2503 sd         \u2503 p0  \u2503 p25   \u2503 p50  \u2503 p75  \u2503 p100  \u2503 hist    \u2503  \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529  \u2502\n\u2502 \u2502 rentals            \u2502    0 \u2502      0 \u2502      205.1 \u2502      227.1 \u2502   1 \u2502    32 \u2502  120 \u2502  306 \u2502  1299 \u2502   \u2587\u2583\u2581   \u2502  \u2502\n\u2502 \u2502 temp               \u2502    0 \u2502      0 \u2502      53.21 \u2502      18.29 \u2502  -2 \u2502    38 \u2502   52 \u2502   69 \u2502    97 \u2502  \u2582\u2587\u2586\u2587\u2582  \u2502  \u2502\n\u2502 \u2502 temp_wb            \u2502    0 \u2502      0 \u2502      47.84 \u2502      16.91 \u2502  -3 \u2502    34 \u2502   48 \u2502   62 \u2502    81 \u2502  \u2582\u2587\u2587\u2587\u2585  \u2502  \u2502\n\u2502 \u2502 rel_humidity       \u2502    0 \u2502      0 \u2502      67.11 \u2502      18.92 \u2502  16 \u2502    53 \u2502   68 \u2502   84 \u2502   100 \u2502 \u2581\u2583\u2587\u2587\u2587\u2587  \u2502  \u2502\n\u2502 \u2502 windspeed          \u2502    0 \u2502      0 \u2502       11.1 \u2502      5.455 \u2502   0 \u2502     7 \u2502   10 \u2502   14 \u2502    48 \u2502   \u2583\u2587\u2582   \u2502  \u2502\n\u2502 \u2502 precipitation      \u2502    0 \u2502      0 \u2502    0.00544 \u2502    0.03407 \u2502   0 \u2502     0 \u2502    0 \u2502    0 \u2502  1.12 \u2502    \u2587    \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                    category                                                     \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \u2502\n\u2502 \u2503 column                           \u2503 NA        \u2503 NA %           \u2503 ordered               \u2503 unique             \u2503  \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529  \u2502\n\u2502 \u2502 month                            \u2502         0 \u2502              0 \u2502 False                 \u2502                 12 \u2502  \u2502\n\u2502 \u2502 day                              \u2502         0 \u2502              0 \u2502 False                 \u2502                 31 \u2502  \u2502\n\u2502 \u2502 hour                             \u2502         0 \u2502              0 \u2502 False                 \u2502                 24 \u2502  \u2502\n\u2502 \u2502 day_of_week                      \u2502         0 \u2502              0 \u2502 False                 \u2502                  7 \u2502  \u2502\n\u2502 \u2502 weekend                          \u2502         0 \u2502              0 \u2502 False                 \u2502                  2 \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 End \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>Lets produce a histogram of rentals data. We've selected bins based on what we saw of the data from the summary we gleaned using 'skim'.</p> In\u00a0[25]: Copied! <pre>plt.hist(df[\"rentals\"], bins=range(0, 1400, 50))\nplt.title(\"Rentals Data\")\nplt.xlabel(\"Number of Rentals\")\nplt.ylabel(\"Number of Data Points\")\n</pre> plt.hist(df[\"rentals\"], bins=range(0, 1400, 50)) plt.title(\"Rentals Data\") plt.xlabel(\"Number of Rentals\") plt.ylabel(\"Number of Data Points\") Out[25]: <pre>Text(0, 0.5, 'Number of Data Points')</pre> <p>The mean number of rental is ~205, with a standard deviation of 227.</p> In\u00a0[26]: Copied! <pre>print(\n    \"Average no. of Rentals: \"\n    + str(np.mean(df[\"rentals\"]))\n    + \" Standard Dev: \"\n    + str(np.std(df[\"rentals\"]))\n)\n</pre> print(     \"Average no. of Rentals: \"     + str(np.mean(df[\"rentals\"]))     + \" Standard Dev: \"     + str(np.std(df[\"rentals\"])) ) <pre>Average no. of Rentals: 205.08578402882716 Standard Dev: 227.0392651748052\n</pre> <p>Next a histogram at a temp of 25F.</p> In\u00a0[27]: Copied! <pre>df_cold = df[df[\"temp\"] == 25]\nplt.hist(df_cold[\"rentals\"], bins=range(0, 1400, 50))\nplt.title(\"Rentals Data at Temp of 25F\")\nplt.xlabel(\"Number of Rentals\")\nplt.ylabel(\"Number of Data Points\")\n</pre> df_cold = df[df[\"temp\"] == 25] plt.hist(df_cold[\"rentals\"], bins=range(0, 1400, 50)) plt.title(\"Rentals Data at Temp of 25F\") plt.xlabel(\"Number of Rentals\") plt.ylabel(\"Number of Data Points\") Out[27]: <pre>Text(0, 0.5, 'Number of Data Points')</pre> <p>Standard deviation is now a lot smaller..</p> In\u00a0[28]: Copied! <pre>print(\n    \"Average no. of Rentals at 25F: \"\n    + str(np.mean(df_cold[\"rentals\"]))\n    + \" Standard Dev: \"\n    + str(np.std(df_cold[\"rentals\"]))\n)\n</pre> print(     \"Average no. of Rentals at 25F: \"     + str(np.mean(df_cold[\"rentals\"]))     + \" Standard Dev: \"     + str(np.std(df_cold[\"rentals\"])) ) <pre>Average no. of Rentals at 25F: 72.55384615384615 Standard Dev: 90.4802623218041\n</pre> <p>A scatter plot of rentals across hour of rental shows that knowing the hour of rental could potentially reduce variability dramatically.</p> In\u00a0[29]: Copied! <pre>plt.scatter(df[\"hour\"], df[\"rentals\"], s=2)\nplt.title(\"Rentals vs. Hour of Day\")\nplt.xlabel(\"Hour of Day\")\nplt.ylabel(\"# Rentals\")\n</pre> plt.scatter(df[\"hour\"], df[\"rentals\"], s=2) plt.title(\"Rentals vs. Hour of Day\") plt.xlabel(\"Hour of Day\") plt.ylabel(\"# Rentals\") Out[29]: <pre>Text(0, 0.5, '# Rentals')</pre> <p>Now lets take a look at rentals vs. temperature. We also look at the mean (or average) number of rentals and notice a definite straight-line like trend</p> In\u00a0[30]: Copied! <pre>plt.scatter(df[\"temp\"], df[\"rentals\"], s=2)\nplt.title(\"Rentals vs. Temp (mean value in orange)\")\nplt.xlabel(\"Temp\")\nplt.ylabel(\"# Rentals\")\nplt.scatter(df[\"temp\"].unique(), df.groupby(\"temp\")[\"rentals\"].mean())\n</pre> plt.scatter(df[\"temp\"], df[\"rentals\"], s=2) plt.title(\"Rentals vs. Temp (mean value in orange)\") plt.xlabel(\"Temp\") plt.ylabel(\"# Rentals\") plt.scatter(df[\"temp\"].unique(), df.groupby(\"temp\")[\"rentals\"].mean()) Out[30]: <pre>&lt;matplotlib.collections.PathCollection at 0x2436e5d7c50&gt;</pre> <p>The above scatter plot is the key motivation for linear regression which we will turn to next. In particular, we will run a number of regressions of the type 'rentals' vs. some other independent variable (eg. temp) that we believe to have explanatory power.</p> In\u00a0[31]: Copied! <pre>est = ols(formula=\"rentals ~ temp\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ temp\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.235\nModel:                            OLS   Adj. R-squared:                  0.235\nMethod:                 Least Squares   F-statistic:                     2641.\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:48:32   Log-Likelihood:                -57727.\nNo. Observations:                8603   AIC:                         1.155e+05\nDf Residuals:                    8601   BIC:                         1.155e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -115.1099      6.588    -17.473      0.000    -128.024    -102.196\ntemp           6.0176      0.117     51.395      0.000       5.788       6.247\n==============================================================================\nOmnibus:                     2088.128   Durbin-Watson:                   0.424\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5175.982\nSkew:                           1.332   Prob(JB):                         0.00\nKurtosis:                       5.709   Cond. No.                         173.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[32]: Copied! <pre>b0 = -115.1099\nb1 = 6.0176\n\n# Generate some example data\nx = range(100)\ny = [b0 + b1 * i for i in x]\n\nplt.scatter(df[\"temp\"], df[\"rentals\"], s=2)\nplt.title(\"Rentals vs. Temp\")\nplt.xlabel(\"Temp\")\nplt.ylabel(\"# Rentals\")\nplt.plot(x, y, color=\"red\")\n</pre> b0 = -115.1099 b1 = 6.0176  # Generate some example data x = range(100) y = [b0 + b1 * i for i in x]  plt.scatter(df[\"temp\"], df[\"rentals\"], s=2) plt.title(\"Rentals vs. Temp\") plt.xlabel(\"Temp\") plt.ylabel(\"# Rentals\") plt.plot(x, y, color=\"red\") Out[32]: <pre>[&lt;matplotlib.lines.Line2D at 0x2436e57e690&gt;]</pre> In\u00a0[33]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>198.6078273046469\n</pre> In\u00a0[34]: Copied! <pre>est = ols(formula=\"rentals ~ rel_humidity\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ rel_humidity\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.028\nModel:                            OLS   Adj. R-squared:                  0.028\nMethod:                 Least Squares   F-statistic:                     249.5\nDate:                Sun, 18 May 2025   Prob (F-statistic):           1.99e-55\nTime:                        16:49:10   Log-Likelihood:                -58756.\nNo. Observations:                8603   AIC:                         1.175e+05\nDf Residuals:                    8601   BIC:                         1.175e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept      340.2762      8.892     38.266      0.000     322.845     357.707\nrel_humidity    -2.0146      0.128    -15.796      0.000      -2.265      -1.765\n==============================================================================\nOmnibus:                     2576.191   Durbin-Watson:                   0.336\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             6709.611\nSkew:                           1.633   Prob(JB):                         0.00\nKurtosis:                       5.837   Cond. No.                         257.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[35]: Copied! <pre>b0 = 340.2762\nb1 = -2.0146\n\n# Generate some example data\nx = range(100)\ny = [b0 + b1 * i for i in x]\n\nplt.scatter(df[\"rel_humidity\"], df[\"rentals\"], s=2)\nplt.title(\"Rentals vs. Rel Humidity\")\nplt.xlabel(\"Rel Humidity\")\nplt.ylabel(\"# Rentals\")\nplt.plot(x, y, color=\"red\")\n</pre> b0 = 340.2762 b1 = -2.0146  # Generate some example data x = range(100) y = [b0 + b1 * i for i in x]  plt.scatter(df[\"rel_humidity\"], df[\"rentals\"], s=2) plt.title(\"Rentals vs. Rel Humidity\") plt.xlabel(\"Rel Humidity\") plt.ylabel(\"# Rentals\") plt.plot(x, y, color=\"red\") Out[35]: <pre>[&lt;matplotlib.lines.Line2D at 0x2436e51e590&gt;]</pre> In\u00a0[36]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>223.84213890754862\n</pre> In\u00a0[37]: Copied! <pre>plt.scatter(df[\"weekend\"], df[\"rentals\"], s=2)\nplt.title(\"Rentals vs. Weekend [1=yes, 0=no]\")\nplt.xlabel(\"Weekend\")\nplt.ylabel(\"# Rentals\")\n</pre> plt.scatter(df[\"weekend\"], df[\"rentals\"], s=2) plt.title(\"Rentals vs. Weekend [1=yes, 0=no]\") plt.xlabel(\"Weekend\") plt.ylabel(\"# Rentals\") Out[37]: <pre>Text(0, 0.5, '# Rentals')</pre> In\u00a0[38]: Copied! <pre>est = ols(formula=\"rentals ~ weekend\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ weekend\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.015\nModel:                            OLS   Adj. R-squared:                  0.015\nMethod:                 Least Squares   F-statistic:                     132.1\nDate:                Sun, 18 May 2025   Prob (F-statistic):           2.42e-30\nTime:                        16:50:01   Log-Likelihood:                -58814.\nNo. Observations:                8603   AIC:                         1.176e+05\nDf Residuals:                    8601   BIC:                         1.176e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept      222.8186      2.878     77.417      0.000     217.177     228.460\nweekend[T.1]   -61.6884      5.368    -11.492      0.000     -72.211     -51.166\n==============================================================================\nOmnibus:                     2401.797   Durbin-Watson:                   0.335\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5705.729\nSkew:                           1.569   Prob(JB):                         0.00\nKurtosis:                       5.464   Cond. No.                         2.43\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[39]: Copied! <pre>plt.scatter(df[\"day_of_week\"], df[\"rentals\"], s=2)\nplt.title(\"Rentals vs. day of week\")\nplt.xlabel(\"Days of the Week\")\nplt.ylabel(\"# Rentals\")\n</pre> plt.scatter(df[\"day_of_week\"], df[\"rentals\"], s=2) plt.title(\"Rentals vs. day of week\") plt.xlabel(\"Days of the Week\") plt.ylabel(\"# Rentals\") Out[39]: <pre>Text(0, 0.5, '# Rentals')</pre> In\u00a0[40]: Copied! <pre>est = ols(formula=\"rentals ~ day_of_week\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ day_of_week\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.017\nModel:                            OLS   Adj. R-squared:                  0.017\nMethod:                 Least Squares   F-statistic:                     25.13\nDate:                Sun, 18 May 2025   Prob (F-statistic):           9.94e-30\nTime:                        16:50:17   Log-Likelihood:                -58805.\nNo. Observations:                8603   AIC:                         1.176e+05\nDf Residuals:                    8596   BIC:                         1.177e+05\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept            225.2480      6.441     34.970      0.000     212.622     237.874\nday_of_week[T.Mon]   -22.2210      9.040     -2.458      0.014     -39.942      -4.500\nday_of_week[T.Sat]   -57.2245      9.082     -6.301      0.000     -75.027     -39.422\nday_of_week[T.Sun]   -71.0166      9.083     -7.818      0.000     -88.822     -53.211\nday_of_week[T.Thu]     6.8572      9.119      0.752      0.452     -11.017      24.732\nday_of_week[T.Tue]    -6.3961      9.136     -0.700      0.484     -24.304      11.512\nday_of_week[T.Wed]    10.2108      9.107      1.121      0.262      -7.642      28.063\n==============================================================================\nOmnibus:                     2399.606   Durbin-Watson:                   0.335\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5702.983\nSkew:                           1.567   Prob(JB):                         0.00\nKurtosis:                       5.468   Cond. No.                         7.89\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>As a first step with multiple linear regression, we look for correlations between our independent variables. The reason is simple enough -- if two distinct variables are highly correlated, we may not want to include both. The reason for this is that it causes something called 'collinearity' which we will examine more carefully a bit later.</p> In\u00a0[41]: Copied! <pre>corr_matrix = df.corr(numeric_only=True)\ncorr_matrix\n</pre> corr_matrix = df.corr(numeric_only=True) corr_matrix Out[41]: rentals temp temp_wb rel_humidity windspeed precipitation rentals 1.000000 0.484716 0.425792 -0.167903 -0.019345 -0.071619 temp 0.484716 1.000000 0.977801 0.126377 -0.148411 0.018469 temp_wb 0.425792 0.977801 1.000000 0.319822 -0.188347 0.057638 rel_humidity -0.167903 0.126377 0.319822 1.000000 -0.218430 0.206801 windspeed -0.019345 -0.148411 -0.188347 -0.218430 1.000000 0.069177 precipitation -0.071619 0.018469 0.057638 0.206801 0.069177 1.000000 In\u00a0[42]: Copied! <pre>sns.heatmap(corr_matrix, annot=True)\n</pre> sns.heatmap(corr_matrix, annot=True) Out[42]: <pre>&lt;Axes: &gt;</pre> In\u00a0[43]: Copied! <pre>est = ols(formula=\"rentals ~ temp + rel_humidity\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ temp + rel_humidity\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.288\nModel:                            OLS   Adj. R-squared:                  0.288\nMethod:                 Least Squares   F-statistic:                     1742.\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:51:45   Log-Likelihood:                -57416.\nNo. Observations:                8603   AIC:                         1.148e+05\nDf Residuals:                    8600   BIC:                         1.149e+05\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       52.9562      9.175      5.772      0.000      34.971      70.941\ntemp             6.3829      0.114     56.066      0.000       6.160       6.606\nrel_humidity    -2.7942      0.110    -25.395      0.000      -3.010      -2.579\n==============================================================================\nOmnibus:                     2380.442   Durbin-Watson:                   0.458\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             6789.829\nSkew:                           1.454   Prob(JB):                         0.00\nKurtosis:                       6.237   Cond. No.                         391.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>What is the standard deviation of our residuals? Our best guess is the square root of the average squared residuals.</p> In\u00a0[44]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>191.56676007313123\n</pre> <p>So if we decided to predict the number of rentals when the temp is 60F and the relative humidity is 50%, the point estimate is simply</p> In\u00a0[45]: Copied! <pre>print(\n    \"y_pred = 52.9562 + 6.3829*60 - 2.7942*50 = \"\n    + str(52.9562 + 6.3829 * 60 - 2.7942 * 50)\n)\n</pre> print(     \"y_pred = 52.9562 + 6.3829*60 - 2.7942*50 = \"     + str(52.9562 + 6.3829 * 60 - 2.7942 * 50) ) <pre>y_pred = 52.9562 + 6.3829*60 - 2.7942*50 = 296.2202\n</pre> <p>So, a probabilistic estimate will be well modeled as a Normal random variable with mean 296.22 and standard deviation 191.63. We can now answer a question like 'What is the chance that the number of rentals exceeds 400 when the temp is 60F and the relative humidity is 50%?'. The answer: the same as the probability that a Normal random variable with mean 296.22 and standard deviation 191.63 exceeds 400!</p> In\u00a0[46]: Copied! <pre>print(\n    \"Probability of at least 400 rentals at 60F with 50% humidity: \"\n    + str(1 - norm.cdf(400, loc=296.22, scale=191.63))\n)\n</pre> print(     \"Probability of at least 400 rentals at 60F with 50% humidity: \"     + str(1 - norm.cdf(400, loc=296.22, scale=191.63)) ) <pre>Probability of at least 400 rentals at 60F with 50% humidity: 0.2940592857195723\n</pre> In\u00a0[47]: Copied! <pre>est.conf_int(alpha=0.05)\n</pre> est.conf_int(alpha=0.05) Out[47]: 0 1 Intercept 34.971280 70.941053 temp 6.159768 6.606104 rel_humidity -3.009922 -2.578540 In\u00a0[48]: Copied! <pre>1 - norm.cdf(10, loc=8, scale=2)\n</pre> 1 - norm.cdf(10, loc=8, scale=2) Out[48]: <pre>np.float64(0.15865525393145707)</pre> In\u00a0[49]: Copied! <pre>random_data = np.random.rand(len(df[\"rentals\"]))\ndf[\"random\"] = random_data\ndf.head(6)\n</pre> random_data = np.random.rand(len(df[\"rentals\"])) df[\"random\"] = random_data df.head(6) Out[49]: rentals month day hour day_of_week weekend temp temp_wb rel_humidity windspeed precipitation random 0 4 1 1 0 Mon 0 2 0 59 16 0.0 0.064791 1 6 1 1 1 Mon 0 1 0 59 11 0.0 0.144366 2 6 1 1 2 Mon 0 1 -1 54 21 0.0 0.367953 3 1 1 1 5 Mon 0 0 -2 54 18 0.0 0.724437 4 3 1 1 6 Mon 0 0 -2 54 15 0.0 0.151816 5 3 1 1 7 Mon 0 0 -2 54 11 0.0 0.901812 In\u00a0[50]: Copied! <pre>df[\"rentals\"].corr(df[\"random\"])\n</pre> df[\"rentals\"].corr(df[\"random\"]) Out[50]: <pre>np.float64(-0.003566719029223307)</pre> In\u00a0[51]: Copied! <pre>plt.scatter(df[\"random\"], df[\"rentals\"], s=2)\nplt.title(\"Rentals vs. Random Data\")\nplt.xlabel(\"Random\")\nplt.ylabel(\"# Rentals\")\n</pre> plt.scatter(df[\"random\"], df[\"rentals\"], s=2) plt.title(\"Rentals vs. Random Data\") plt.xlabel(\"Random\") plt.ylabel(\"# Rentals\") Out[51]: <pre>Text(0, 0.5, '# Rentals')</pre> In\u00a0[52]: Copied! <pre>est = ols(formula=\"rentals ~ temp + rel_humidity + random\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ temp + rel_humidity + random\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.288\nModel:                            OLS   Adj. R-squared:                  0.288\nMethod:                 Least Squares   F-statistic:                     1161.\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:54:42   Log-Likelihood:                -57416.\nNo. Observations:                8603   AIC:                         1.148e+05\nDf Residuals:                    8599   BIC:                         1.149e+05\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       53.3817      9.867      5.410      0.000      34.040      72.723\ntemp             6.3828      0.114     56.060      0.000       6.160       6.606\nrel_humidity    -2.7943      0.110    -25.393      0.000      -3.010      -2.579\nrandom          -0.8449      7.204     -0.117      0.907     -14.967      13.277\n==============================================================================\nOmnibus:                     2380.705   Durbin-Watson:                   0.458\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             6791.405\nSkew:                           1.454   Prob(JB):                         0.00\nKurtosis:                       6.238   Cond. No.                         445.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[53]: Copied! <pre>est = ols(formula=\"rentals ~ temp + temp_wb\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ temp + temp_wb\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.288\nModel:                            OLS   Adj. R-squared:                  0.288\nMethod:                 Least Squares   F-statistic:                     1738.\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:54:50   Log-Likelihood:                -57420.\nNo. Observations:                8603   AIC:                         1.148e+05\nDf Residuals:                    8600   BIC:                         1.149e+05\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -119.2405      6.359    -18.752      0.000    -131.705    -106.776\ntemp          19.3338      0.539     35.858      0.000      18.277      20.391\ntemp_wb      -14.7260      0.583    -25.258      0.000     -15.869     -13.583\n==============================================================================\nOmnibus:                     2473.945   Durbin-Watson:                   0.460\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             7319.712\nSkew:                           1.497   Prob(JB):                         0.00\nKurtosis:                       6.384   Cond. No.                         233.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[54]: Copied! <pre>est = ols(\n    formula=\"rentals ~ temp + rel_humidity + windspeed + precipitation\", data=df\n).fit()\nprint(est.summary())\n</pre> est = ols(     formula=\"rentals ~ temp + rel_humidity + windspeed + precipitation\", data=df ).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.290\nModel:                            OLS   Adj. R-squared:                  0.289\nMethod:                 Least Squares   F-statistic:                     876.1\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:54:57   Log-Likelihood:                -57409.\nNo. Observations:                8603   AIC:                         1.148e+05\nDf Residuals:                    8598   BIC:                         1.149e+05\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept        40.8156     11.488      3.553      0.000      18.297      63.334\ntemp              6.3960      0.115     55.782      0.000       6.171       6.621\nrel_humidity     -2.6789      0.115    -23.236      0.000      -2.905      -2.453\nwindspeed         0.4503      0.394      1.144      0.253      -0.321       1.222\nprecipitation  -237.9935     62.368     -3.816      0.000    -360.251    -115.736\n==============================================================================\nOmnibus:                     2370.634   Durbin-Watson:                   0.462\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             6743.262\nSkew:                           1.449   Prob(JB):                         0.00\nKurtosis:                       6.226   Cond. No.                     2.68e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.68e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n</pre> In\u00a0[55]: Copied! <pre>est = ols(formula=\"rentals ~ temp + rel_humidity + precipitation\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ temp + rel_humidity + precipitation\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.289\nModel:                            OLS   Adj. R-squared:                  0.289\nMethod:                 Least Squares   F-statistic:                     1168.\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:55:03   Log-Likelihood:                -57410.\nNo. Observations:                8603   AIC:                         1.148e+05\nDf Residuals:                    8599   BIC:                         1.149e+05\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept        48.6214      9.242      5.261      0.000      30.504      66.739\ntemp              6.3796      0.114     56.076      0.000       6.157       6.603\nrel_humidity     -2.7084      0.112    -24.104      0.000      -2.929      -2.488\nprecipitation  -229.4505     61.921     -3.706      0.000    -350.831    -108.070\n==============================================================================\nOmnibus:                     2367.029   Durbin-Watson:                   0.462\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             6731.705\nSkew:                           1.447   Prob(JB):                         0.00\nKurtosis:                       6.225   Cond. No.                     2.64e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.64e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n</pre> In\u00a0[56]: Copied! <pre>est = ols(formula=\"rentals ~ temp + temp_wb\", data=df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"rentals ~ temp + temp_wb\", data=df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.288\nModel:                            OLS   Adj. R-squared:                  0.288\nMethod:                 Least Squares   F-statistic:                     1738.\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:55:12   Log-Likelihood:                -57420.\nNo. Observations:                8603   AIC:                         1.148e+05\nDf Residuals:                    8600   BIC:                         1.149e+05\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -119.2405      6.359    -18.752      0.000    -131.705    -106.776\ntemp          19.3338      0.539     35.858      0.000      18.277      20.391\ntemp_wb      -14.7260      0.583    -25.258      0.000     -15.869     -13.583\n==============================================================================\nOmnibus:                     2473.945   Durbin-Watson:                   0.460\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             7319.712\nSkew:                           1.497   Prob(JB):                         0.00\nKurtosis:                       6.384   Cond. No.                         233.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[57]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>191.6383021375479\n</pre> <p>Let's split our data into a training set (70% of the data) and a test set (the remaining 30%). We will use the train_test_split utility we imported to accomplish this.</p> In\u00a0[58]: Copied! <pre>df_train, df_test = train_test_split(df, test_size=0.3)\n</pre> df_train, df_test = train_test_split(df, test_size=0.3) <p>Let's retrain a model on just the training data:</p> In\u00a0[59]: Copied! <pre>est_train = ols(\n    formula=\"rentals ~ temp + rel_humidity + precipitation\", data=df_train\n).fit()\nprint(est_train.summary())\n</pre> est_train = ols(     formula=\"rentals ~ temp + rel_humidity + precipitation\", data=df_train ).fit() print(est_train.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                rentals   R-squared:                       0.296\nModel:                            OLS   Adj. R-squared:                  0.296\nMethod:                 Least Squares   F-statistic:                     843.1\nDate:                Sun, 18 May 2025   Prob (F-statistic):               0.00\nTime:                        16:55:50   Log-Likelihood:                -40218.\nNo. Observations:                6022   AIC:                         8.044e+04\nDf Residuals:                    6018   BIC:                         8.047e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept        49.3205     11.047      4.464      0.000      27.664      70.977\ntemp              6.4834      0.137     47.480      0.000       6.216       6.751\nrel_humidity     -2.7822      0.134    -20.691      0.000      -3.046      -2.519\nprecipitation  -286.4779     81.383     -3.520      0.000    -446.017    -126.938\n==============================================================================\nOmnibus:                     1594.891   Durbin-Watson:                   1.997\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4308.606\nSkew:                           1.411   Prob(JB):                         0.00\nKurtosis:                       6.035   Cond. No.                     2.88e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.88e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n</pre> <p>How well does it do on the test data? Lets use the model we learned on the training data to predict rentals on the test data and then measure the OOS R^2</p> In\u00a0[60]: Copied! <pre>print(np.sqrt(est_train.mse_resid))\n</pre> print(np.sqrt(est_train.mse_resid)) <pre>192.4637563274933\n</pre> In\u00a0[61]: Copied! <pre>test_pred = est_train.predict(df_test)\nprint(\"OOS R-squared: \" + str(r2_score(df_test[\"rentals\"], test_pred)))\n</pre> test_pred = est_train.predict(df_test) print(\"OOS R-squared: \" + str(r2_score(df_test[\"rentals\"], test_pred))) <pre>OOS R-squared: 0.27236571111267827\n</pre> In\u00a0[62]: Copied! <pre>df_test.head()\n</pre> df_test.head() Out[62]: rentals month day hour day_of_week weekend temp temp_wb rel_humidity windspeed precipitation random 595 6 1 28 6 Sun 1 47 46 93 14 0.0 0.466239 3968 105 6 20 23 Wed 0 70 63 66 7 0.0 0.520552 1774 150 3 21 7 Wed 0 35 32 72 22 0.0 0.621599 741 64 2 3 13 Sat 1 27 21 37 17 0.0 0.644899 8010 29 12 7 5 Fri 0 35 32 70 15 0.0 0.804495 In\u00a0[63]: Copied! <pre>r = 50.7752 + 6.4101 * (46) - 2.7708 * (56) - 246.8192 * (0.0)\nr\n</pre> r = 50.7752 + 6.4101 * (46) - 2.7708 * (56) - 246.8192 * (0.0) r Out[63]: <pre>190.475</pre> In\u00a0[64]: Copied! <pre>test_pred.head()\n</pre> test_pred.head() Out[64]: <pre>595      95.296704\n3968    319.533277\n1774     75.921915\n741     121.431156\n8010     81.486274\ndtype: float64</pre> In\u00a0[65]: Copied! <pre>plt.scatter(df_test[\"rentals\"], test_pred, s=2)\nplt.title(\"Rentals vs. Rentals\")\nplt.xlabel(\"Actual Rentals\")\nplt.ylabel(\"Predicted Rentals\")\n</pre> plt.scatter(df_test[\"rentals\"], test_pred, s=2) plt.title(\"Rentals vs. Rentals\") plt.xlabel(\"Actual Rentals\") plt.ylabel(\"Predicted Rentals\") Out[65]: <pre>Text(0, 0.5, 'Predicted Rentals')</pre> <p>Lets load the publicly available diabetes dataset and print out a description of the dataset</p> In\u00a0[66]: Copied! <pre>diabetes = datasets.load_diabetes(as_frame=True)\ndiabetes_df = diabetes[\"frame\"]\n</pre> diabetes = datasets.load_diabetes(as_frame=True) diabetes_df = diabetes[\"frame\"] In\u00a0[67]: Copied! <pre>print(diabetes[\"DESCR\"])\n</pre> print(diabetes[\"DESCR\"]) <pre>.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n:Number of Instances: 442\n\n:Number of Attributes: First 10 columns are numeric predictive values\n\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n:Attribute Information:\n    - age     age in years\n    - sex\n    - bmi     body mass index\n    - bp      average blood pressure\n    - s1      tc, total serum cholesterol\n    - s2      ldl, low-density lipoproteins\n    - s3      hdl, high-density lipoproteins\n    - s4      tch, total cholesterol / HDL\n    - s5      ltg, possibly log of serum triglycerides level\n    - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n</pre> <p>Your task is to build the best linear regression model you can using this data to predict the 'target' field.</p> In\u00a0[68]: Copied! <pre>diabetes_df.head(6)\n</pre> diabetes_df.head(6) Out[68]: age sex bmi bp s1 s2 s3 s4 s5 s6 target 0 0.038076 0.050680 0.061696 0.021872 -0.044223 -0.034821 -0.043401 -0.002592 0.019907 -0.017646 151.0 1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163 0.074412 -0.039493 -0.068332 -0.092204 75.0 2 0.085299 0.050680 0.044451 -0.005670 -0.045599 -0.034194 -0.032356 -0.002592 0.002861 -0.025930 141.0 3 -0.089063 -0.044642 -0.011595 -0.036656 0.012191 0.024991 -0.036038 0.034309 0.022688 -0.009362 206.0 4 0.005383 -0.044642 -0.036385 0.021872 0.003935 0.015596 0.008142 -0.002592 -0.031988 -0.046641 135.0 5 -0.092695 -0.044642 -0.040696 -0.019442 -0.068991 -0.079288 0.041277 -0.076395 -0.041176 -0.096346 97.0 In\u00a0[69]: Copied! <pre>skim(diabetes_df)\n</pre> skim(diabetes_df) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 skimpy summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502          Data Summary                Data Types                                                                 \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513                                                          \u2502\n\u2502 \u2503 Dataframe         \u2503 Values \u2503 \u2503 Column Type \u2503 Count \u2503                                                          \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529                                                          \u2502\n\u2502 \u2502 Number of rows    \u2502 442    \u2502 \u2502 float64     \u2502 11    \u2502                                                          \u2502\n\u2502 \u2502 Number of columns \u2502 11     \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                          \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                                                  \u2502\n\u2502                                                     number                                                      \u2502\n\u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \u2502\n\u2502 \u2503 column  \u2503 NA  \u2503 NA % \u2503 mean       \u2503 sd      \u2503 p0       \u2503 p25      \u2503 p50       \u2503 p75     \u2503 p100    \u2503 hist   \u2503  \u2502\n\u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529  \u2502\n\u2502 \u2502 age     \u2502   0 \u2502    0 \u2502 -2.512e-19 \u2502 0.04762 \u2502  -0.1072 \u2502  -0.0373 \u2502  0.005383 \u2502 0.03808 \u2502  0.1107 \u2502 \u2582\u2585\u2586\u2587\u2586\u2581 \u2502  \u2502\n\u2502 \u2502 sex     \u2502   0 \u2502    0 \u2502  1.231e-17 \u2502 0.04762 \u2502 -0.04464 \u2502 -0.04464 \u2502  -0.04464 \u2502 0.05068 \u2502 0.05068 \u2502 \u2587    \u2587 \u2502  \u2502\n\u2502 \u2502 bmi     \u2502   0 \u2502    0 \u2502 -2.246e-16 \u2502 0.04762 \u2502 -0.09028 \u2502 -0.03423 \u2502 -0.007284 \u2502 0.03125 \u2502  0.1706 \u2502 \u2583\u2587\u2585\u2583\u2581  \u2502  \u2502\n\u2502 \u2502 bp      \u2502   0 \u2502    0 \u2502 -4.798e-17 \u2502 0.04762 \u2502  -0.1124 \u2502 -0.03666 \u2502  -0.00567 \u2502 0.03564 \u2502   0.132 \u2502 \u2581\u2586\u2587\u2585\u2583\u2581 \u2502  \u2502\n\u2502 \u2502 s1      \u2502   0 \u2502    0 \u2502 -1.381e-17 \u2502 0.04762 \u2502  -0.1268 \u2502 -0.03425 \u2502 -0.004321 \u2502 0.02836 \u2502  0.1539 \u2502 \u2581\u2585\u2587\u2585\u2582\u2581 \u2502  \u2502\n\u2502 \u2502 s2      \u2502   0 \u2502    0 \u2502  3.918e-17 \u2502 0.04762 \u2502  -0.1156 \u2502 -0.03036 \u2502 -0.003819 \u2502 0.02984 \u2502  0.1988 \u2502 \u2582\u2587\u2587\u2583\u2581  \u2502  \u2502\n\u2502 \u2502 s3      \u2502   0 \u2502    0 \u2502 -5.777e-18 \u2502 0.04762 \u2502  -0.1023 \u2502 -0.03512 \u2502 -0.006584 \u2502 0.02931 \u2502  0.1812 \u2502 \u2582\u2587\u2587\u2583\u2581  \u2502  \u2502\n\u2502 \u2502 s4      \u2502   0 \u2502    0 \u2502 -9.043e-18 \u2502 0.04762 \u2502 -0.07639 \u2502 -0.03949 \u2502 -0.002592 \u2502 0.03431 \u2502  0.1852 \u2502 \u2587\u2586\u2585\u2582\u2581  \u2502  \u2502\n\u2502 \u2502 s5      \u2502   0 \u2502    0 \u2502  9.294e-17 \u2502 0.04762 \u2502  -0.1261 \u2502 -0.03325 \u2502 -0.001947 \u2502 0.03243 \u2502  0.1336 \u2502 \u2581\u2585\u2587\u2587\u2583\u2581 \u2502  \u2502\n\u2502 \u2502 s6      \u2502   0 \u2502    0 \u2502   1.13e-17 \u2502 0.04762 \u2502  -0.1378 \u2502 -0.03318 \u2502 -0.001078 \u2502 0.02792 \u2502  0.1356 \u2502 \u2581\u2583\u2587\u2587\u2583\u2581 \u2502  \u2502\n\u2502 \u2502 target  \u2502   0 \u2502    0 \u2502      152.1 \u2502   77.09 \u2502       25 \u2502       87 \u2502     140.5 \u2502   211.5 \u2502     346 \u2502 \u2587\u2587\u2587\u2585\u2585\u2581 \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 End \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[70]: Copied! <pre>diabetes_df.info()\n</pre> diabetes_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 442 entries, 0 to 441\nData columns (total 11 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   age     442 non-null    float64\n 1   sex     442 non-null    float64\n 2   bmi     442 non-null    float64\n 3   bp      442 non-null    float64\n 4   s1      442 non-null    float64\n 5   s2      442 non-null    float64\n 6   s3      442 non-null    float64\n 7   s4      442 non-null    float64\n 8   s5      442 non-null    float64\n 9   s6      442 non-null    float64\n 10  target  442 non-null    float64\ndtypes: float64(11)\nmemory usage: 38.1 KB\n</pre> In\u00a0[71]: Copied! <pre>plt.hist(diabetes_df[\"target\"], bins=range(0, 400, 10))\nplt.title(\"Target data\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Number of Data Points\")\n</pre> plt.hist(diabetes_df[\"target\"], bins=range(0, 400, 10)) plt.title(\"Target data\") plt.xlabel(\"Target\") plt.ylabel(\"Number of Data Points\") Out[71]: <pre>Text(0, 0.5, 'Number of Data Points')</pre> In\u00a0[72]: Copied! <pre>print(\n    \"Average no. for Target: \"\n    + str(np.mean(diabetes_df[\"target\"]))\n    + \" Standard Dev: \"\n    + str(np.std(diabetes_df[\"target\"]))\n)\n</pre> print(     \"Average no. for Target: \"     + str(np.mean(diabetes_df[\"target\"]))     + \" Standard Dev: \"     + str(np.std(diabetes_df[\"target\"])) ) <pre>Average no. for Target: 152.13348416289594 Standard Dev: 77.00574586945044\n</pre> In\u00a0[73]: Copied! <pre>plt.scatter(diabetes_df[\"bmi\"], diabetes_df[\"target\"], s=2)\nplt.title(\"Target vs. BMI\")\nplt.xlabel(\"BMI\")\nplt.ylabel(\"Target\")\n</pre> plt.scatter(diabetes_df[\"bmi\"], diabetes_df[\"target\"], s=2) plt.title(\"Target vs. BMI\") plt.xlabel(\"BMI\") plt.ylabel(\"Target\") Out[73]: <pre>Text(0, 0.5, 'Target')</pre> In\u00a0[74]: Copied! <pre>est = ols(formula=\"target ~ bmi\", data=diabetes_df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"target ~ bmi\", data=diabetes_df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.344\nModel:                            OLS   Adj. R-squared:                  0.342\nMethod:                 Least Squares   F-statistic:                     230.7\nDate:                Sun, 18 May 2025   Prob (F-statistic):           3.47e-42\nTime:                        16:58:51   Log-Likelihood:                -2454.0\nNo. Observations:                 442   AIC:                             4912.\nDf Residuals:                     440   BIC:                             4920.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.1335      2.974     51.162      0.000     146.289     157.978\nbmi          949.4353     62.515     15.187      0.000     826.570    1072.301\n==============================================================================\nOmnibus:                       11.674   Durbin-Watson:                   1.848\nProb(Omnibus):                  0.003   Jarque-Bera (JB):                7.310\nSkew:                           0.156   Prob(JB):                       0.0259\nKurtosis:                       2.453   Cond. No.                         21.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[75]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>62.51512200285265\n</pre> In\u00a0[76]: Copied! <pre>plt.scatter(diabetes_df[\"s3\"], diabetes_df[\"target\"], s=2)\nplt.title(\"Target vs. s3\")\nplt.xlabel(\"s3\")\nplt.ylabel(\"Target\")\n</pre> plt.scatter(diabetes_df[\"s3\"], diabetes_df[\"target\"], s=2) plt.title(\"Target vs. s3\") plt.xlabel(\"s3\") plt.ylabel(\"Target\") Out[76]: <pre>Text(0, 0.5, 'Target')</pre> In\u00a0[77]: Copied! <pre>est = ols(formula=\"target ~ s3\", data=diabetes_df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"target ~ s3\", data=diabetes_df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.156\nModel:                            OLS   Adj. R-squared:                  0.154\nMethod:                 Least Squares   F-statistic:                     81.24\nDate:                Sun, 18 May 2025   Prob (F-statistic):           6.16e-18\nTime:                        16:59:10   Log-Likelihood:                -2509.7\nNo. Observations:                 442   AIC:                             5023.\nDf Residuals:                     440   BIC:                             5032.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.1335      3.373     45.105      0.000     145.504     158.762\ns3          -639.1453     70.911     -9.013      0.000    -778.512    -499.778\n==============================================================================\nOmnibus:                       30.490   Durbin-Watson:                   1.908\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.293\nSkew:                           0.353   Prob(JB):                     0.000107\nKurtosis:                       2.297   Cond. No.                         21.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[78]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>70.91131523302553\n</pre> In\u00a0[79]: Copied! <pre># Calculate correlation matrix\ncorrelation_matrix = diabetes_df.corr(numeric_only=True)\ncorrelation_matrix\n</pre> # Calculate correlation matrix correlation_matrix = diabetes_df.corr(numeric_only=True) correlation_matrix Out[79]: age sex bmi bp s1 s2 s3 s4 s5 s6 target age 1.000000 0.173737 0.185085 0.335428 0.260061 0.219243 -0.075181 0.203841 0.270774 0.301731 0.187889 sex 0.173737 1.000000 0.088161 0.241010 0.035277 0.142637 -0.379090 0.332115 0.149916 0.208133 0.043062 bmi 0.185085 0.088161 1.000000 0.395411 0.249777 0.261170 -0.366811 0.413807 0.446157 0.388680 0.586450 bp 0.335428 0.241010 0.395411 1.000000 0.242464 0.185548 -0.178762 0.257650 0.393480 0.390430 0.441482 s1 0.260061 0.035277 0.249777 0.242464 1.000000 0.896663 0.051519 0.542207 0.515503 0.325717 0.212022 s2 0.219243 0.142637 0.261170 0.185548 0.896663 1.000000 -0.196455 0.659817 0.318357 0.290600 0.174054 s3 -0.075181 -0.379090 -0.366811 -0.178762 0.051519 -0.196455 1.000000 -0.738493 -0.398577 -0.273697 -0.394789 s4 0.203841 0.332115 0.413807 0.257650 0.542207 0.659817 -0.738493 1.000000 0.617859 0.417212 0.430453 s5 0.270774 0.149916 0.446157 0.393480 0.515503 0.318357 -0.398577 0.617859 1.000000 0.464669 0.565883 s6 0.301731 0.208133 0.388680 0.390430 0.325717 0.290600 -0.273697 0.417212 0.464669 1.000000 0.382483 target 0.187889 0.043062 0.586450 0.441482 0.212022 0.174054 -0.394789 0.430453 0.565883 0.382483 1.000000 In\u00a0[80]: Copied! <pre># Create a mask for correlations less than 0.5\nmask = np.abs(correlation_matrix) &lt; 0.5\n\nsns.heatmap(diabetes_df.corr(), annot=True, mask=mask, cmap=\"coolwarm\", center=0)\n</pre> # Create a mask for correlations less than 0.5 mask = np.abs(correlation_matrix) &lt; 0.5  sns.heatmap(diabetes_df.corr(), annot=True, mask=mask, cmap=\"coolwarm\", center=0) Out[80]: <pre>&lt;Axes: &gt;</pre> In\u00a0[81]: Copied! <pre>plt.scatter(diabetes_df[\"sex\"], diabetes_df[\"target\"], s=2)\nplt.title(\"Target vs. Sex [1=male, 0=female]\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Target\")\n</pre> plt.scatter(diabetes_df[\"sex\"], diabetes_df[\"target\"], s=2) plt.title(\"Target vs. Sex [1=male, 0=female]\") plt.xlabel(\"Sex\") plt.ylabel(\"Target\") Out[81]: <pre>Text(0, 0.5, 'Target')</pre> In\u00a0[82]: Copied! <pre>est = ols(formula=\"target ~ sex\", data=diabetes_df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"target ~ sex\", data=diabetes_df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.002\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.8174\nDate:                Sun, 18 May 2025   Prob (F-statistic):              0.366\nTime:                        16:59:44   Log-Likelihood:                -2546.8\nNo. Observations:                 442   AIC:                             5098.\nDf Residuals:                     440   BIC:                             5106.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.1335      3.668     41.479      0.000     144.925     159.342\nsex           69.7154     77.109      0.904      0.366     -81.832     221.263\n==============================================================================\nOmnibus:                       64.308   Durbin-Watson:                   1.908\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               28.530\nSkew:                           0.436   Prob(JB):                     6.38e-07\nKurtosis:                       2.112   Cond. No.                         21.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[83]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>77.10896796118259\n</pre> In\u00a0[84]: Copied! <pre>est = ols(formula=\"target ~ age\", data=diabetes_df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"target ~ age\", data=diabetes_df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.035\nModel:                            OLS   Adj. R-squared:                  0.033\nMethod:                 Least Squares   F-statistic:                     16.10\nDate:                Sun, 18 May 2025   Prob (F-statistic):           7.06e-05\nTime:                        16:59:59   Log-Likelihood:                -2539.2\nNo. Observations:                 442   AIC:                             5082.\nDf Residuals:                     440   BIC:                             5091.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.1335      3.606     42.192      0.000     145.047     159.220\nage          304.1831     75.806      4.013      0.000     155.196     453.170\n==============================================================================\nOmnibus:                       52.996   Durbin-Watson:                   1.921\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               26.909\nSkew:                           0.438   Prob(JB):                     1.43e-06\nKurtosis:                       2.167   Cond. No.                         21.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[85]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>75.80599912703144\n</pre> In\u00a0[86]: Copied! <pre>est = ols(\n    formula=\"target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5\", data=diabetes_df\n).fit()\nprint(est.summary())\n</pre> est = ols(     formula=\"target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5\", data=diabetes_df ).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.517\nModel:                            OLS   Adj. R-squared:                  0.507\nMethod:                 Least Squares   F-statistic:                     51.29\nDate:                Sun, 18 May 2025   Prob (F-statistic):           8.68e-63\nTime:                        17:00:13   Log-Likelihood:                -2386.5\nNo. Observations:                 442   AIC:                             4793.\nDf Residuals:                     432   BIC:                             4834.\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.1335      2.576     59.058      0.000     147.070     157.197\nage           -1.9476     59.233     -0.033      0.974    -118.368     114.472\nsex         -235.2740     61.065     -3.853      0.000    -355.296    -115.252\nbmi          530.1282     65.777      8.060      0.000     400.846     659.410\nbp           334.9495     64.609      5.184      0.000     207.963     461.936\ns1          -797.2829    416.674     -1.913      0.056   -1616.244      21.678\ns2           482.3017    339.007      1.423      0.156    -184.006    1148.610\ns3           106.8011    212.470      0.503      0.615    -310.802     524.404\ns4           188.7791    161.080      1.172      0.242    -127.819     505.377\ns5           767.0074    171.223      4.480      0.000     430.473    1103.541\n==============================================================================\nOmnibus:                        1.193   Durbin-Watson:                   2.039\nProb(Omnibus):                  0.551   Jarque-Bera (JB):                1.183\nSkew:                           0.026   Prob(JB):                        0.554\nKurtosis:                       2.752   Cond. No.                         227.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[87]: Copied! <pre>est = ols(formula=\"target ~ sex + bmi + bp + s5\", data=diabetes_df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"target ~ sex + bmi + bp + s5\", data=diabetes_df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.487\nModel:                            OLS   Adj. R-squared:                  0.482\nMethod:                 Least Squares   F-statistic:                     103.6\nDate:                Sun, 18 May 2025   Prob (F-statistic):           5.42e-62\nTime:                        17:00:23   Log-Likelihood:                -2399.8\nNo. Observations:                 442   AIC:                             4810.\nDf Residuals:                     437   BIC:                             4830.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.1335      2.639     57.648      0.000     146.947     157.320\nsex         -136.7580     57.304     -2.387      0.017    -249.383     -24.132\nbmi          598.2839     64.365      9.295      0.000     471.781     724.786\nbp           292.9722     63.935      4.582      0.000     167.314     418.630\ns5           554.4326     64.427      8.606      0.000     427.807     681.059\n==============================================================================\nOmnibus:                        5.261   Durbin-Watson:                   1.982\nProb(Omnibus):                  0.072   Jarque-Bera (JB):                4.282\nSkew:                           0.145   Prob(JB):                        0.118\nKurtosis:                       2.614   Cond. No.                         28.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[88]: Copied! <pre>est = ols(formula=\"target ~ bmi + s5\", data=diabetes_df).fit()\nprint(est.summary())\n</pre> est = ols(formula=\"target ~ bmi + s5\", data=diabetes_df).fit() print(est.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.459\nModel:                            OLS   Adj. R-squared:                  0.457\nMethod:                 Least Squares   F-statistic:                     186.6\nDate:                Sun, 18 May 2025   Prob (F-statistic):           2.25e-59\nTime:                        17:00:35   Log-Likelihood:                -2411.2\nNo. Observations:                 442   AIC:                             4828.\nDf Residuals:                     439   BIC:                             4841.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.1335      2.702     56.303      0.000     146.823     157.444\nbmi          675.0714     63.475     10.635      0.000     550.318     799.825\ns5           614.9499     63.475      9.688      0.000     490.197     739.703\n==============================================================================\nOmnibus:                        7.406   Durbin-Watson:                   1.919\nProb(Omnibus):                  0.025   Jarque-Bera (JB):                5.552\nSkew:                           0.160   Prob(JB):                       0.0623\nKurtosis:                       2.554   Cond. No.                         28.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[89]: Copied! <pre>print(np.sqrt(est.mse_resid))\n</pre> print(np.sqrt(est.mse_resid)) <pre>56.807512054914135\n</pre> In\u00a0[90]: Copied! <pre>df_train, df_test = train_test_split(diabetes_df, test_size=0.3)\n</pre> df_train, df_test = train_test_split(diabetes_df, test_size=0.3) In\u00a0[91]: Copied! <pre>est_train = ols(formula=\"target ~ sex + bmi + bp + s5\", data=df_train).fit()\nprint(est_train.summary())\n</pre> est_train = ols(formula=\"target ~ sex + bmi + bp + s5\", data=df_train).fit() print(est_train.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.483\nModel:                            OLS   Adj. R-squared:                  0.476\nMethod:                 Least Squares   F-statistic:                     70.98\nDate:                Sun, 18 May 2025   Prob (F-statistic):           2.14e-42\nTime:                        17:01:06   Log-Likelihood:                -1677.5\nNo. Observations:                 309   AIC:                             3365.\nDf Residuals:                     304   BIC:                             3384.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    152.2473      3.169     48.040      0.000     146.011     158.484\nsex         -168.8222     68.852     -2.452      0.015    -304.308     -33.336\nbmi          581.0621     75.850      7.661      0.000     431.804     730.320\nbp           329.1392     77.901      4.225      0.000     175.845     482.433\ns5           574.0595     79.412      7.229      0.000     417.793     730.327\n==============================================================================\nOmnibus:                        3.568   Durbin-Watson:                   2.053\nProb(Omnibus):                  0.168   Jarque-Bera (JB):                2.874\nSkew:                           0.120   Prob(JB):                        0.238\nKurtosis:                       2.594   Cond. No.                         28.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[92]: Copied! <pre>print(np.sqrt(est_train.mse_resid))\n</pre> print(np.sqrt(est_train.mse_resid)) <pre>55.58467333439272\n</pre> In\u00a0[93]: Copied! <pre>test_pred = est_train.predict(df_test)\nprint(\"OOS R-squared: \" + str(r2_score(df_test[\"target\"], test_pred)))\n</pre> test_pred = est_train.predict(df_test) print(\"OOS R-squared: \" + str(r2_score(df_test[\"target\"], test_pred))) <pre>OOS R-squared: 0.4884507706169847\n</pre> In\u00a0[94]: Copied! <pre>df_test.head()\n</pre> df_test.head() Out[94]: age sex bmi bp s1 s2 s3 s4 s5 s6 target 189 -0.001882 -0.044642 -0.066563 0.001215 -0.002945 0.003070 0.011824 -0.002592 -0.020292 -0.025930 79.0 37 -0.009147 -0.044642 0.011039 -0.057313 -0.024960 -0.042963 0.030232 -0.039493 0.017036 -0.005220 276.0 4 0.005383 -0.044642 -0.036385 0.021872 0.003935 0.015596 0.008142 -0.002592 -0.031988 -0.046641 135.0 225 0.030811 0.050680 0.032595 0.049415 -0.040096 -0.043589 -0.069172 0.034309 0.063015 0.003064 208.0 77 -0.096328 -0.044642 -0.036385 -0.074527 -0.038720 -0.027618 0.015505 -0.039493 -0.074093 -0.001078 200.0 In\u00a0[95]: Copied! <pre>test_pred.head()\n</pre> test_pred.head() Out[95]: <pre>189    109.857331\n37     157.113898\n4      127.478307\n225    215.070221\n77      71.578589\ndtype: float64</pre> In\u00a0[96]: Copied! <pre>x = df_test[\"target\"]\ny = test_pred\n\nplt.scatter(x, y, s=2)\nplt.title(\"Predicted Target vs. Actual Target\")\nplt.xlabel(\"Actual Target\")\nplt.ylabel(\"Predicted Target\")\n\n# Calculate linear regression line\nslope, intercept = np.polyfit(x, y, 1)\ny_pred = slope * x + intercept  # y-values of the regression line\n\n# Plot the regression line\nplt.plot(x, y_pred, color=\"red\", label=f\"Line: y = {intercept:.2f} + {slope:.2f}x\")\n</pre> x = df_test[\"target\"] y = test_pred  plt.scatter(x, y, s=2) plt.title(\"Predicted Target vs. Actual Target\") plt.xlabel(\"Actual Target\") plt.ylabel(\"Predicted Target\")  # Calculate linear regression line slope, intercept = np.polyfit(x, y, 1) y_pred = slope * x + intercept  # y-values of the regression line  # Plot the regression line plt.plot(x, y_pred, color=\"red\", label=f\"Line: y = {intercept:.2f} + {slope:.2f}x\") Out[96]: <pre>[&lt;matplotlib.lines.Line2D at 0x2436ed05d10&gt;]</pre>"},{"location":"notebooks/other/regression/#regression","title":"Regression\u00b6","text":""},{"location":"notebooks/other/regression/#blue-bikes-rental-data","title":"Blue Bikes Rental Data\u00b6","text":""},{"location":"notebooks/other/regression/#diabetes-dataset","title":"Diabetes Dataset\u00b6","text":""},{"location":"samples/capstone/","title":"Measured Particulate Matter Concentrations in New York City\u2019s Subway System Suggest Exceedance of U.S. Environmental Protection","text":"<p>Fall 2021</p> <p></p>"},{"location":"samples/capstone/#objective","title":"Objective","text":"<p>To assess particulate matter (PM2.5) concentrations in NYC\u2019s subway system using low-cost air sensors and determine whether they exceed EPA standards, with a focus on exposure risks for student commuters.</p>"},{"location":"samples/capstone/#methods","title":"Methods","text":"<p>Devices Used:</p> <ul> <li>PurpleAir PA-II-SD: Low-cost laser particle counter.</li> <li>UPAS: Gravimetric filter-based sampler used for calibration.</li> <li>Temptop M2000C: Nephelometer for real-time PM and CO\u2082 data.</li> </ul> <p>Data Collection:</p> <ul> <li>25+ subway ride events.</li> <li>Locations: Manhattan, Staten Island, Brooklyn, Queens.</li> <li>Two strategies: (a) extended station sampling, (b) full commute sampling.</li> <li>Calibration done via UPAS filters and outdoor reference stations.</li> </ul>"},{"location":"samples/capstone/#key-sites","title":"Key Sites","text":"<ul> <li>Subway lines: 1, F, L, 7, SIR, W, etc.</li> <li>Stations near schools: WHEELS (Manhattan), Curtis HS (Staten Island), West End Secondary.</li> </ul>"},{"location":"samples/capstone/#key-findings","title":"Key Findings","text":"<p>PM2.5 Concentrations:</p> <ul> <li>Average subway PM2.5: ~105 \u00b5g/m\u00b3</li> <li>NYC outdoor average: ~6.55 \u00b5g/m\u00b3</li> <li>EPA 24-hour standard: 35 \u00b5g/m\u00b3</li> </ul> <p>Worst Stations:</p> <ul> <li>2nd Ave (F line): 407.88 \u00b5g/m\u00b3</li> <li>Lexington Ave-63rd: 113.19 \u00b5g/m\u00b3</li> </ul> <p>Best Air Quality:</p> <ul> <li>SIR and W trains (mostly aboveground): &lt; EPA thresholds</li> </ul> <p>Commuter Exposure:</p> <ul> <li>2-hour daily commute: 19% above daily EPA threshold</li> <li>MTA workers (8 hrs underground): 246% above threshold</li> </ul> <p>Train Car Behavior:</p> <ul> <li>PM drops during transit, rises at stops due to door opening</li> </ul>"},{"location":"samples/capstone/#health-implications","title":"Health Implications","text":"<ul> <li>PM2.5 linked to respiratory and cardiovascular disease, especially in youth and elderly.</li> <li>Underground PM contains metals (Fe, Mn, Cr) more harmful than typical urban PM.</li> <li>Subway exposure could pose significant health risks over time.</li> </ul>"},{"location":"samples/capstone/#limitations","title":"Limitations","text":"<ul> <li>Only 4 PurpleAir sensors available.</li> <li>Temporal and spatial coverage limited.</li> <li>Lack of full chemical characterization of PM2.5.</li> <li>Calibration methods varied slightly for aboveground vs underground data.</li> </ul>"},{"location":"samples/capstone/#recommendations","title":"Recommendations","text":"<ul> <li>Install a network of calibrated low-cost air quality sensors across the subway system.</li> <li>Conduct long-term air quality monitoring and integrate findings into public health alerts.</li> <li>Focus on environmental justice, identifying communities disproportionately affected by subway-related pollution.</li> </ul>"},{"location":"samples/capstone/#future-directions","title":"Future Directions","text":"<ul> <li>Expand data collection to all subway lines and times of day.</li> <li>Apply similar monitoring in other indoor public spaces (e.g. schools, buses).</li> <li>Use findings to inform ventilation upgrades and policy changes in subway systems.</li> </ul>"},{"location":"samples/capstone/#acknowledgements","title":"Acknowledgements","text":"<ul> <li>NYC Outward Bound Schools</li> <li>NYC Department of Education</li> <li>Faculty Advisor: Dr. Benjamin Bostick</li> <li>Capstone Team: Greg Hopper, Noah Portman, Joseph Galbiati</li> </ul> <p>Full report available here.</p>"},{"location":"samples/natural_capital/","title":"Accounting for Natural Capital - Breaking the Carbon Budget","text":"<p>Winter 2020</p> <p></p>"},{"location":"samples/natural_capital/#objective","title":"Objective","text":"<p>To explore how rethinking natural and human capital accounting can inform fairer and more sustainable global carbon budget policies, highlighting the need for equitable climate action through better asset valuation and policy alignment.</p>"},{"location":"samples/natural_capital/#key-concepts","title":"Key Concepts","text":"<ul> <li>Carbon Budget: Global limit of 850\u20131550 GtCO\u2082 to stay below 2\u00b0C warming.</li> <li>Natural Capital: Earth\u2019s stock of resources and ecosystems (e.g. air, water, land, biodiversity).</li> <li>Human Capital: Educated, healthy, skilled populations driving sustainable development.</li> <li>Disparity: Only a few of the 195 Paris Agreement signatories are on track to meet targets.</li> </ul>"},{"location":"samples/natural_capital/#data-references","title":"Data &amp; References","text":"<ul> <li>Reports Cited:<ul> <li>World Bank\u2019s Changing Wealth of Nations 2018</li> <li>McKinsey Global Institute\u2019s Reduced Dividends on Natural Capital</li> <li>Nationally Determined Contributions (NDCs) from Morocco, Costa Rica, The Gambia</li> <li>Climate Action Tracker (CAT)</li> </ul> </li> </ul>"},{"location":"samples/natural_capital/#capital-stock-analysis","title":"Capital Stock Analysis","text":"<ul> <li>Natural Capital undervalued \u2192 exploited beyond replenishment (Tragedy of the Commons).</li> <li>Accounting Frameworks:<ul> <li>EVA (Ecosystem Valuation &amp; Accounting) by Conservation International.</li> <li>SEEA (System of Environmental-Economic Accounting) \u2013 harmonized with GDP.</li> </ul> </li> <li>World Bank Findings:<ul> <li>Global wealth \u2191 66% from 1995\u20132014; human capital = 2/3 of global wealth.</li> <li>Natural capital = 9% globally, but nearly 50% of low-income countries' wealth.</li> </ul> </li> </ul>"},{"location":"samples/natural_capital/#country-comparisons-leadership","title":"Country Comparisons &amp; Leadership","text":"<ul> <li> <p>Top Performers:</p> <ul> <li>Morocco: 42\u201352% renewable energy by 2030; Noor solar plant = global model.</li> <li>The Gambia: Massive forest restoration + solar PV expansion.</li> <li>Costa Rica: 98% renewable electricity; moratorium on oil drilling.</li> </ul> </li> <li> <p>Lagging Nations: Developed countries with high historical emissions.</p> </li> </ul>"},{"location":"samples/natural_capital/#policy-effort-sharing-frameworks","title":"Policy &amp; Effort-Sharing Frameworks","text":"<ul> <li> <p>Equity Principles Used for Carbon Budget Allocation:</p> <ul> <li>Grandfathering (GF)</li> <li>Per Capita Convergence (PCC, IEPC)</li> <li>Equal Cumulative Per Capita Emissions (ECPC)</li> <li>Ability to Pay (AP)</li> <li>Greenhouse Development Rights (GDR)</li> <li>Cost-Optimal (CO)</li> </ul> </li> <li> <p>Conclusion: Equity-driven models often result in negative budgets for wealthy nations, implying the need for urgent and ambitious policy reform.</p> </li> </ul>"},{"location":"samples/natural_capital/#case-studies-models","title":"Case Studies &amp; Models","text":"<ul> <li>Three Gorges Reservoir (China): Demonstrated negative decoupling between land use and sustainability; emphasized need for stronger ecological policies.</li> <li>Zhang et al.: Introduced a formula to measure sustainability via land capital utilization ratio (stock vs. flow).</li> <li>van den Berg et al.: Compared seven effort-sharing models; illustrated massive implications on fair carbon budget distribution.</li> </ul>"},{"location":"samples/natural_capital/#recommendations","title":"Recommendations","text":"<ul> <li>Recognize natural capital as core to wealth and development.</li> <li>Encourage mainstreaming of natural capital accounting in government and business decisions.</li> <li>Use Natural Capital Protocol to help businesses balance growth with ecological impact.</li> <li>Invest globally in ecosystem services (e.g. coral insurance, mangrove protection).</li> <li>Pursue coordinated international response, particularly for climate-vulnerable regions.</li> </ul>"},{"location":"samples/natural_capital/#conclusion","title":"Conclusion","text":"<ul> <li>Rethinking economic progress by incorporating natural capital valuation is critical.</li> <li>Equity, transparency, and long-term vision must guide global carbon budget negotiations.</li> <li>Collaborative leadership from both developed and developing countries is key to climate resilience.</li> </ul> <p>Full report available here.</p>"},{"location":"samples/summary/","title":"Samples","text":"<p>This collection of applied research projects spans air and water quality, climate policy, and ecological accounting. Through data-driven analysis, remote sensing, and policy evaluation, each study explores sustainable development challenges and proposes actionable solutions for decision-makers, urban planners, and environmental managers.</p> <p>Program: Columbia University, MS in Sustainability Science (2020\u20132021)</p>"},{"location":"samples/summary/#brief-overviews","title":"Brief Overviews","text":"<p>Toggle between tabs</p> \ud83d\ude87 Capstone\ud83d\udeb0 WCI\ud83c\udf0a Wetland\ud83c\udf33 Natural Capital <p> PM2.5 in NYC Subway System: Air Quality Analysis</p> <ul> <li>Focus: Measuring underground air pollution in NYC subway lines using low-cost sensors.</li> <li>Devices: PurpleAir, UPAS, Temptop.</li> <li>Findings:<ul> <li>Average PM2.5 = 105 \u00b5g/m\u00b3 (vs. 6.55 \u00b5g/m\u00b3 outdoors).</li> <li>Many stations exceed EPA 24-hr standard (35 \u00b5g/m\u00b3).</li> <li>Commuters and workers face significant health risks.</li> </ul> </li> <li>Recommendation: Build a sensor network and revise ventilation policy.</li> </ul> <p> Water Confidence Index (WCI)</p> <ul> <li>Focus: A composite index ranking U.S. public water systems on compliance and reporting transparency.</li> <li>Method: EPA SDWIS/ECHO data (2011\u20132020), normalized into Environmental Compliance + Truthful Reporting scores.</li> <li>Top States: Indiana, Minnesota, Michigan</li> <li>Bottom States: Texas, California, Mississippi</li> <li>Applications: Infrastructure funding prioritization, public trust, environmental justice.</li> </ul> <p> Coastal Wetland Eutrophication (Gulf Coast)</p> <ul> <li>Focus: Evaluating how wetlands vs. urbanized coasts impact harmful algal blooms (HABs).</li> <li>Method: MODIS-Aqua (2005\u20132020) and Landsat 8 chlorophyll-a anomaly analysis.</li> <li>Findings:<ul> <li>Wetlands reduced bloom severity by ~14% in high-bloom years.</li> <li>Urban areas showed higher chlorophyll and nutrient loading.</li> </ul> </li> <li>Implication: Coastal wetland restoration can mitigate eutrophication and climate risk.</li> </ul> <p> Breaking the Carbon Budget: Accounting for Natural Capital</p> <ul> <li>Focus: Redefining economic development by valuing natural capital and reallocating the global carbon budget.</li> <li>Themes: Paris Agreement compliance, natural vs. human capital, equity in emissions reductions.</li> <li>Key Insights:<ul> <li>Natural capital accounts for nearly 50% of wealth in low-income nations.</li> <li>Few countries (e.g., Morocco, The Gambia) align with 1.5\u00b0C targets.</li> <li>Policy must shift from short-term growth to ecosystem investment and capital rebalancing.</li> </ul> </li> </ul>"},{"location":"samples/summary/#common-threads","title":"Common Threads","text":"Theme Description Environmental Justice Low-income or vulnerable groups often bear disproportionate environmental harm. Data for Decision-Making All projects use empirical data to build actionable metrics or insights. Systems Thinking Interconnectedness between economy, environment, and health is emphasized. Equity &amp; Policy Reform Equity-based frameworks proposed for emissions, infrastructure, and resource allocation."},{"location":"samples/summary/#skills-demonstrated","title":"Skills Demonstrated","text":"<ul> <li>Environmental indicators and composite index design</li> <li>Remote sensing (MODIS, Landsat), GIS, and anomaly analysis</li> <li>Public health impact modeling</li> <li>Sustainable development policy review</li> <li>Communication of technical findings to policymakers and public stakeholders</li> </ul>"},{"location":"samples/wci/","title":"The Water Confidence Index (WCI): Its Development and Construction","text":"<p>April 26, 2021</p> <p></p>"},{"location":"samples/wci/#objective","title":"Objective","text":"<p>To develop an environmental composite index\u2014Water Confidence Index (WCI)\u2014that measures and ranks the performance of large U.S. public water systems (PWS) based on environmental compliance and truthful reporting.</p>"},{"location":"samples/wci/#motivation","title":"Motivation","text":"<ul> <li>No existing index ranks U.S. water utilities by environmental health performance.</li> <li>Infrastructure challenges include underfunding, aging pipes, cybersecurity threats, and increasing violations.</li> <li>The index supports transparency, prioritization for funding, and accountability in public water infrastructure.</li> </ul>"},{"location":"samples/wci/#wci-structure","title":"WCI Structure","text":"<p>WCI = Average of:</p> <ol> <li> <p>Environmental Compliance (EC):</p> <ul> <li>Health-based violations (40%)</li> <li>Serious violator trends (40%)</li> <li>Violations per site visit (20%)</li> </ul> </li> <li> <p>Truthful Reporting (TR):</p> <ul> <li>Public notice + monitoring violations (40%)</li> <li>Enforcement-to-violation ratio (60%)</li> </ul> </li> </ol> <p>Normalization: Min-max method used to scale inputs.</p>"},{"location":"samples/wci/#data-sources","title":"Data Sources","text":"<ul> <li>EPA ECHO / SDWIS datasets (2011\u20132020)</li> <li>Filtered to large &amp; very large PWS (serving &gt;10,000 people)</li> <li>Excludes: Small systems, Tribal systems, Territories</li> <li>Population data: U.S. Census Bureau estimates</li> </ul>"},{"location":"samples/wci/#methodology","title":"Methodology","text":"<ul> <li>Indicators derived from violations, enforcements, and compliance trends.</li> <li>Weighted arithmetic calculations yield EC and TR scores per state.</li> <li>Combined into a 0\u20131 WCI score, where lower = better.</li> </ul>"},{"location":"samples/wci/#results","title":"Results","text":"<p>Top-performing states (lowest WCI scores):</p> Rank State 1 Indiana --&gt; (0.03) 2 North Dakota 3 Minnesota 4 South Dakota 5 Michigan <p>Lowest-performing states (highest WCI scores):</p> Rank State 46 Arizona 47 Texas 48 California 49 Mississippi 50 Idaho   --&gt; (0.86) <p>Top EPA Region: Region 5 (Midwest)</p> <p>Worst EPA Region: Region 10 (Northwest)</p>"},{"location":"samples/wci/#sensitivity-analyses","title":"Sensitivity Analyses","text":"<ul> <li>SA-1: Reweighted EC to focus on health violations \u2192 minor rank shifts.</li> <li>SA-2: Reweighted TR to focus on public health violations \u2192 larger shifts for lower-ranking states.</li> <li>SA-3: Equal weighting of all inputs \u2192 stable top/bottom rankings; mid-range states more sensitive.</li> </ul>"},{"location":"samples/wci/#limitations-future-directions","title":"Limitations &amp; Future Directions","text":"<ul> <li>Does not include small PWS or private wells.</li> <li>TR score may overemphasize certain violations due to uneven frequency.</li> <li>No economic or health outcome data included yet (e.g., cost of repair, hospital visits).</li> <li>Future enhancements:<ul> <li>Add third component for public notification.</li> <li>Link violations to health impacts or cost estimates.</li> <li>Visualize results by EPA Region for policy impact.</li> </ul> </li> </ul>"},{"location":"samples/wci/#conclusion","title":"Conclusion","text":"<ul> <li>The WCI provides a transparent, data-driven ranking of public water utilities based on SDWA compliance.</li> <li>It can inform public awareness, EPA oversight, and infrastructure funding allocation.</li> <li>It highlights the power of composite indicators in environmental decision-making.</li> </ul> <p>Full report available here.</p>"},{"location":"samples/wetland/","title":"Case Study on Coastal Wetland Eutrophication: Gordon Pass, The Gulf Coast","text":"<p>December 14, 2020</p> <p></p>"},{"location":"samples/wetland/#objective","title":"Objective","text":"<p>To examine how coastal wetlands versus urban coastlines impact eutrophication, particularly harmful algal blooms (HABs), using remote sensing techniques off the southwestern coast of Florida.</p>"},{"location":"samples/wetland/#key-concepts","title":"Key Concepts","text":"<ul> <li>Eutrophication: Nutrient enrichment in water bodies, leading to excessive algae growth.</li> <li>Harmful Algal Blooms (HABs): Especially Karenia brevis (red tide), producing toxins harmful to humans and marine ecosystems.</li> <li>Remote Sensing: Tools like MODIS-Aqua and Landsat 8 used to track chlorophyll concentrations (a proxy for eutrophication).</li> </ul>"},{"location":"samples/wetland/#study-area","title":"Study Area","text":"<ul> <li>Location: Gordon Pass between:<ul> <li>Naples (urban coast)</li> <li>Everglades (wetlands)</li> </ul> </li> <li>Regions Analyzed (4km x 4km):<ul> <li>Region A: Offshore Urban (North)</li> <li>Region B: Gordon Pass (Middle / Control)</li> <li>Region C: Offshore Wetlands (South)</li> </ul> </li> </ul>"},{"location":"samples/wetland/#methods","title":"Methods","text":"<ul> <li>MODIS-Aqua: 8-day chlorophyll-a data (2005\u20132020).</li> <li>Landsat 8: High-resolution (30m) imagery from 2014 &amp; 2019, processed via Acolite and SeaDAS.</li> <li>Anomaly Analysis: Compared seasonal chlorophyll values to a 15-year baseline.</li> <li>Ocean Currents: Used to interpret HAB movement patterns.</li> </ul>"},{"location":"samples/wetland/#key-findings","title":"Key Findings","text":"<ul> <li>Seasonality: Fall chlorophyll peaks observed consistently.</li> <li>Bloom Year 2019:<ul> <li>Region A (urban): Highest chlorophyll levels.</li> <li>Region C (wetlands): Lowest levels.</li> <li>2014 was a lower bloom year.</li> </ul> </li> <li>Wetland Effectiveness:<ul> <li>Region C had ~14% less chlorophyll than Region B.</li> <li>Region A had ~16% more than Region B.</li> <li>Wetlands likely help buffer nutrients and reduce HAB impact during extreme bloom events.</li> </ul> </li> </ul>"},{"location":"samples/wetland/#conclusions","title":"Conclusions","text":"<ul> <li>Coastal wetlands may act as nutrient sinks, reducing the intensity of harmful algal blooms.</li> <li>Urbanized coasts may contribute more nutrients, exacerbating eutrophication.</li> <li>Highlights the importance of Everglades restoration efforts, particularly around water quality management and wetland expansion.</li> </ul>"},{"location":"samples/wetland/#reflections","title":"Reflections","text":"<ul> <li>Remote Sensing Tools: Crucial for analyzing large-scale coastal changes. Landsat 8 was especially helpful for high-resolution analysis.</li> <li>Biggest Challenge: Defining spatial boundaries offshore that reflect true impacts without overlap or distortion.</li> <li>What Could Be Improved: Including more high-bloom and low-bloom years for a stronger statistical case.</li> <li>Key Learning: Accessing and applying remote sensing data (NASA, USGS) to develop and test scientific hypotheses confidently.</li> </ul> <p>Full report available here.</p>"}]}